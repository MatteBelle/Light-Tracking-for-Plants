{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RpaqyX17CC5",
        "outputId": "f16e8bef-8fd7-41a6-c2d0-a3457171da43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 39.76382254857484\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# import xgboost as xgb\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# # Prepare the data as before\n",
        "# df = pd.read_csv(\"light_data_with_hour_only.csv\")\n",
        "\n",
        "# # Shift the light level to get the next light level\n",
        "# df['next_light_level'] = df.groupby('room')['light_level'].shift(-1)\n",
        "# df = df.dropna(subset=['next_light_level'])\n",
        "\n",
        "# # One-hot encode 'room' column\n",
        "# encoder = OneHotEncoder(sparse_output=False)\n",
        "# encoded_rooms = encoder.fit_transform(df[['room']])\n",
        "# encoded_rooms_df = pd.DataFrame(encoded_rooms, columns=encoder.categories_[0])\n",
        "\n",
        "# # Merge everything into a final DataFrame\n",
        "# df = pd.concat([df, encoded_rooms_df], axis=1)\n",
        "# X = df[['timestamp'] + list(encoded_rooms_df.columns) + ['light_level']]\n",
        "# y = df['next_light_level']\n",
        "\n",
        "# # Split data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Train XGBoost model\n",
        "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "# dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# params = {'objective': 'reg:squarederror', 'eval_metric': 'rmse'}\n",
        "# model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# # Save the trained model\n",
        "# model.save_model('xgb_model.json')  # Save model to a file (JSON format)\n",
        "\n",
        "# # Predict on test data\n",
        "# y_pred = model.predict(dtest)\n",
        "\n",
        "# # Evaluate model\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "# print(f'Mean Squared Error: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXRhByB2Prp0"
      },
      "source": [
        "This first version remakes the xgb model based on the new dataset, which is appropriately formatted for influxdb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWkPMcLfPqOq",
        "outputId": "27283017-c60b-40b0-e915-c57ef637eaec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  result  table                       _start  \\\n",
            "2         NaN     NaN      1  2025-01-08T10:46:44.129733Z   \n",
            "3         NaN     NaN      1  2025-01-08T10:46:44.129733Z   \n",
            "4         NaN     NaN      1  2025-01-08T10:46:44.129733Z   \n",
            "5         NaN     NaN      1  2025-01-08T10:46:44.129733Z   \n",
            "6         NaN     NaN      1  2025-01-08T10:46:44.129733Z   \n",
            "\n",
            "                         _stop                     _time  _value  \\\n",
            "2  2025-01-08T16:46:44.129889Z 2024-11-01 01:00:00+00:00       0   \n",
            "3  2025-01-08T16:46:44.129889Z 2024-11-01 01:00:00+00:00       0   \n",
            "4  2025-01-08T16:46:44.129889Z 2024-11-01 02:00:00+00:00       0   \n",
            "5  2025-01-08T16:46:44.129889Z 2024-11-01 02:00:00+00:00       0   \n",
            "6  2025-01-08T16:46:44.129889Z 2024-11-01 03:00:00+00:00       0   \n",
            "\n",
            "         _field    _measurement device_id position  sampling_rate  \n",
            "2  sensors_mean  light_tracking   ESP32_1  balcony           5000  \n",
            "3  sensors_mean  light_tracking   ESP32_1  bedroom           5000  \n",
            "4  sensors_mean  light_tracking   ESP32_1  balcony           5000  \n",
            "5  sensors_mean  light_tracking   ESP32_1  bedroom           5000  \n",
            "6  sensors_mean  light_tracking   ESP32_1  balcony           5000  \n",
            "   hour                     _time\n",
            "2     1 2024-11-01 01:00:00+00:00\n",
            "3     1 2024-11-01 01:00:00+00:00\n",
            "4     2 2024-11-01 02:00:00+00:00\n",
            "5     2 2024-11-01 02:00:00+00:00\n",
            "6     3 2024-11-01 03:00:00+00:00\n",
            "\n",
            "Let's print the shifted dataset\n",
            "    hour     room  light_level  next_light_level\n",
            "3      1  bedroom            0                 0\n",
            "4      2  balcony            0                 0\n",
            "5      2  bedroom            0                 0\n",
            "6      3  balcony            0                 0\n",
            "7      3  bedroom            0                 0\n",
            "8      4  balcony            0                 0\n",
            "9      4  bedroom            0                 0\n",
            "10     5  balcony            0                 0\n",
            "11     5  bedroom            0                 0\n",
            "12     6  balcony            0                97\n",
            "13     6  bedroom            0                 0\n",
            "14     7  balcony           97                97\n",
            "15     7  bedroom            0                 0\n",
            "16     8  balcony           97                96\n",
            "17     8  bedroom            0                93\n",
            "18     9  balcony           96                97\n",
            "19     9  bedroom           93                92\n",
            "20    10  balcony           97                96\n",
            "21    10  bedroom           92                92\n",
            "I now print the training values\n",
            "   hour  balcony  bedroom  light_level\n",
            "0     1      1.0      0.0            0\n",
            "1     1      0.0      1.0            0\n",
            "2     2      1.0      0.0            0\n",
            "3     2      0.0      1.0            0\n",
            "4     3      1.0      0.0            0\n",
            "Room       | Current Light Level | Real Next Light | Predicted Next Light\n",
            "bedroom    | 92.0              | 7            | 15\n",
            "bedroom    | 0.0              | 0            | 0\n",
            "balcony    | 96.0              | 97            | 96\n",
            "balcony    | 0.0              | 97            | 49\n",
            "balcony    | 0.0              | 0            | 0\n",
            "bedroom    | 0.0              | 0            | 0\n",
            "balcony    | 7.0              | 0            | 0\n",
            "bedroom    | 0.0              | 0            | 0\n",
            "bedroom    | 7.0              | 8            | 6\n",
            "bedroom    | 92.0              | 89            | 92\n",
            "bedroom    | 3.0              | 6            | 7\n",
            "bedroom    | 8.0              | 0            | 0\n",
            "balcony    | 0.0              | 0            | 0\n",
            "bedroom    | 93.0              | 8            | 12\n",
            "bedroom    | 0.0              | 0            | 0\n",
            "balcony    | 0.0              | 0            | 0\n",
            "bedroom    | 90.0              | 92            | 91\n",
            "balcony    | 95.0              | 96            | 96\n",
            "bedroom    | 5.0              | 6            | 7\n",
            "bedroom    | 0.0              | 92            | 92\n",
            "balcony    | 0.0              | 0            | 0\n",
            "Mean Squared Error: 71.62962962962963\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset, skipping lines starting with '#'\n",
        "df = pd.read_csv(\"annotated_light_tracking.csv\", comment=\"#\")\n",
        "# The first three lines are for header\n",
        "df = df.iloc[2:]\n",
        "# Strip any extra whitespace from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "# Verify column names to make sure '_time' exists\n",
        "#print(df.columns)\n",
        "\n",
        "# Convert the '_time' column to datetime if it's not already\n",
        "df['_time'] = pd.to_datetime(df['_time'])\n",
        "\n",
        "# Check the first few rows to ensure data is loaded correctly\n",
        "print(df.head())\n",
        "\n",
        "# If '_time' is parsed correctly, continue with feature engineering\n",
        "df['hour'] = df['_time'].dt.hour\n",
        "\n",
        "# Check if the transformation works\n",
        "print(df[['hour', '_time']].head())\n",
        "\n",
        "# Rename and select required columns\n",
        "df = df.rename(columns={\"position\": \"room\", \"_value\": \"light_level\"})\n",
        "df = df[['hour', 'room', 'light_level']]\n",
        "\n",
        "# Shift light level to create the target for the next time step\n",
        "df['next_light_level'] = df.groupby('room')['light_level'].shift(-1)\n",
        "df = df.dropna(subset=['next_light_level'])\n",
        "df['next_light_level'] = df['next_light_level'].apply(lambda x: int(x))\n",
        "print()\n",
        "print(\"Let's print the shifted dataset\")\n",
        "print(df[1:20])\n",
        "\n",
        "# One-hot encode the 'room' column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_rooms = encoder.fit_transform(df[['room']])\n",
        "room_columns = encoder.categories_[0]  # Room categories (e.g., balcony, bedroom, living_room)\n",
        "encoded_rooms_df = pd.DataFrame(encoded_rooms, columns=room_columns)\n",
        "\n",
        "# Merge encoded rooms with the original DataFrame\n",
        "df = pd.concat([df.reset_index(drop=True), encoded_rooms_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[['hour'] + list(room_columns) + ['light_level']]\n",
        "print(\"I now print the training values\")\n",
        "print(X.head())\n",
        "y = df['next_light_level']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "params = {'objective': 'reg:squarederror', 'eval_metric': 'rmse'}\n",
        "model = xgb.train(params, dtrain, num_boost_round=20)\n",
        "\n",
        "# Save the trained model and encoder\n",
        "model.save_model(\"xgb_model.json\")\n",
        "encoder_path = \"room_encoder.npy\"\n",
        "np.save(encoder_path, encoder)  # Save encoder for future predictions\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(dtest).astype(int)\n",
        "\n",
        "# Print some prediction examples\n",
        "print(f'Room       | Current Light Level | Real Next Light | Predicted Next Light')\n",
        "for i, (real, pred) in enumerate(zip(y_test, y_pred)):\n",
        "    current_light = X_test.iloc[i]['light_level']\n",
        "    room = X_test.iloc[i][['balcony', 'bedroom']].idxmax()\n",
        "    print(f'{room:<10} | {current_light}              | {real}            | {pred}')\n",
        "    if i >= 20:\n",
        "        break\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QDba0OMRamQ"
      },
      "source": [
        "The following code is to train a NN on the formatted dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxuyGLDaRZsd",
        "outputId": "083fa988-a36d-4424-ef16-8910fff8678e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3235.2041 - mean_squared_error: 3235.2041 - val_loss: 1207.3223 - val_mean_squared_error: 1207.3223\n",
            "Epoch 2/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 850.8309 - mean_squared_error: 850.8309 - val_loss: 569.1741 - val_mean_squared_error: 569.1741\n",
            "Epoch 3/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 527.9602 - mean_squared_error: 527.9602 - val_loss: 529.7346 - val_mean_squared_error: 529.7346\n",
            "Epoch 4/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 485.5536 - mean_squared_error: 485.5536 - val_loss: 501.5987 - val_mean_squared_error: 501.5987\n",
            "Epoch 5/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 512.2957 - mean_squared_error: 512.2957 - val_loss: 482.8434 - val_mean_squared_error: 482.8434\n",
            "Epoch 6/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 481.3140 - mean_squared_error: 481.3140 - val_loss: 466.7931 - val_mean_squared_error: 466.7931\n",
            "Epoch 7/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 447.3614 - mean_squared_error: 447.3614 - val_loss: 442.2513 - val_mean_squared_error: 442.2513\n",
            "Epoch 8/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 424.7379 - mean_squared_error: 424.7379 - val_loss: 419.3463 - val_mean_squared_error: 419.3463\n",
            "Epoch 9/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 433.6460 - mean_squared_error: 433.6460 - val_loss: 400.9416 - val_mean_squared_error: 400.9416\n",
            "Epoch 10/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 440.0178 - mean_squared_error: 440.0178 - val_loss: 382.6322 - val_mean_squared_error: 382.6322\n",
            "Epoch 11/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 358.5761 - mean_squared_error: 358.5761 - val_loss: 365.2930 - val_mean_squared_error: 365.2930\n",
            "Epoch 12/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 360.5836 - mean_squared_error: 360.5836 - val_loss: 344.7200 - val_mean_squared_error: 344.7200\n",
            "Epoch 13/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 353.7510 - mean_squared_error: 353.7510 - val_loss: 318.2786 - val_mean_squared_error: 318.2786\n",
            "Epoch 14/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 363.8495 - mean_squared_error: 363.8495 - val_loss: 297.4583 - val_mean_squared_error: 297.4583\n",
            "Epoch 15/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 306.2339 - mean_squared_error: 306.2339 - val_loss: 279.1365 - val_mean_squared_error: 279.1365\n",
            "Epoch 16/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 300.4332 - mean_squared_error: 300.4332 - val_loss: 252.3154 - val_mean_squared_error: 252.3154\n",
            "Epoch 17/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 286.9852 - mean_squared_error: 286.9852 - val_loss: 231.3884 - val_mean_squared_error: 231.3884\n",
            "Epoch 18/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 276.4349 - mean_squared_error: 276.4349 - val_loss: 206.7861 - val_mean_squared_error: 206.7861\n",
            "Epoch 19/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 261.1388 - mean_squared_error: 261.1388 - val_loss: 176.9632 - val_mean_squared_error: 176.9632\n",
            "Epoch 20/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262.2240 - mean_squared_error: 262.2240 - val_loss: 178.9575 - val_mean_squared_error: 178.9575\n",
            "Epoch 21/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 208.4043 - mean_squared_error: 208.4043 - val_loss: 153.3495 - val_mean_squared_error: 153.3495\n",
            "Epoch 22/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 179.3151 - mean_squared_error: 179.3151 - val_loss: 163.2348 - val_mean_squared_error: 163.2348\n",
            "Epoch 23/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 207.7439 - mean_squared_error: 207.7439 - val_loss: 148.0870 - val_mean_squared_error: 148.0870\n",
            "Epoch 24/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 183.8515 - mean_squared_error: 183.8515 - val_loss: 132.6105 - val_mean_squared_error: 132.6105\n",
            "Epoch 25/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186.8950 - mean_squared_error: 186.8950 - val_loss: 120.5645 - val_mean_squared_error: 120.5645\n",
            "Epoch 26/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 137.3607 - mean_squared_error: 137.3607 - val_loss: 103.3814 - val_mean_squared_error: 103.3814\n",
            "Epoch 27/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 152.5079 - mean_squared_error: 152.5079 - val_loss: 103.2229 - val_mean_squared_error: 103.2229\n",
            "Epoch 28/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 148.8718 - mean_squared_error: 148.8718 - val_loss: 105.6938 - val_mean_squared_error: 105.6938\n",
            "Epoch 29/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 128.6220 - mean_squared_error: 128.6220 - val_loss: 96.9266 - val_mean_squared_error: 96.9266\n",
            "Epoch 30/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 139.0880 - mean_squared_error: 139.0880 - val_loss: 91.4392 - val_mean_squared_error: 91.4392\n",
            "Epoch 31/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.6518 - mean_squared_error: 109.6518 - val_loss: 82.5511 - val_mean_squared_error: 82.5511\n",
            "Epoch 32/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 115.4314 - mean_squared_error: 115.4314 - val_loss: 83.4577 - val_mean_squared_error: 83.4577\n",
            "Epoch 33/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 118.4651 - mean_squared_error: 118.4651 - val_loss: 79.2838 - val_mean_squared_error: 79.2838\n",
            "Epoch 34/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.3148 - mean_squared_error: 120.3148 - val_loss: 85.0717 - val_mean_squared_error: 85.0717\n",
            "Epoch 35/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 129.1951 - mean_squared_error: 129.1951 - val_loss: 77.6233 - val_mean_squared_error: 77.6233\n",
            "Epoch 36/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 132.5287 - mean_squared_error: 132.5287 - val_loss: 82.1249 - val_mean_squared_error: 82.1249\n",
            "Epoch 37/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101.5839 - mean_squared_error: 101.5839 - val_loss: 75.8834 - val_mean_squared_error: 75.8834\n",
            "Epoch 38/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102.5661 - mean_squared_error: 102.5661 - val_loss: 87.2427 - val_mean_squared_error: 87.2427\n",
            "Epoch 39/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 135.7374 - mean_squared_error: 135.7374 - val_loss: 72.6366 - val_mean_squared_error: 72.6366\n",
            "Epoch 40/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101.5545 - mean_squared_error: 101.5545 - val_loss: 73.7286 - val_mean_squared_error: 73.7286\n",
            "Epoch 41/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.2644 - mean_squared_error: 104.2644 - val_loss: 77.5935 - val_mean_squared_error: 77.5935\n",
            "Epoch 42/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.7559 - mean_squared_error: 103.7559 - val_loss: 76.5461 - val_mean_squared_error: 76.5461\n",
            "Epoch 43/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 128.4491 - mean_squared_error: 128.4491 - val_loss: 80.0336 - val_mean_squared_error: 80.0336\n",
            "Epoch 44/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 95.4312 - mean_squared_error: 95.4312 - val_loss: 73.2587 - val_mean_squared_error: 73.2587\n",
            "Epoch 45/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.3168 - mean_squared_error: 103.3168 - val_loss: 72.5712 - val_mean_squared_error: 72.5712\n",
            "Epoch 46/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 79.9820 - mean_squared_error: 79.9820 - val_loss: 83.8155 - val_mean_squared_error: 83.8155\n",
            "Epoch 47/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 95.2727 - mean_squared_error: 95.2727 - val_loss: 75.0289 - val_mean_squared_error: 75.0289\n",
            "Epoch 48/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 95.7974 - mean_squared_error: 95.7974 - val_loss: 72.4636 - val_mean_squared_error: 72.4636\n",
            "Epoch 49/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 123.5509 - mean_squared_error: 123.5509 - val_loss: 80.3431 - val_mean_squared_error: 80.3431\n",
            "Epoch 50/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 102.3410 - mean_squared_error: 102.3410 - val_loss: 77.6404 - val_mean_squared_error: 77.6404\n",
            "Epoch 51/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 112.5889 - mean_squared_error: 112.5889 - val_loss: 80.7211 - val_mean_squared_error: 80.7211\n",
            "Epoch 52/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.6967 - mean_squared_error: 86.6967 - val_loss: 73.6728 - val_mean_squared_error: 73.6728\n",
            "Epoch 53/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92.3714 - mean_squared_error: 92.3714 - val_loss: 74.7206 - val_mean_squared_error: 74.7206\n",
            "Epoch 54/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78.4108 - mean_squared_error: 78.4108 - val_loss: 75.4827 - val_mean_squared_error: 75.4827\n",
            "Epoch 55/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101.7876 - mean_squared_error: 101.7876 - val_loss: 78.2183 - val_mean_squared_error: 78.2183\n",
            "Epoch 56/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.1313 - mean_squared_error: 104.1313 - val_loss: 74.1040 - val_mean_squared_error: 74.1040\n",
            "Epoch 57/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.2809 - mean_squared_error: 111.2809 - val_loss: 74.0979 - val_mean_squared_error: 74.0979\n",
            "Epoch 58/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91.2589 - mean_squared_error: 91.2589 - val_loss: 73.1099 - val_mean_squared_error: 73.1099\n",
            "Epoch 59/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.3949 - mean_squared_error: 106.3949 - val_loss: 73.9386 - val_mean_squared_error: 73.9386\n",
            "Epoch 60/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.3329 - mean_squared_error: 103.3329 - val_loss: 77.6488 - val_mean_squared_error: 77.6488\n",
            "Epoch 61/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.7758 - mean_squared_error: 108.7758 - val_loss: 74.6839 - val_mean_squared_error: 74.6839\n",
            "Epoch 62/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.0709 - mean_squared_error: 113.0709 - val_loss: 79.0711 - val_mean_squared_error: 79.0711\n",
            "Epoch 63/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92.0697 - mean_squared_error: 92.0697 - val_loss: 75.3798 - val_mean_squared_error: 75.3798\n",
            "Epoch 64/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.3535 - mean_squared_error: 112.3535 - val_loss: 74.4049 - val_mean_squared_error: 74.4049\n",
            "Epoch 65/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82.3235 - mean_squared_error: 82.3235 - val_loss: 76.8223 - val_mean_squared_error: 76.8223\n",
            "Epoch 66/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.2256 - mean_squared_error: 106.2256 - val_loss: 73.9802 - val_mean_squared_error: 73.9802\n",
            "Epoch 67/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.5439 - mean_squared_error: 89.5439 - val_loss: 72.9807 - val_mean_squared_error: 72.9807\n",
            "Epoch 68/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101.6716 - mean_squared_error: 101.6716 - val_loss: 76.6822 - val_mean_squared_error: 76.6822\n",
            "Epoch 69/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87.9148 - mean_squared_error: 87.9148 - val_loss: 73.3379 - val_mean_squared_error: 73.3379\n",
            "Epoch 70/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.0156 - mean_squared_error: 113.0156 - val_loss: 73.4028 - val_mean_squared_error: 73.4028\n",
            "Epoch 71/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82.6174 - mean_squared_error: 82.6174 - val_loss: 72.6234 - val_mean_squared_error: 72.6234\n",
            "Epoch 72/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.6257 - mean_squared_error: 89.6257 - val_loss: 70.3195 - val_mean_squared_error: 70.3195\n",
            "Epoch 73/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.9436 - mean_squared_error: 79.9436 - val_loss: 74.8865 - val_mean_squared_error: 74.8865\n",
            "Epoch 74/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.3628 - mean_squared_error: 79.3628 - val_loss: 71.7020 - val_mean_squared_error: 71.7020\n",
            "Epoch 75/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 77.5401 - mean_squared_error: 77.5401 - val_loss: 71.6406 - val_mean_squared_error: 71.6406\n",
            "Epoch 76/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76.5362 - mean_squared_error: 76.5362 - val_loss: 69.7338 - val_mean_squared_error: 69.7338\n",
            "Epoch 77/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.0603 - mean_squared_error: 85.0603 - val_loss: 72.3317 - val_mean_squared_error: 72.3317\n",
            "Epoch 78/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91.8790 - mean_squared_error: 91.8790 - val_loss: 69.7033 - val_mean_squared_error: 69.7033\n",
            "Epoch 79/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87.6550 - mean_squared_error: 87.6550 - val_loss: 72.6213 - val_mean_squared_error: 72.6213\n",
            "Epoch 80/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.8290 - mean_squared_error: 102.8290 - val_loss: 72.4605 - val_mean_squared_error: 72.4605\n",
            "Epoch 81/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.6623 - mean_squared_error: 67.6623 - val_loss: 72.1475 - val_mean_squared_error: 72.1475\n",
            "Epoch 82/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.1637 - mean_squared_error: 71.1637 - val_loss: 72.7522 - val_mean_squared_error: 72.7522\n",
            "Epoch 83/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87.4074 - mean_squared_error: 87.4074 - val_loss: 75.2743 - val_mean_squared_error: 75.2743\n",
            "Epoch 84/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.5113 - mean_squared_error: 86.5113 - val_loss: 70.5131 - val_mean_squared_error: 70.5131\n",
            "Epoch 85/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93.1662 - mean_squared_error: 93.1662 - val_loss: 71.4762 - val_mean_squared_error: 71.4762\n",
            "Epoch 86/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.3820 - mean_squared_error: 71.3820 - val_loss: 70.1955 - val_mean_squared_error: 70.1955\n",
            "Epoch 87/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 69.1301 - mean_squared_error: 69.1301 - val_loss: 72.2968 - val_mean_squared_error: 72.2968\n",
            "Epoch 88/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 78.2765 - mean_squared_error: 78.2765 - val_loss: 72.1100 - val_mean_squared_error: 72.1100\n",
            "Epoch 89/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 81.6904 - mean_squared_error: 81.6904 - val_loss: 71.4917 - val_mean_squared_error: 71.4917\n",
            "Epoch 90/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 86.4616 - mean_squared_error: 86.4616 - val_loss: 72.5673 - val_mean_squared_error: 72.5673\n",
            "Epoch 91/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 78.8849 - mean_squared_error: 78.8849 - val_loss: 72.9040 - val_mean_squared_error: 72.9040\n",
            "Epoch 92/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 70.9658 - mean_squared_error: 70.9658 - val_loss: 71.6192 - val_mean_squared_error: 71.6192\n",
            "Epoch 93/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.4480 - mean_squared_error: 83.4480 - val_loss: 71.8551 - val_mean_squared_error: 71.8551\n",
            "Epoch 94/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.0103 - mean_squared_error: 84.0103 - val_loss: 74.5243 - val_mean_squared_error: 74.5243\n",
            "Epoch 95/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.1868 - mean_squared_error: 94.1868 - val_loss: 71.7910 - val_mean_squared_error: 71.7910\n",
            "Epoch 96/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.9528 - mean_squared_error: 72.9528 - val_loss: 71.3853 - val_mean_squared_error: 71.3853\n",
            "Epoch 97/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.0317 - mean_squared_error: 69.0317 - val_loss: 72.8746 - val_mean_squared_error: 72.8746\n",
            "Epoch 98/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87.3521 - mean_squared_error: 87.3521 - val_loss: 70.7403 - val_mean_squared_error: 70.7403\n",
            "Epoch 99/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78.1482 - mean_squared_error: 78.1482 - val_loss: 71.4413 - val_mean_squared_error: 71.4413\n",
            "Epoch 100/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.4594 - mean_squared_error: 107.4594 - val_loss: 71.9025 - val_mean_squared_error: 71.9025\n",
            "Epoch 101/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.4955 - mean_squared_error: 70.4955 - val_loss: 71.7883 - val_mean_squared_error: 71.7883\n",
            "Epoch 102/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.2201 - mean_squared_error: 69.2201 - val_loss: 72.0809 - val_mean_squared_error: 72.0809\n",
            "Epoch 103/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 81.8561 - mean_squared_error: 81.8561 - val_loss: 71.7288 - val_mean_squared_error: 71.7288\n",
            "Epoch 104/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.0513 - mean_squared_error: 64.0513 - val_loss: 71.0661 - val_mean_squared_error: 71.0661\n",
            "Epoch 105/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.7932 - mean_squared_error: 67.7932 - val_loss: 72.2110 - val_mean_squared_error: 72.2110\n",
            "Epoch 106/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.8792 - mean_squared_error: 71.8792 - val_loss: 72.5166 - val_mean_squared_error: 72.5166\n",
            "Epoch 107/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94.0068 - mean_squared_error: 94.0068 - val_loss: 71.6562 - val_mean_squared_error: 71.6562\n",
            "Epoch 108/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.4445 - mean_squared_error: 70.4445 - val_loss: 72.6783 - val_mean_squared_error: 72.6783\n",
            "Epoch 109/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90.7787 - mean_squared_error: 90.7787 - val_loss: 71.5172 - val_mean_squared_error: 71.5172\n",
            "Epoch 110/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62.4994 - mean_squared_error: 62.4994 - val_loss: 71.5720 - val_mean_squared_error: 71.5720\n",
            "Epoch 111/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.4657 - mean_squared_error: 71.4657 - val_loss: 71.2355 - val_mean_squared_error: 71.2355\n",
            "Epoch 112/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.3168 - mean_squared_error: 71.3168 - val_loss: 71.3727 - val_mean_squared_error: 71.3727\n",
            "Epoch 113/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.3545 - mean_squared_error: 68.3545 - val_loss: 71.4480 - val_mean_squared_error: 71.4480\n",
            "Epoch 114/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.5757 - mean_squared_error: 69.5757 - val_loss: 71.0294 - val_mean_squared_error: 71.0294\n",
            "Epoch 115/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.7354 - mean_squared_error: 69.7354 - val_loss: 71.1910 - val_mean_squared_error: 71.1910\n",
            "Epoch 116/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.6221 - mean_squared_error: 83.6221 - val_loss: 71.6812 - val_mean_squared_error: 71.6812\n",
            "Epoch 117/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71.3257 - mean_squared_error: 71.3257 - val_loss: 71.5615 - val_mean_squared_error: 71.5615\n",
            "Epoch 118/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.1058 - mean_squared_error: 67.1058 - val_loss: 72.3083 - val_mean_squared_error: 72.3083\n",
            "Epoch 119/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.7171 - mean_squared_error: 63.7171 - val_loss: 71.2723 - val_mean_squared_error: 71.2723\n",
            "Epoch 120/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87.9969 - mean_squared_error: 87.9969 - val_loss: 71.9468 - val_mean_squared_error: 71.9468\n",
            "Epoch 121/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.9535 - mean_squared_error: 69.9535 - val_loss: 71.4861 - val_mean_squared_error: 71.4861\n",
            "Epoch 122/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.2131 - mean_squared_error: 58.2131 - val_loss: 70.9029 - val_mean_squared_error: 70.9029\n",
            "Epoch 123/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.4151 - mean_squared_error: 84.4151 - val_loss: 71.1645 - val_mean_squared_error: 71.1645\n",
            "Epoch 124/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91.3792 - mean_squared_error: 91.3792 - val_loss: 72.1798 - val_mean_squared_error: 72.1798\n",
            "Epoch 125/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 81.0915 - mean_squared_error: 81.0915 - val_loss: 71.6670 - val_mean_squared_error: 71.6670\n",
            "Epoch 126/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 74.7458 - mean_squared_error: 74.7458 - val_loss: 71.2688 - val_mean_squared_error: 71.2688\n",
            "Epoch 127/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.8678 - mean_squared_error: 102.8678 - val_loss: 72.9866 - val_mean_squared_error: 72.9866\n",
            "Epoch 128/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 79.3978 - mean_squared_error: 79.3978 - val_loss: 71.1888 - val_mean_squared_error: 71.1888\n",
            "Epoch 129/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 69.0891 - mean_squared_error: 69.0891 - val_loss: 70.8521 - val_mean_squared_error: 70.8521\n",
            "Epoch 130/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 93.4601 - mean_squared_error: 93.4601 - val_loss: 71.6624 - val_mean_squared_error: 71.6624\n",
            "Epoch 131/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.8947 - mean_squared_error: 65.8947 - val_loss: 70.6577 - val_mean_squared_error: 70.6577\n",
            "Epoch 132/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 73.9522 - mean_squared_error: 73.9522 - val_loss: 70.7496 - val_mean_squared_error: 70.7496\n",
            "Epoch 133/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.2671 - mean_squared_error: 73.2671 - val_loss: 70.8670 - val_mean_squared_error: 70.8670\n",
            "Epoch 134/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.4039 - mean_squared_error: 64.4039 - val_loss: 72.0362 - val_mean_squared_error: 72.0362\n",
            "Epoch 135/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.9752 - mean_squared_error: 60.9752 - val_loss: 71.8486 - val_mean_squared_error: 71.8486\n",
            "Epoch 136/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66.9798 - mean_squared_error: 66.9798 - val_loss: 73.6891 - val_mean_squared_error: 73.6891\n",
            "Epoch 137/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 75.2636 - mean_squared_error: 75.2636 - val_loss: 74.2973 - val_mean_squared_error: 74.2973\n",
            "Epoch 138/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.8694 - mean_squared_error: 88.8694 - val_loss: 70.8318 - val_mean_squared_error: 70.8318\n",
            "Epoch 139/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53.9688 - mean_squared_error: 53.9688 - val_loss: 70.8043 - val_mean_squared_error: 70.8043\n",
            "Epoch 140/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.2502 - mean_squared_error: 75.2502 - val_loss: 71.4619 - val_mean_squared_error: 71.4619\n",
            "Epoch 141/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.4749 - mean_squared_error: 59.4749 - val_loss: 71.2745 - val_mean_squared_error: 71.2745\n",
            "Epoch 142/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 73.6860 - mean_squared_error: 73.6860 - val_loss: 71.4760 - val_mean_squared_error: 71.4760\n",
            "Epoch 143/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 68.7973 - mean_squared_error: 68.7973 - val_loss: 71.8590 - val_mean_squared_error: 71.8590\n",
            "Epoch 144/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.6013 - mean_squared_error: 69.6013 - val_loss: 71.3918 - val_mean_squared_error: 71.3918\n",
            "Epoch 145/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.0849 - mean_squared_error: 63.0849 - val_loss: 70.8680 - val_mean_squared_error: 70.8680\n",
            "Epoch 146/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.7348 - mean_squared_error: 70.7348 - val_loss: 70.9886 - val_mean_squared_error: 70.9886\n",
            "Epoch 147/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.3075 - mean_squared_error: 63.3075 - val_loss: 72.5844 - val_mean_squared_error: 72.5844\n",
            "Epoch 148/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.3564 - mean_squared_error: 84.3564 - val_loss: 72.6912 - val_mean_squared_error: 72.6912\n",
            "Epoch 149/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.9313 - mean_squared_error: 70.9313 - val_loss: 71.7742 - val_mean_squared_error: 71.7742\n",
            "Epoch 150/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82.7304 - mean_squared_error: 82.7304 - val_loss: 72.4074 - val_mean_squared_error: 72.4074\n",
            "Epoch 151/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.5036 - mean_squared_error: 88.5036 - val_loss: 72.1164 - val_mean_squared_error: 72.1164\n",
            "Epoch 152/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.9583 - mean_squared_error: 83.9583 - val_loss: 71.0475 - val_mean_squared_error: 71.0475\n",
            "Epoch 153/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.0425 - mean_squared_error: 74.0425 - val_loss: 70.8204 - val_mean_squared_error: 70.8204\n",
            "Epoch 154/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.4762 - mean_squared_error: 60.4762 - val_loss: 71.4762 - val_mean_squared_error: 71.4762\n",
            "Epoch 155/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78.0573 - mean_squared_error: 78.0573 - val_loss: 71.2816 - val_mean_squared_error: 71.2816\n",
            "Epoch 156/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.2942 - mean_squared_error: 74.2942 - val_loss: 70.8929 - val_mean_squared_error: 70.8929\n",
            "Epoch 157/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 97.2367 - mean_squared_error: 97.2367 - val_loss: 72.4440 - val_mean_squared_error: 72.4440\n",
            "Epoch 158/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85.2071 - mean_squared_error: 85.2071 - val_loss: 70.6374 - val_mean_squared_error: 70.6374\n",
            "Epoch 159/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.2514 - mean_squared_error: 57.2514 - val_loss: 70.8078 - val_mean_squared_error: 70.8078\n",
            "Epoch 160/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.3137 - mean_squared_error: 63.3137 - val_loss: 72.0214 - val_mean_squared_error: 72.0214\n",
            "Epoch 161/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52.6711 - mean_squared_error: 52.6711 - val_loss: 70.2960 - val_mean_squared_error: 70.2960\n",
            "Epoch 162/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.7472 - mean_squared_error: 66.7472 - val_loss: 72.1206 - val_mean_squared_error: 72.1206\n",
            "Epoch 163/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.2026 - mean_squared_error: 75.2026 - val_loss: 71.1068 - val_mean_squared_error: 71.1068\n",
            "Epoch 164/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 82.8276 - mean_squared_error: 82.8276 - val_loss: 70.6680 - val_mean_squared_error: 70.6680\n",
            "Epoch 165/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 73.4862 - mean_squared_error: 73.4862 - val_loss: 70.9508 - val_mean_squared_error: 70.9508\n",
            "Epoch 166/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 73.9586 - mean_squared_error: 73.9586 - val_loss: 70.9314 - val_mean_squared_error: 70.9314\n",
            "Epoch 167/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 80.1371 - mean_squared_error: 80.1371 - val_loss: 71.1055 - val_mean_squared_error: 71.1055\n",
            "Epoch 168/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71.9101 - mean_squared_error: 71.9101 - val_loss: 71.9658 - val_mean_squared_error: 71.9658\n",
            "Epoch 169/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 58.3079 - mean_squared_error: 58.3079 - val_loss: 70.6622 - val_mean_squared_error: 70.6622\n",
            "Epoch 170/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 75.2118 - mean_squared_error: 75.2118 - val_loss: 71.8736 - val_mean_squared_error: 71.8736\n",
            "Epoch 171/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56.6410 - mean_squared_error: 56.6410 - val_loss: 71.2108 - val_mean_squared_error: 71.2108\n",
            "Epoch 172/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.7237 - mean_squared_error: 74.7237 - val_loss: 71.5158 - val_mean_squared_error: 71.5158\n",
            "Epoch 173/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.1085 - mean_squared_error: 84.1085 - val_loss: 70.1895 - val_mean_squared_error: 70.1895\n",
            "Epoch 174/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.5003 - mean_squared_error: 69.5003 - val_loss: 70.8071 - val_mean_squared_error: 70.8071\n",
            "Epoch 175/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.6160 - mean_squared_error: 65.6160 - val_loss: 70.5437 - val_mean_squared_error: 70.5437\n",
            "Epoch 176/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.1935 - mean_squared_error: 85.1935 - val_loss: 70.4742 - val_mean_squared_error: 70.4742\n",
            "Epoch 177/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.6049 - mean_squared_error: 71.6049 - val_loss: 71.4604 - val_mean_squared_error: 71.4604\n",
            "Epoch 178/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.7227 - mean_squared_error: 51.7227 - val_loss: 70.1039 - val_mean_squared_error: 70.1039\n",
            "Epoch 179/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.5724 - mean_squared_error: 63.5724 - val_loss: 70.8034 - val_mean_squared_error: 70.8034\n",
            "Epoch 180/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.7036 - mean_squared_error: 60.7036 - val_loss: 70.0193 - val_mean_squared_error: 70.0193\n",
            "Epoch 181/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.9904 - mean_squared_error: 63.9904 - val_loss: 70.8077 - val_mean_squared_error: 70.8077\n",
            "Epoch 182/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.6407 - mean_squared_error: 70.6407 - val_loss: 71.8869 - val_mean_squared_error: 71.8869\n",
            "Epoch 183/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.6350 - mean_squared_error: 66.6350 - val_loss: 70.6012 - val_mean_squared_error: 70.6012\n",
            "Epoch 184/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.7819 - mean_squared_error: 85.7819 - val_loss: 70.3930 - val_mean_squared_error: 70.3930\n",
            "Epoch 185/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.0403 - mean_squared_error: 69.0403 - val_loss: 70.0519 - val_mean_squared_error: 70.0519\n",
            "Epoch 186/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.8541 - mean_squared_error: 57.8541 - val_loss: 70.7568 - val_mean_squared_error: 70.7568\n",
            "Epoch 187/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90.2180 - mean_squared_error: 90.2180 - val_loss: 70.8474 - val_mean_squared_error: 70.8474\n",
            "Epoch 188/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.7014 - mean_squared_error: 66.7014 - val_loss: 71.2449 - val_mean_squared_error: 71.2449\n",
            "Epoch 189/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.4906 - mean_squared_error: 68.4906 - val_loss: 70.6699 - val_mean_squared_error: 70.6699\n",
            "Epoch 190/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.9523 - mean_squared_error: 65.9523 - val_loss: 69.4247 - val_mean_squared_error: 69.4247\n",
            "Epoch 191/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 74.2853 - mean_squared_error: 74.2853 - val_loss: 70.5176 - val_mean_squared_error: 70.5176\n",
            "Epoch 192/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 57.7094 - mean_squared_error: 57.7094 - val_loss: 70.5142 - val_mean_squared_error: 70.5142\n",
            "Epoch 193/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76.7431 - mean_squared_error: 76.7431 - val_loss: 70.6188 - val_mean_squared_error: 70.6188\n",
            "Epoch 194/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.4345 - mean_squared_error: 66.4345 - val_loss: 71.2001 - val_mean_squared_error: 71.2001\n",
            "Epoch 195/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 69.7473 - mean_squared_error: 69.7473 - val_loss: 70.7199 - val_mean_squared_error: 70.7199\n",
            "Epoch 196/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 71.1828 - mean_squared_error: 71.1828 - val_loss: 70.8093 - val_mean_squared_error: 70.8093\n",
            "Epoch 197/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 66.3445 - mean_squared_error: 66.3445 - val_loss: 70.2177 - val_mean_squared_error: 70.2177\n",
            "Epoch 198/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 64.6142 - mean_squared_error: 64.6142 - val_loss: 70.7373 - val_mean_squared_error: 70.7373\n",
            "Epoch 199/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 71.2483 - mean_squared_error: 71.2483 - val_loss: 70.2729 - val_mean_squared_error: 70.2729\n",
            "Epoch 200/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 69.1518 - mean_squared_error: 69.1518 - val_loss: 70.3730 - val_mean_squared_error: 70.3730\n",
            "Epoch 201/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 63.8977 - mean_squared_error: 63.8977 - val_loss: 70.2080 - val_mean_squared_error: 70.2080\n",
            "Epoch 202/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 78.9066 - mean_squared_error: 78.9066 - val_loss: 70.4368 - val_mean_squared_error: 70.4368\n",
            "Epoch 203/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 77.0485 - mean_squared_error: 77.0485 - val_loss: 70.4332 - val_mean_squared_error: 70.4332\n",
            "Epoch 204/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 54.2539 - mean_squared_error: 54.2539 - val_loss: 70.2333 - val_mean_squared_error: 70.2333\n",
            "Epoch 205/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 69.9840 - mean_squared_error: 69.9840 - val_loss: 71.5015 - val_mean_squared_error: 71.5015\n",
            "Epoch 206/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.6243 - mean_squared_error: 71.6243 - val_loss: 70.7916 - val_mean_squared_error: 70.7916\n",
            "Epoch 207/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76.8225 - mean_squared_error: 76.8225 - val_loss: 70.6068 - val_mean_squared_error: 70.6068\n",
            "Epoch 208/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76.6944 - mean_squared_error: 76.6944 - val_loss: 70.6786 - val_mean_squared_error: 70.6786\n",
            "Epoch 209/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.1974 - mean_squared_error: 72.1974 - val_loss: 70.4589 - val_mean_squared_error: 70.4589\n",
            "Epoch 210/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.6411 - mean_squared_error: 72.6411 - val_loss: 70.3223 - val_mean_squared_error: 70.3223\n",
            "Epoch 211/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76.6019 - mean_squared_error: 76.6019 - val_loss: 70.6070 - val_mean_squared_error: 70.6070\n",
            "Epoch 212/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 63.7448 - mean_squared_error: 63.7448 - val_loss: 73.6484 - val_mean_squared_error: 73.6484\n",
            "Epoch 213/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60.1836 - mean_squared_error: 60.1836 - val_loss: 70.1812 - val_mean_squared_error: 70.1812\n",
            "Epoch 214/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.5477 - mean_squared_error: 58.5477 - val_loss: 70.3020 - val_mean_squared_error: 70.3020\n",
            "Epoch 215/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.4370 - mean_squared_error: 88.4370 - val_loss: 70.3460 - val_mean_squared_error: 70.3460\n",
            "Epoch 216/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.7818 - mean_squared_error: 70.7818 - val_loss: 70.1918 - val_mean_squared_error: 70.1918\n",
            "Epoch 217/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.7822 - mean_squared_error: 69.7822 - val_loss: 70.6779 - val_mean_squared_error: 70.6779\n",
            "Epoch 218/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.1831 - mean_squared_error: 63.1831 - val_loss: 70.2727 - val_mean_squared_error: 70.2727\n",
            "Epoch 219/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 46.2131 - mean_squared_error: 46.2131 - val_loss: 71.0936 - val_mean_squared_error: 71.0936\n",
            "Epoch 220/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.2974 - mean_squared_error: 64.2974 - val_loss: 71.1966 - val_mean_squared_error: 71.1966\n",
            "Epoch 221/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.4052 - mean_squared_error: 70.4052 - val_loss: 71.2770 - val_mean_squared_error: 71.2770\n",
            "Epoch 222/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.1768 - mean_squared_error: 59.1768 - val_loss: 70.0742 - val_mean_squared_error: 70.0742\n",
            "Epoch 223/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.7258 - mean_squared_error: 65.7258 - val_loss: 70.9668 - val_mean_squared_error: 70.9668\n",
            "Epoch 224/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56.3763 - mean_squared_error: 56.3763 - val_loss: 70.4880 - val_mean_squared_error: 70.4880\n",
            "Epoch 225/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.5962 - mean_squared_error: 74.5962 - val_loss: 70.1222 - val_mean_squared_error: 70.1222\n",
            "Epoch 226/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 61.1119 - mean_squared_error: 61.1119 - val_loss: 70.8575 - val_mean_squared_error: 70.8575\n",
            "Epoch 227/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.3263 - mean_squared_error: 60.3263 - val_loss: 70.7557 - val_mean_squared_error: 70.7557\n",
            "Epoch 228/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.0274 - mean_squared_error: 63.0274 - val_loss: 70.2502 - val_mean_squared_error: 70.2502\n",
            "Epoch 229/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.4777 - mean_squared_error: 64.4777 - val_loss: 70.7003 - val_mean_squared_error: 70.7003\n",
            "Epoch 230/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.1537 - mean_squared_error: 62.1537 - val_loss: 70.9420 - val_mean_squared_error: 70.9420\n",
            "Epoch 231/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.1309 - mean_squared_error: 64.1309 - val_loss: 71.1941 - val_mean_squared_error: 71.1941\n",
            "Epoch 232/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.9762 - mean_squared_error: 84.9762 - val_loss: 70.2485 - val_mean_squared_error: 70.2485\n",
            "Epoch 233/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 72.6878 - mean_squared_error: 72.6878 - val_loss: 70.2620 - val_mean_squared_error: 70.2620\n",
            "Epoch 234/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.2920 - mean_squared_error: 68.2920 - val_loss: 69.9321 - val_mean_squared_error: 69.9321\n",
            "Epoch 235/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52.7103 - mean_squared_error: 52.7103 - val_loss: 70.2204 - val_mean_squared_error: 70.2204\n",
            "Epoch 236/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.8702 - mean_squared_error: 74.8702 - val_loss: 70.3688 - val_mean_squared_error: 70.3688\n",
            "Epoch 237/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.4015 - mean_squared_error: 72.4015 - val_loss: 70.0758 - val_mean_squared_error: 70.0758\n",
            "Epoch 238/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58.9558 - mean_squared_error: 58.9558 - val_loss: 70.4879 - val_mean_squared_error: 70.4879\n",
            "Epoch 239/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 52.2088 - mean_squared_error: 52.2088 - val_loss: 70.5245 - val_mean_squared_error: 70.5245\n",
            "Epoch 240/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 77.9804 - mean_squared_error: 77.9804 - val_loss: 70.7109 - val_mean_squared_error: 70.7109\n",
            "Epoch 241/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 51.9229 - mean_squared_error: 51.9229 - val_loss: 70.9017 - val_mean_squared_error: 70.9017\n",
            "Epoch 242/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 59.9035 - mean_squared_error: 59.9035 - val_loss: 71.6023 - val_mean_squared_error: 71.6023\n",
            "Epoch 243/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 66.1597 - mean_squared_error: 66.1597 - val_loss: 70.6731 - val_mean_squared_error: 70.6731\n",
            "Epoch 244/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.0531 - mean_squared_error: 60.0531 - val_loss: 70.7687 - val_mean_squared_error: 70.7687\n",
            "Epoch 245/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58.4236 - mean_squared_error: 58.4236 - val_loss: 70.8598 - val_mean_squared_error: 70.8598\n",
            "Epoch 246/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.1632 - mean_squared_error: 55.1632 - val_loss: 70.9034 - val_mean_squared_error: 70.9034\n",
            "Epoch 247/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.6826 - mean_squared_error: 68.6826 - val_loss: 70.6531 - val_mean_squared_error: 70.6531\n",
            "Epoch 248/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.4412 - mean_squared_error: 69.4412 - val_loss: 70.7983 - val_mean_squared_error: 70.7983\n",
            "Epoch 249/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.5076 - mean_squared_error: 70.5076 - val_loss: 70.2900 - val_mean_squared_error: 70.2900\n",
            "Epoch 250/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.8064 - mean_squared_error: 65.8064 - val_loss: 70.6423 - val_mean_squared_error: 70.6423\n",
            "Epoch 251/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.4732 - mean_squared_error: 72.4732 - val_loss: 70.1733 - val_mean_squared_error: 70.1733\n",
            "Epoch 252/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.7770 - mean_squared_error: 63.7770 - val_loss: 70.7401 - val_mean_squared_error: 70.7401\n",
            "Epoch 253/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60.5819 - mean_squared_error: 60.5819 - val_loss: 70.5347 - val_mean_squared_error: 70.5347\n",
            "Epoch 254/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.6517 - mean_squared_error: 67.6517 - val_loss: 70.4465 - val_mean_squared_error: 70.4465\n",
            "Epoch 255/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.3415 - mean_squared_error: 79.3415 - val_loss: 70.6644 - val_mean_squared_error: 70.6644\n",
            "Epoch 256/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.9805 - mean_squared_error: 67.9805 - val_loss: 70.3456 - val_mean_squared_error: 70.3456\n",
            "Epoch 257/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 79.5526 - mean_squared_error: 79.5526 - val_loss: 70.4875 - val_mean_squared_error: 70.4875\n",
            "Epoch 258/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.2093 - mean_squared_error: 57.2093 - val_loss: 70.2126 - val_mean_squared_error: 70.2126\n",
            "Epoch 259/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52.9719 - mean_squared_error: 52.9719 - val_loss: 70.0526 - val_mean_squared_error: 70.0526\n",
            "Epoch 260/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.2232 - mean_squared_error: 62.2232 - val_loss: 70.4057 - val_mean_squared_error: 70.4057\n",
            "Epoch 261/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.2100 - mean_squared_error: 63.2100 - val_loss: 70.0730 - val_mean_squared_error: 70.0730\n",
            "Epoch 262/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61.8477 - mean_squared_error: 61.8477 - val_loss: 70.3085 - val_mean_squared_error: 70.3085\n",
            "Epoch 263/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.4390 - mean_squared_error: 67.4390 - val_loss: 70.4947 - val_mean_squared_error: 70.4947\n",
            "Epoch 264/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.0055 - mean_squared_error: 64.0055 - val_loss: 70.3018 - val_mean_squared_error: 70.3018\n",
            "Epoch 265/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.2294 - mean_squared_error: 55.2294 - val_loss: 70.0629 - val_mean_squared_error: 70.0629\n",
            "Epoch 266/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.9796 - mean_squared_error: 72.9796 - val_loss: 70.2499 - val_mean_squared_error: 70.2499\n",
            "Epoch 267/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.3321 - mean_squared_error: 60.3321 - val_loss: 70.4838 - val_mean_squared_error: 70.4838\n",
            "Epoch 268/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.2659 - mean_squared_error: 59.2659 - val_loss: 70.2380 - val_mean_squared_error: 70.2380\n",
            "Epoch 269/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.0493 - mean_squared_error: 55.0493 - val_loss: 70.3188 - val_mean_squared_error: 70.3188\n",
            "Epoch 270/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 61.3412 - mean_squared_error: 61.3412 - val_loss: 70.0452 - val_mean_squared_error: 70.0452\n",
            "Epoch 271/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 74.6752 - mean_squared_error: 74.6752 - val_loss: 70.3971 - val_mean_squared_error: 70.3971\n",
            "Epoch 272/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 47.5442 - mean_squared_error: 47.5442 - val_loss: 70.0508 - val_mean_squared_error: 70.0508\n",
            "Epoch 273/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.3612 - mean_squared_error: 65.3612 - val_loss: 70.2872 - val_mean_squared_error: 70.2872\n",
            "Epoch 274/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.9660 - mean_squared_error: 67.9660 - val_loss: 69.7974 - val_mean_squared_error: 69.7974\n",
            "Epoch 275/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 78.3631 - mean_squared_error: 78.3631 - val_loss: 70.2626 - val_mean_squared_error: 70.2626\n",
            "Epoch 276/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 42.4460 - mean_squared_error: 42.4460 - val_loss: 70.2719 - val_mean_squared_error: 70.2719\n",
            "Epoch 277/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.7088 - mean_squared_error: 51.7088 - val_loss: 70.0814 - val_mean_squared_error: 70.0814\n",
            "Epoch 278/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 61.6508 - mean_squared_error: 61.6508 - val_loss: 70.5197 - val_mean_squared_error: 70.5197\n",
            "Epoch 279/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 86.9377 - mean_squared_error: 86.9377 - val_loss: 70.1173 - val_mean_squared_error: 70.1173\n",
            "Epoch 280/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 58.6706 - mean_squared_error: 58.6706 - val_loss: 69.9934 - val_mean_squared_error: 69.9934\n",
            "Epoch 281/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 70.9511 - mean_squared_error: 70.9511 - val_loss: 70.4769 - val_mean_squared_error: 70.4769\n",
            "Epoch 282/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 66.6996 - mean_squared_error: 66.6996 - val_loss: 70.4828 - val_mean_squared_error: 70.4828\n",
            "Epoch 283/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 76.5459 - mean_squared_error: 76.5459 - val_loss: 70.4486 - val_mean_squared_error: 70.4486\n",
            "Epoch 284/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.7550 - mean_squared_error: 73.7550 - val_loss: 70.3581 - val_mean_squared_error: 70.3581\n",
            "Epoch 285/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52.7657 - mean_squared_error: 52.7657 - val_loss: 70.1325 - val_mean_squared_error: 70.1325\n",
            "Epoch 286/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52.9775 - mean_squared_error: 52.9775 - val_loss: 70.1316 - val_mean_squared_error: 70.1316\n",
            "Epoch 287/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.6137 - mean_squared_error: 54.6137 - val_loss: 70.1557 - val_mean_squared_error: 70.1557\n",
            "Epoch 288/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 50.7332 - mean_squared_error: 50.7332 - val_loss: 70.2330 - val_mean_squared_error: 70.2330\n",
            "Epoch 289/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.8501 - mean_squared_error: 71.8501 - val_loss: 70.0030 - val_mean_squared_error: 70.0030\n",
            "Epoch 290/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.3728 - mean_squared_error: 66.3728 - val_loss: 70.1833 - val_mean_squared_error: 70.1833\n",
            "Epoch 291/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.7622 - mean_squared_error: 75.7622 - val_loss: 70.7858 - val_mean_squared_error: 70.7858\n",
            "Epoch 292/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.7784 - mean_squared_error: 65.7784 - val_loss: 70.2020 - val_mean_squared_error: 70.2020\n",
            "Epoch 293/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 49.8700 - mean_squared_error: 49.8700 - val_loss: 70.5218 - val_mean_squared_error: 70.5218\n",
            "Epoch 294/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 51.0630 - mean_squared_error: 51.0630 - val_loss: 70.2258 - val_mean_squared_error: 70.2258\n",
            "Epoch 295/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.8666 - mean_squared_error: 58.8666 - val_loss: 70.4924 - val_mean_squared_error: 70.4924\n",
            "Epoch 296/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52.2260 - mean_squared_error: 52.2260 - val_loss: 70.7933 - val_mean_squared_error: 70.7933\n",
            "Epoch 297/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.6711 - mean_squared_error: 54.6711 - val_loss: 70.8314 - val_mean_squared_error: 70.8314\n",
            "Epoch 298/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 46.3468 - mean_squared_error: 46.3468 - val_loss: 70.6589 - val_mean_squared_error: 70.6589\n",
            "Epoch 299/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 59.5002 - mean_squared_error: 59.5002 - val_loss: 70.0808 - val_mean_squared_error: 70.0808\n",
            "Epoch 300/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.5225 - mean_squared_error: 59.5225 - val_loss: 70.6097 - val_mean_squared_error: 70.6097\n",
            "Epoch 301/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56.0505 - mean_squared_error: 56.0505 - val_loss: 71.0059 - val_mean_squared_error: 71.0059\n",
            "Epoch 302/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53.6548 - mean_squared_error: 53.6548 - val_loss: 70.6501 - val_mean_squared_error: 70.6501\n",
            "Epoch 303/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.7004 - mean_squared_error: 55.7004 - val_loss: 70.4438 - val_mean_squared_error: 70.4438\n",
            "Epoch 304/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52.8513 - mean_squared_error: 52.8513 - val_loss: 70.5111 - val_mean_squared_error: 70.5111\n",
            "Epoch 305/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 38.4672 - mean_squared_error: 38.4672 - val_loss: 70.0353 - val_mean_squared_error: 70.0353\n",
            "Epoch 306/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 54.6965 - mean_squared_error: 54.6965 - val_loss: 70.4658 - val_mean_squared_error: 70.4658\n",
            "Epoch 307/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.3560 - mean_squared_error: 70.3560 - val_loss: 70.1161 - val_mean_squared_error: 70.1161\n",
            "Epoch 308/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66.9162 - mean_squared_error: 66.9162 - val_loss: 69.9727 - val_mean_squared_error: 69.9727\n",
            "Epoch 309/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 63.5581 - mean_squared_error: 63.5581 - val_loss: 69.8832 - val_mean_squared_error: 69.8832\n",
            "Epoch 310/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53.7270 - mean_squared_error: 53.7270 - val_loss: 69.9595 - val_mean_squared_error: 69.9595\n",
            "Epoch 311/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56.2172 - mean_squared_error: 56.2172 - val_loss: 70.2570 - val_mean_squared_error: 70.2570\n",
            "Epoch 312/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52.5001 - mean_squared_error: 52.5001 - val_loss: 69.8698 - val_mean_squared_error: 69.8698\n",
            "Epoch 313/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.9820 - mean_squared_error: 51.9820 - val_loss: 70.0613 - val_mean_squared_error: 70.0613\n",
            "Epoch 314/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.6811 - mean_squared_error: 55.6811 - val_loss: 69.8240 - val_mean_squared_error: 69.8240\n",
            "Epoch 315/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 46.4604 - mean_squared_error: 46.4604 - val_loss: 70.2172 - val_mean_squared_error: 70.2172\n",
            "Epoch 316/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 57.8850 - mean_squared_error: 57.8850 - val_loss: 70.0833 - val_mean_squared_error: 70.0833\n",
            "Epoch 317/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 70.9928 - mean_squared_error: 70.9928 - val_loss: 69.9416 - val_mean_squared_error: 69.9416\n",
            "Epoch 318/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 74.9355 - mean_squared_error: 74.9355 - val_loss: 70.3321 - val_mean_squared_error: 70.3321\n",
            "Epoch 319/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 80.7151 - mean_squared_error: 80.7151 - val_loss: 70.0031 - val_mean_squared_error: 70.0031\n",
            "Epoch 320/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 75.8733 - mean_squared_error: 75.8733 - val_loss: 70.1018 - val_mean_squared_error: 70.1018\n",
            "Epoch 321/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 58.4561 - mean_squared_error: 58.4561 - val_loss: 70.3631 - val_mean_squared_error: 70.3631\n",
            "Epoch 322/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 54.6957 - mean_squared_error: 54.6957 - val_loss: 70.2366 - val_mean_squared_error: 70.2366\n",
            "Epoch 323/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61.5967 - mean_squared_error: 61.5967 - val_loss: 69.8987 - val_mean_squared_error: 69.8987\n",
            "Epoch 324/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 76.2242 - mean_squared_error: 76.2242 - val_loss: 69.8297 - val_mean_squared_error: 69.8297\n",
            "Epoch 325/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 45.2437 - mean_squared_error: 45.2437 - val_loss: 70.2046 - val_mean_squared_error: 70.2046\n",
            "Epoch 326/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 59.3376 - mean_squared_error: 59.3376 - val_loss: 70.2460 - val_mean_squared_error: 70.2460\n",
            "Epoch 327/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.6489 - mean_squared_error: 65.6489 - val_loss: 70.5890 - val_mean_squared_error: 70.5890\n",
            "Epoch 328/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.0248 - mean_squared_error: 62.0248 - val_loss: 70.1339 - val_mean_squared_error: 70.1339\n",
            "Epoch 329/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 44.1660 - mean_squared_error: 44.1660 - val_loss: 70.2801 - val_mean_squared_error: 70.2801\n",
            "Epoch 330/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.2882 - mean_squared_error: 57.2882 - val_loss: 70.1439 - val_mean_squared_error: 70.1439\n",
            "Epoch 331/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.4484 - mean_squared_error: 41.4484 - val_loss: 70.0247 - val_mean_squared_error: 70.0247\n",
            "Epoch 332/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.4059 - mean_squared_error: 60.4059 - val_loss: 70.1255 - val_mean_squared_error: 70.1255\n",
            "Epoch 333/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.4835 - mean_squared_error: 68.4835 - val_loss: 70.5084 - val_mean_squared_error: 70.5084\n",
            "Epoch 334/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 49.0091 - mean_squared_error: 49.0091 - val_loss: 69.9080 - val_mean_squared_error: 69.9080\n",
            "Epoch 335/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.7313 - mean_squared_error: 67.7313 - val_loss: 70.1395 - val_mean_squared_error: 70.1395\n",
            "Epoch 336/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76.4143 - mean_squared_error: 76.4143 - val_loss: 69.8636 - val_mean_squared_error: 69.8636\n",
            "Epoch 337/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.6618 - mean_squared_error: 66.6618 - val_loss: 69.7908 - val_mean_squared_error: 69.7908\n",
            "Epoch 338/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.5544 - mean_squared_error: 66.5544 - val_loss: 70.1794 - val_mean_squared_error: 70.1794\n",
            "Epoch 339/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.9415 - mean_squared_error: 70.9415 - val_loss: 70.2586 - val_mean_squared_error: 70.2586\n",
            "Epoch 340/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 54.1082 - mean_squared_error: 54.1082 - val_loss: 69.9311 - val_mean_squared_error: 69.9311\n",
            "Epoch 341/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.7812 - mean_squared_error: 60.7812 - val_loss: 70.4745 - val_mean_squared_error: 70.4745\n",
            "Epoch 342/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 47.3565 - mean_squared_error: 47.3565 - val_loss: 69.8239 - val_mean_squared_error: 69.8239\n",
            "Epoch 343/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.9062 - mean_squared_error: 69.9062 - val_loss: 69.4925 - val_mean_squared_error: 69.4925\n",
            "Epoch 344/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.5069 - mean_squared_error: 67.5069 - val_loss: 69.6183 - val_mean_squared_error: 69.6183\n",
            "Epoch 345/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.3704 - mean_squared_error: 70.3704 - val_loss: 69.7585 - val_mean_squared_error: 69.7585\n",
            "Epoch 346/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63.3524 - mean_squared_error: 63.3524 - val_loss: 70.0621 - val_mean_squared_error: 70.0621\n",
            "Epoch 347/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.7818 - mean_squared_error: 65.7818 - val_loss: 70.2789 - val_mean_squared_error: 70.2789\n",
            "Epoch 348/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.3941 - mean_squared_error: 60.3941 - val_loss: 70.1343 - val_mean_squared_error: 70.1343\n",
            "Epoch 349/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 61.9023 - mean_squared_error: 61.9023 - val_loss: 70.0558 - val_mean_squared_error: 70.0558\n",
            "Epoch 350/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.7239 - mean_squared_error: 63.7239 - val_loss: 70.2402 - val_mean_squared_error: 70.2402\n",
            "Epoch 351/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.5571 - mean_squared_error: 62.5571 - val_loss: 70.2289 - val_mean_squared_error: 70.2289\n",
            "Epoch 352/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.5105 - mean_squared_error: 59.5105 - val_loss: 69.8715 - val_mean_squared_error: 69.8715\n",
            "Epoch 353/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.0980 - mean_squared_error: 60.0980 - val_loss: 69.9173 - val_mean_squared_error: 69.9173\n",
            "Epoch 354/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.7178 - mean_squared_error: 58.7178 - val_loss: 69.9445 - val_mean_squared_error: 69.9445\n",
            "Epoch 355/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.7657 - mean_squared_error: 73.7657 - val_loss: 70.2573 - val_mean_squared_error: 70.2573\n",
            "Epoch 356/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 64.7644 - mean_squared_error: 64.7644 - val_loss: 69.9053 - val_mean_squared_error: 69.9053\n",
            "Epoch 357/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 64.4142 - mean_squared_error: 64.4142 - val_loss: 69.8841 - val_mean_squared_error: 69.8841\n",
            "Epoch 358/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 66.8459 - mean_squared_error: 66.8459 - val_loss: 69.8292 - val_mean_squared_error: 69.8292\n",
            "Epoch 359/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.7411 - mean_squared_error: 68.7411 - val_loss: 70.3560 - val_mean_squared_error: 70.3560\n",
            "Epoch 360/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 59.8712 - mean_squared_error: 59.8712 - val_loss: 70.0105 - val_mean_squared_error: 70.0105\n",
            "Epoch 361/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 52.7101 - mean_squared_error: 52.7101 - val_loss: 70.1522 - val_mean_squared_error: 70.1522\n",
            "Epoch 362/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58.3678 - mean_squared_error: 58.3678 - val_loss: 69.9784 - val_mean_squared_error: 69.9784\n",
            "Epoch 363/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.4827 - mean_squared_error: 63.4827 - val_loss: 70.2039 - val_mean_squared_error: 70.2039\n",
            "Epoch 364/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52.7441 - mean_squared_error: 52.7441 - val_loss: 70.0310 - val_mean_squared_error: 70.0310\n",
            "Epoch 365/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.1432 - mean_squared_error: 70.1432 - val_loss: 70.2395 - val_mean_squared_error: 70.2395\n",
            "Epoch 366/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.7499 - mean_squared_error: 66.7499 - val_loss: 70.2504 - val_mean_squared_error: 70.2504\n",
            "Epoch 367/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43.1306 - mean_squared_error: 43.1306 - val_loss: 70.5817 - val_mean_squared_error: 70.5817\n",
            "Epoch 368/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60.9268 - mean_squared_error: 60.9268 - val_loss: 70.3279 - val_mean_squared_error: 70.3279\n",
            "Epoch 369/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.5836 - mean_squared_error: 66.5836 - val_loss: 70.4043 - val_mean_squared_error: 70.4043\n",
            "Epoch 370/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.2684 - mean_squared_error: 60.2684 - val_loss: 70.2339 - val_mean_squared_error: 70.2339\n",
            "Epoch 371/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.5821 - mean_squared_error: 60.5821 - val_loss: 70.1159 - val_mean_squared_error: 70.1159\n",
            "Epoch 372/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57.9574 - mean_squared_error: 57.9574 - val_loss: 69.7851 - val_mean_squared_error: 69.7851\n",
            "Epoch 373/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 51.5464 - mean_squared_error: 51.5464 - val_loss: 70.1249 - val_mean_squared_error: 70.1249\n",
            "Epoch 374/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62.9333 - mean_squared_error: 62.9333 - val_loss: 69.9812 - val_mean_squared_error: 69.9812\n",
            "Epoch 375/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52.0841 - mean_squared_error: 52.0841 - val_loss: 69.7277 - val_mean_squared_error: 69.7277\n",
            "Epoch 376/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60.3783 - mean_squared_error: 60.3783 - val_loss: 69.9008 - val_mean_squared_error: 69.9008\n",
            "Epoch 377/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56.8213 - mean_squared_error: 56.8213 - val_loss: 70.1950 - val_mean_squared_error: 70.1950\n",
            "Epoch 378/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.0834 - mean_squared_error: 68.0834 - val_loss: 69.7978 - val_mean_squared_error: 69.7978\n",
            "Epoch 379/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 61.0503 - mean_squared_error: 61.0503 - val_loss: 69.8580 - val_mean_squared_error: 69.8580\n",
            "Epoch 380/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 49.1656 - mean_squared_error: 49.1656 - val_loss: 70.1128 - val_mean_squared_error: 70.1128\n",
            "Epoch 381/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 61.3815 - mean_squared_error: 61.3815 - val_loss: 69.8306 - val_mean_squared_error: 69.8306\n",
            "Epoch 382/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.6562 - mean_squared_error: 58.6562 - val_loss: 70.1155 - val_mean_squared_error: 70.1155\n",
            "Epoch 383/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.2952 - mean_squared_error: 51.2952 - val_loss: 69.0587 - val_mean_squared_error: 69.0587\n",
            "Epoch 384/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.5125 - mean_squared_error: 54.5125 - val_loss: 70.7109 - val_mean_squared_error: 70.7109\n",
            "Epoch 385/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.2786 - mean_squared_error: 67.2786 - val_loss: 69.9763 - val_mean_squared_error: 69.9763\n",
            "Epoch 386/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.2368 - mean_squared_error: 48.2368 - val_loss: 70.3202 - val_mean_squared_error: 70.3202\n",
            "Epoch 387/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.2018 - mean_squared_error: 55.2018 - val_loss: 70.4199 - val_mean_squared_error: 70.4199\n",
            "Epoch 388/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.1796 - mean_squared_error: 75.1796 - val_loss: 70.3698 - val_mean_squared_error: 70.3698\n",
            "Epoch 389/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.3115 - mean_squared_error: 68.3115 - val_loss: 70.5319 - val_mean_squared_error: 70.5319\n",
            "Epoch 390/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.9636 - mean_squared_error: 70.9636 - val_loss: 69.6966 - val_mean_squared_error: 69.6966\n",
            "Epoch 391/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56.1642 - mean_squared_error: 56.1642 - val_loss: 69.8653 - val_mean_squared_error: 69.8653\n",
            "Epoch 392/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 53.7653 - mean_squared_error: 53.7653 - val_loss: 69.9044 - val_mean_squared_error: 69.9044\n",
            "Epoch 393/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.9947 - mean_squared_error: 68.9947 - val_loss: 69.7407 - val_mean_squared_error: 69.7407\n",
            "Epoch 394/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 54.1071 - mean_squared_error: 54.1071 - val_loss: 69.7149 - val_mean_squared_error: 69.7149\n",
            "Epoch 395/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 60.5951 - mean_squared_error: 60.5951 - val_loss: 69.6078 - val_mean_squared_error: 69.6078\n",
            "Epoch 396/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 70.9940 - mean_squared_error: 70.9940 - val_loss: 70.0033 - val_mean_squared_error: 70.0033\n",
            "Epoch 397/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48.1514 - mean_squared_error: 48.1514 - val_loss: 69.6176 - val_mean_squared_error: 69.6176\n",
            "Epoch 398/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 52.1774 - mean_squared_error: 52.1774 - val_loss: 70.3278 - val_mean_squared_error: 70.3278\n",
            "Epoch 399/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 59.1976 - mean_squared_error: 59.1976 - val_loss: 70.2969 - val_mean_squared_error: 70.2969\n",
            "Epoch 400/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60.9381 - mean_squared_error: 60.9381 - val_loss: 70.3017 - val_mean_squared_error: 70.3017\n",
            "Epoch 401/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.3563 - mean_squared_error: 59.3563 - val_loss: 69.8518 - val_mean_squared_error: 69.8518\n",
            "Epoch 402/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63.1796 - mean_squared_error: 63.1796 - val_loss: 69.8558 - val_mean_squared_error: 69.8558\n",
            "Epoch 403/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 47.2066 - mean_squared_error: 47.2066 - val_loss: 70.1262 - val_mean_squared_error: 70.1262\n",
            "Epoch 404/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.7987 - mean_squared_error: 67.7987 - val_loss: 69.8504 - val_mean_squared_error: 69.8504\n",
            "Epoch 405/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 68.7241 - mean_squared_error: 68.7241 - val_loss: 69.7520 - val_mean_squared_error: 69.7520\n",
            "Epoch 406/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 59.7307 - mean_squared_error: 59.7307 - val_loss: 69.6310 - val_mean_squared_error: 69.6310\n",
            "Epoch 407/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 47.6865 - mean_squared_error: 47.6865 - val_loss: 69.7532 - val_mean_squared_error: 69.7532\n",
            "Epoch 408/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48.6997 - mean_squared_error: 48.6997 - val_loss: 69.6875 - val_mean_squared_error: 69.6875\n",
            "Epoch 409/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 45.1668 - mean_squared_error: 45.1668 - val_loss: 69.7816 - val_mean_squared_error: 69.7816\n",
            "Epoch 410/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57.7639 - mean_squared_error: 57.7639 - val_loss: 69.6473 - val_mean_squared_error: 69.6473\n",
            "Epoch 411/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61.2310 - mean_squared_error: 61.2310 - val_loss: 70.0191 - val_mean_squared_error: 70.0191\n",
            "Epoch 412/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.3768 - mean_squared_error: 57.3768 - val_loss: 69.7486 - val_mean_squared_error: 69.7486\n",
            "Epoch 413/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56.3475 - mean_squared_error: 56.3475 - val_loss: 69.5466 - val_mean_squared_error: 69.5466\n",
            "Epoch 414/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 63.3901 - mean_squared_error: 63.3901 - val_loss: 69.6516 - val_mean_squared_error: 69.6516\n",
            "Epoch 415/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.7416 - mean_squared_error: 64.7416 - val_loss: 69.9840 - val_mean_squared_error: 69.9840\n",
            "Epoch 416/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.0168 - mean_squared_error: 58.0168 - val_loss: 69.7092 - val_mean_squared_error: 69.7092\n",
            "Epoch 417/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.9179 - mean_squared_error: 51.9179 - val_loss: 69.7265 - val_mean_squared_error: 69.7265\n",
            "Epoch 418/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.4652 - mean_squared_error: 66.4652 - val_loss: 70.1195 - val_mean_squared_error: 70.1195\n",
            "Epoch 419/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.7063 - mean_squared_error: 55.7063 - val_loss: 70.1163 - val_mean_squared_error: 70.1163\n",
            "Epoch 420/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 80.6414 - mean_squared_error: 80.6414 - val_loss: 70.2015 - val_mean_squared_error: 70.2015\n",
            "Epoch 421/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.2307 - mean_squared_error: 54.2307 - val_loss: 69.5400 - val_mean_squared_error: 69.5400\n",
            "Epoch 422/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.2418 - mean_squared_error: 59.2418 - val_loss: 69.2736 - val_mean_squared_error: 69.2736\n",
            "Epoch 423/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.7510 - mean_squared_error: 62.7510 - val_loss: 69.6287 - val_mean_squared_error: 69.6287\n",
            "Epoch 424/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.0864 - mean_squared_error: 67.0864 - val_loss: 69.9105 - val_mean_squared_error: 69.9105\n",
            "Epoch 425/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 50.2054 - mean_squared_error: 50.2054 - val_loss: 69.5929 - val_mean_squared_error: 69.5929\n",
            "Epoch 426/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 69.0859 - mean_squared_error: 69.0859 - val_loss: 70.2970 - val_mean_squared_error: 70.2970\n",
            "Epoch 427/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 51.0947 - mean_squared_error: 51.0947 - val_loss: 70.5419 - val_mean_squared_error: 70.5419\n",
            "Epoch 428/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 58.2154 - mean_squared_error: 58.2154 - val_loss: 70.3592 - val_mean_squared_error: 70.3592\n",
            "Epoch 429/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 40.8198 - mean_squared_error: 40.8198 - val_loss: 70.3057 - val_mean_squared_error: 70.3057\n",
            "Epoch 430/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 55.3064 - mean_squared_error: 55.3064 - val_loss: 70.2559 - val_mean_squared_error: 70.2559\n",
            "Epoch 431/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 51.9704 - mean_squared_error: 51.9704 - val_loss: 70.2291 - val_mean_squared_error: 70.2291\n",
            "Epoch 432/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 60.8178 - mean_squared_error: 60.8178 - val_loss: 70.4165 - val_mean_squared_error: 70.4165\n",
            "Epoch 433/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 56.5344 - mean_squared_error: 56.5344 - val_loss: 69.9011 - val_mean_squared_error: 69.9011\n",
            "Epoch 434/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.7712 - mean_squared_error: 51.7712 - val_loss: 70.0907 - val_mean_squared_error: 70.0907\n",
            "Epoch 435/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 43.2535 - mean_squared_error: 43.2535 - val_loss: 70.2952 - val_mean_squared_error: 70.2952\n",
            "Epoch 436/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 44.8986 - mean_squared_error: 44.8986 - val_loss: 70.1418 - val_mean_squared_error: 70.1418\n",
            "Epoch 437/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.4875 - mean_squared_error: 54.4875 - val_loss: 70.0231 - val_mean_squared_error: 70.0231\n",
            "Epoch 438/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53.6075 - mean_squared_error: 53.6075 - val_loss: 70.5716 - val_mean_squared_error: 70.5716\n",
            "Epoch 439/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.6663 - mean_squared_error: 58.6663 - val_loss: 70.2875 - val_mean_squared_error: 70.2875\n",
            "Epoch 440/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.5914 - mean_squared_error: 62.5914 - val_loss: 70.5830 - val_mean_squared_error: 70.5830\n",
            "Epoch 441/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.1246 - mean_squared_error: 48.1246 - val_loss: 70.2778 - val_mean_squared_error: 70.2778\n",
            "Epoch 442/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.4037 - mean_squared_error: 64.4037 - val_loss: 70.0741 - val_mean_squared_error: 70.0741\n",
            "Epoch 443/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.6887 - mean_squared_error: 55.6887 - val_loss: 69.8948 - val_mean_squared_error: 69.8948\n",
            "Epoch 444/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 46.5496 - mean_squared_error: 46.5496 - val_loss: 70.2868 - val_mean_squared_error: 70.2868\n",
            "Epoch 445/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.5655 - mean_squared_error: 70.5655 - val_loss: 70.3102 - val_mean_squared_error: 70.3102\n",
            "Epoch 446/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 51.7645 - mean_squared_error: 51.7645 - val_loss: 70.3200 - val_mean_squared_error: 70.3200\n",
            "Epoch 447/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 45.1744 - mean_squared_error: 45.1744 - val_loss: 69.9584 - val_mean_squared_error: 69.9584\n",
            "Epoch 448/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.0887 - mean_squared_error: 65.0887 - val_loss: 70.3153 - val_mean_squared_error: 70.3153\n",
            "Epoch 449/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 72.2915 - mean_squared_error: 72.2915 - val_loss: 69.9575 - val_mean_squared_error: 69.9575\n",
            "Epoch 450/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.6266 - mean_squared_error: 72.6266 - val_loss: 70.2591 - val_mean_squared_error: 70.2591\n",
            "Epoch 451/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 50.5290 - mean_squared_error: 50.5290 - val_loss: 70.3690 - val_mean_squared_error: 70.3690\n",
            "Epoch 452/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 47.0869 - mean_squared_error: 47.0869 - val_loss: 70.7668 - val_mean_squared_error: 70.7668\n",
            "Epoch 453/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 41.7455 - mean_squared_error: 41.7455 - val_loss: 70.2379 - val_mean_squared_error: 70.2379\n",
            "Epoch 454/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.9558 - mean_squared_error: 63.9558 - val_loss: 70.0420 - val_mean_squared_error: 70.0420\n",
            "Epoch 455/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56.4173 - mean_squared_error: 56.4173 - val_loss: 69.9179 - val_mean_squared_error: 69.9179\n",
            "Epoch 456/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52.4382 - mean_squared_error: 52.4382 - val_loss: 70.0025 - val_mean_squared_error: 70.0025\n",
            "Epoch 457/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.8101 - mean_squared_error: 60.8101 - val_loss: 70.5933 - val_mean_squared_error: 70.5933\n",
            "Epoch 458/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.3018 - mean_squared_error: 55.3018 - val_loss: 70.1428 - val_mean_squared_error: 70.1428\n",
            "Epoch 459/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 66.4735 - mean_squared_error: 66.4735 - val_loss: 70.3694 - val_mean_squared_error: 70.3694\n",
            "Epoch 460/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 73.0359 - mean_squared_error: 73.0359 - val_loss: 70.4836 - val_mean_squared_error: 70.4836\n",
            "Epoch 461/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.6274 - mean_squared_error: 64.6274 - val_loss: 69.8574 - val_mean_squared_error: 69.8574\n",
            "Epoch 462/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61.3197 - mean_squared_error: 61.3197 - val_loss: 70.6943 - val_mean_squared_error: 70.6943\n",
            "Epoch 463/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 63.6490 - mean_squared_error: 63.6490 - val_loss: 69.8638 - val_mean_squared_error: 69.8638\n",
            "Epoch 464/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 63.0213 - mean_squared_error: 63.0213 - val_loss: 70.2278 - val_mean_squared_error: 70.2278\n",
            "Epoch 465/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 73.0393 - mean_squared_error: 73.0393 - val_loss: 69.9455 - val_mean_squared_error: 69.9455\n",
            "Epoch 466/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 42.3012 - mean_squared_error: 42.3012 - val_loss: 69.6443 - val_mean_squared_error: 69.6443\n",
            "Epoch 467/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 83.8531 - mean_squared_error: 83.8531 - val_loss: 69.8939 - val_mean_squared_error: 69.8939\n",
            "Epoch 468/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 63.9575 - mean_squared_error: 63.9575 - val_loss: 69.7094 - val_mean_squared_error: 69.7094\n",
            "Epoch 469/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48.0602 - mean_squared_error: 48.0602 - val_loss: 70.1387 - val_mean_squared_error: 70.1387\n",
            "Epoch 470/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 49.2569 - mean_squared_error: 49.2569 - val_loss: 68.9392 - val_mean_squared_error: 68.9392\n",
            "Epoch 471/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.5508 - mean_squared_error: 59.5508 - val_loss: 69.9307 - val_mean_squared_error: 69.9307\n",
            "Epoch 472/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.2345 - mean_squared_error: 57.2345 - val_loss: 69.3421 - val_mean_squared_error: 69.3421\n",
            "Epoch 473/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.1030 - mean_squared_error: 66.1030 - val_loss: 69.6253 - val_mean_squared_error: 69.6253\n",
            "Epoch 474/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.0680 - mean_squared_error: 64.0680 - val_loss: 69.9512 - val_mean_squared_error: 69.9512\n",
            "Epoch 475/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.0877 - mean_squared_error: 58.0877 - val_loss: 69.8032 - val_mean_squared_error: 69.8032\n",
            "Epoch 476/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 45.7570 - mean_squared_error: 45.7570 - val_loss: 69.6907 - val_mean_squared_error: 69.6907\n",
            "Epoch 477/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.9004 - mean_squared_error: 65.9004 - val_loss: 70.1804 - val_mean_squared_error: 70.1804\n",
            "Epoch 478/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 57.0457 - mean_squared_error: 57.0457 - val_loss: 69.9957 - val_mean_squared_error: 69.9957\n",
            "Epoch 479/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.0340 - mean_squared_error: 55.0340 - val_loss: 69.8130 - val_mean_squared_error: 69.8130\n",
            "Epoch 480/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 50.6200 - mean_squared_error: 50.6200 - val_loss: 69.6882 - val_mean_squared_error: 69.6882\n",
            "Epoch 481/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 44.1744 - mean_squared_error: 44.1744 - val_loss: 69.9504 - val_mean_squared_error: 69.9504\n",
            "Epoch 482/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.4217 - mean_squared_error: 63.4217 - val_loss: 69.8878 - val_mean_squared_error: 69.8878\n",
            "Epoch 483/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56.3837 - mean_squared_error: 56.3837 - val_loss: 69.7413 - val_mean_squared_error: 69.7413\n",
            "Epoch 484/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56.9749 - mean_squared_error: 56.9749 - val_loss: 69.5883 - val_mean_squared_error: 69.5883\n",
            "Epoch 485/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 55.7751 - mean_squared_error: 55.7751 - val_loss: 70.4295 - val_mean_squared_error: 70.4295\n",
            "Epoch 486/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69.0343 - mean_squared_error: 69.0343 - val_loss: 69.7090 - val_mean_squared_error: 69.7090\n",
            "Epoch 487/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 76.6227 - mean_squared_error: 76.6227 - val_loss: 69.8604 - val_mean_squared_error: 69.8604\n",
            "Epoch 488/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.0133 - mean_squared_error: 60.0133 - val_loss: 69.8027 - val_mean_squared_error: 69.8027\n",
            "Epoch 489/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.1617 - mean_squared_error: 51.1617 - val_loss: 69.9455 - val_mean_squared_error: 69.9455\n",
            "Epoch 490/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71.9189 - mean_squared_error: 71.9189 - val_loss: 70.5715 - val_mean_squared_error: 70.5715\n",
            "Epoch 491/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.9199 - mean_squared_error: 67.9199 - val_loss: 70.3162 - val_mean_squared_error: 70.3162\n",
            "Epoch 492/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.9647 - mean_squared_error: 64.9647 - val_loss: 69.9223 - val_mean_squared_error: 69.9223\n",
            "Epoch 493/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.8583 - mean_squared_error: 64.8583 - val_loss: 69.9197 - val_mean_squared_error: 69.9197\n",
            "Epoch 494/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57.2873 - mean_squared_error: 57.2873 - val_loss: 70.5521 - val_mean_squared_error: 70.5521\n",
            "Epoch 495/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.6321 - mean_squared_error: 70.6321 - val_loss: 70.2043 - val_mean_squared_error: 70.2043\n",
            "Epoch 496/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 65.1112 - mean_squared_error: 65.1112 - val_loss: 70.2596 - val_mean_squared_error: 70.2596\n",
            "Epoch 497/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 77.5510 - mean_squared_error: 77.5510 - val_loss: 70.7434 - val_mean_squared_error: 70.7434\n",
            "Epoch 498/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 61.0477 - mean_squared_error: 61.0477 - val_loss: 69.5158 - val_mean_squared_error: 69.5158\n",
            "Epoch 499/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 48.0296 - mean_squared_error: 48.0296 - val_loss: 69.3829 - val_mean_squared_error: 69.3829\n",
            "Epoch 500/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 52.2654 - mean_squared_error: 52.2654 - val_loss: 69.5695 - val_mean_squared_error: 69.5695\n",
            "Epoch 501/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.4872 - mean_squared_error: 55.4872 - val_loss: 69.5313 - val_mean_squared_error: 69.5313\n",
            "Epoch 502/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.9670 - mean_squared_error: 60.9670 - val_loss: 69.9937 - val_mean_squared_error: 69.9937\n",
            "Epoch 503/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.5746 - mean_squared_error: 70.5746 - val_loss: 69.2435 - val_mean_squared_error: 69.2435\n",
            "Epoch 504/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.4527 - mean_squared_error: 74.4527 - val_loss: 69.5905 - val_mean_squared_error: 69.5905\n",
            "Epoch 505/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.8186 - mean_squared_error: 59.8186 - val_loss: 69.4656 - val_mean_squared_error: 69.4656\n",
            "Epoch 506/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 72.8899 - mean_squared_error: 72.8899 - val_loss: 69.4319 - val_mean_squared_error: 69.4319\n",
            "Epoch 507/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 54.3547 - mean_squared_error: 54.3547 - val_loss: 69.3135 - val_mean_squared_error: 69.3135\n",
            "Epoch 508/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 54.8532 - mean_squared_error: 54.8532 - val_loss: 69.5533 - val_mean_squared_error: 69.5533\n",
            "Epoch 509/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.9358 - mean_squared_error: 67.9358 - val_loss: 69.5620 - val_mean_squared_error: 69.5620\n",
            "Epoch 510/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.3517 - mean_squared_error: 55.3517 - val_loss: 69.4899 - val_mean_squared_error: 69.4899\n",
            "Epoch 511/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.5747 - mean_squared_error: 54.5747 - val_loss: 69.5743 - val_mean_squared_error: 69.5743\n",
            "Epoch 512/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43.2381 - mean_squared_error: 43.2381 - val_loss: 69.3500 - val_mean_squared_error: 69.3500\n",
            "Epoch 513/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.2929 - mean_squared_error: 67.2929 - val_loss: 69.8864 - val_mean_squared_error: 69.8864\n",
            "Epoch 514/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.3545 - mean_squared_error: 65.3545 - val_loss: 69.4770 - val_mean_squared_error: 69.4770\n",
            "Epoch 515/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61.0628 - mean_squared_error: 61.0628 - val_loss: 69.5255 - val_mean_squared_error: 69.5255\n",
            "Epoch 516/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 53.8668 - mean_squared_error: 53.8668 - val_loss: 69.8878 - val_mean_squared_error: 69.8878\n",
            "Epoch 517/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.9195 - mean_squared_error: 57.9195 - val_loss: 69.9504 - val_mean_squared_error: 69.9504\n",
            "Epoch 518/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.0164 - mean_squared_error: 67.0164 - val_loss: 69.3578 - val_mean_squared_error: 69.3578\n",
            "Epoch 519/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57.8320 - mean_squared_error: 57.8320 - val_loss: 69.3669 - val_mean_squared_error: 69.3669\n",
            "Epoch 520/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 72.5423 - mean_squared_error: 72.5423 - val_loss: 69.3393 - val_mean_squared_error: 69.3393\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 81.88303113445454\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load dataset, skipping lines starting with '#'\n",
        "df = pd.read_csv(\"annotated_light_tracking.csv\", comment=\"#\")\n",
        "# The first three lines are for header\n",
        "df = df.iloc[2:]\n",
        "# Strip any extra whitespace from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Convert the '_time' column to datetime if it's not already\n",
        "df['_time'] = pd.to_datetime(df['_time'])\n",
        "\n",
        "# Feature engineering: Extract hour from the '_time' column\n",
        "df['hour'] = df['_time'].dt.hour\n",
        "\n",
        "# Rename and select required columns\n",
        "df = df.rename(columns={\"position\": \"room\", \"_value\": \"light_level\"})\n",
        "df = df[['hour', 'room', 'light_level']]\n",
        "\n",
        "# Shift light level to create the target for the next time step\n",
        "df['next_light_level'] = df.groupby('room')['light_level'].shift(-1)\n",
        "df = df.dropna(subset=['next_light_level'])\n",
        "df = df.groupby('room').apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n",
        "print(df[1_30])\n",
        "\n",
        "# One-hot encode the 'room' column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_rooms = encoder.fit_transform(df[['room']])\n",
        "room_columns = encoder.categories_[0]  # Room categories (e.g., balcony, bedroom, living_room)\n",
        "encoded_rooms_df = pd.DataFrame(encoded_rooms, columns=room_columns)\n",
        "\n",
        "# Merge encoded rooms with the original DataFrame\n",
        "df = pd.concat([df.reset_index(drop=True), encoded_rooms_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[['hour'] + list(room_columns) + ['light_level']]\n",
        "y = df['next_light_level']\n",
        "\n",
        "# Normalize the features (important for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the Neural Network model using Keras\n",
        "model = keras.Sequential([\n",
        "    layers.InputLayer(input_shape=(X_train.shape[1],)),  # Input layer with shape of features\n",
        "    layers.Dense(128, activation='relu'),  # First hidden layer with more units\n",
        "    layers.Dropout(0.3),  # Dropout for regularization\n",
        "    layers.Dense(64, activation='relu'),  # Second hidden layer with more units\n",
        "    layers.Dropout(0.3),  # Dropout for regularization\n",
        "    layers.Dense(32, activation='relu'),  # Third hidden layer\n",
        "    layers.Dense(1)  # Output layer (single regression value)\n",
        "])\n",
        "\n",
        "# Compile the model with the Adam optimizer and learning rate decay\n",
        "initial_lr = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=initial_lr,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_squared_error'])\n",
        "\n",
        "#\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=520, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Save the trained model and encoder\n",
        "model.save(\"neural_network_model.h5\")\n",
        "encoder_path = \"room_encoder.npy\"\n",
        "np.save(encoder_path, encoder)  # Save encoder for future predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load dataset, skipping lines starting with '#'\n",
        "df = pd.read_csv(\"annotated_light_tracking.csv\", comment=\"#\")\n",
        "# The first three lines are for header\n",
        "df = df.iloc[2:]\n",
        "# Strip any extra whitespace from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Convert the '_time' column to datetime if it's not already\n",
        "df['_time'] = pd.to_datetime(df['_time'])\n",
        "\n",
        "# Feature engineering: Extract hour from the '_time' column\n",
        "df['hour'] = df['_time'].dt.hour\n",
        "\n",
        "# Rename and select required columns\n",
        "df = df.rename(columns={\"position\": \"room\", \"_value\": \"light_level\"})\n",
        "df = df[['hour', 'room', 'light_level']]\n",
        "\n",
        "# Shift light level to create the target for the next time step\n",
        "df['next_light_level'] = df.groupby('room')['light_level'].shift(-1)\n",
        "\n",
        "# Create lagged light features\n",
        "df['light_level_lag_1'] = df.groupby('room')['light_level'].shift(1)\n",
        "df['light_level_lag_2'] = df.groupby('room')['light_level'].shift(2)\n",
        "df['light_level_lag_3'] = df.groupby('room')['light_level'].shift(3)\n",
        "\n",
        "# Drop rows with missing values due to shifting\n",
        "df = df.dropna(subset=['next_light_level', 'light_level_lag_1', 'light_level_lag_2', 'light_level_lag_3'])\n",
        "print(df.head())\n",
        "\n",
        "# One-hot encode the 'room' column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_rooms = encoder.fit_transform(df[['room']])\n",
        "room_columns = encoder.categories_[0]  # Room categories (e.g., balcony, bedroom, living_room)\n",
        "encoded_rooms_df = pd.DataFrame(encoded_rooms, columns=room_columns)\n",
        "\n",
        "# Merge encoded rooms with the original DataFrame\n",
        "df = pd.concat([df.reset_index(drop=True), encoded_rooms_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[['hour'] + list(room_columns) + ['light_level', 'light_level_lag_1', 'light_level_lag_2', 'light_level_lag_3']]\n",
        "y = df['next_light_level']\n",
        "\n",
        "# Normalize the features (important for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the Neural Network model using Keras\n",
        "model = keras.Sequential([\n",
        "    layers.InputLayer(input_shape=(X_train.shape[1],)),  # Input layer with shape of features\n",
        "    layers.Dense(128, activation='relu'),  # First hidden layer with more units\n",
        "    layers.Dropout(0.3),  # Dropout for regularization\n",
        "    layers.Dense(64, activation='relu'),  # Second hidden layer with more units\n",
        "    layers.Dropout(0.3),  # Dropout for regularization\n",
        "    layers.Dense(32, activation='relu'),  # Third hidden layer\n",
        "    layers.Dense(1)  # Output layer (single regression value)\n",
        "])\n",
        "\n",
        "# Compile the model with the Adam optimizer and learning rate decay\n",
        "initial_lr = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=initial_lr,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_squared_error'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=520, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Save the trained model and encoder\n",
        "model.save(\"neural_network_model.h5\")\n",
        "encoder_path = \"room_encoder.npy\"\n",
        "np.save(encoder_path, encoder)  # Save encoder for future predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8WoBKDf9Cm3",
        "outputId": "d946df32-1ee1-4bce-f02a-7d7007f02b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    hour     room  light_level  next_light_level  light_level_lag_1  \\\n",
            "8      4  balcony            0               0.0                0.0   \n",
            "9      4  bedroom            0               0.0                0.0   \n",
            "10     5  balcony            0               0.0                0.0   \n",
            "11     5  bedroom            0               0.0                0.0   \n",
            "12     6  balcony            0              97.0                0.0   \n",
            "\n",
            "    light_level_lag_2  light_level_lag_3  \n",
            "8                 0.0                0.0  \n",
            "9                 0.0                0.0  \n",
            "10                0.0                0.0  \n",
            "11                0.0                0.0  \n",
            "12                0.0                0.0  \n",
            "Epoch 1/520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2929.2017 - mean_squared_error: 2929.2017 - val_loss: 1093.5249 - val_mean_squared_error: 1093.5249\n",
            "Epoch 2/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 934.0981 - mean_squared_error: 934.0981 - val_loss: 602.2454 - val_mean_squared_error: 602.2454\n",
            "Epoch 3/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 617.1867 - mean_squared_error: 617.1867 - val_loss: 520.5230 - val_mean_squared_error: 520.5230\n",
            "Epoch 4/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 541.7854 - mean_squared_error: 541.7854 - val_loss: 483.2127 - val_mean_squared_error: 483.2127\n",
            "Epoch 5/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 579.8109 - mean_squared_error: 579.8109 - val_loss: 442.8338 - val_mean_squared_error: 442.8338\n",
            "Epoch 6/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 501.6541 - mean_squared_error: 501.6541 - val_loss: 411.0299 - val_mean_squared_error: 411.0299\n",
            "Epoch 7/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 481.9392 - mean_squared_error: 481.9392 - val_loss: 399.0027 - val_mean_squared_error: 399.0027\n",
            "Epoch 8/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 407.4476 - mean_squared_error: 407.4476 - val_loss: 361.0270 - val_mean_squared_error: 361.0270\n",
            "Epoch 9/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 443.6089 - mean_squared_error: 443.6089 - val_loss: 330.1096 - val_mean_squared_error: 330.1096\n",
            "Epoch 10/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 421.1352 - mean_squared_error: 421.1352 - val_loss: 303.8599 - val_mean_squared_error: 303.8599\n",
            "Epoch 11/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 374.0597 - mean_squared_error: 374.0597 - val_loss: 280.3049 - val_mean_squared_error: 280.3049\n",
            "Epoch 12/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 353.6513 - mean_squared_error: 353.6513 - val_loss: 255.4589 - val_mean_squared_error: 255.4589\n",
            "Epoch 13/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 319.3099 - mean_squared_error: 319.3099 - val_loss: 238.8571 - val_mean_squared_error: 238.8571\n",
            "Epoch 14/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 330.0874 - mean_squared_error: 330.0874 - val_loss: 227.1770 - val_mean_squared_error: 227.1770\n",
            "Epoch 15/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 307.9786 - mean_squared_error: 307.9786 - val_loss: 191.7902 - val_mean_squared_error: 191.7902\n",
            "Epoch 16/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 268.4775 - mean_squared_error: 268.4775 - val_loss: 177.4892 - val_mean_squared_error: 177.4892\n",
            "Epoch 17/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267.8709 - mean_squared_error: 267.8709 - val_loss: 163.8221 - val_mean_squared_error: 163.8221\n",
            "Epoch 18/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 256.1369 - mean_squared_error: 256.1369 - val_loss: 147.7517 - val_mean_squared_error: 147.7517\n",
            "Epoch 19/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 250.2603 - mean_squared_error: 250.2603 - val_loss: 135.8917 - val_mean_squared_error: 135.8917\n",
            "Epoch 20/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 224.0127 - mean_squared_error: 224.0127 - val_loss: 139.5970 - val_mean_squared_error: 139.5970\n",
            "Epoch 21/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 204.7144 - mean_squared_error: 204.7144 - val_loss: 126.0193 - val_mean_squared_error: 126.0193\n",
            "Epoch 22/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 205.5317 - mean_squared_error: 205.5317 - val_loss: 117.0375 - val_mean_squared_error: 117.0375\n",
            "Epoch 23/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 207.9493 - mean_squared_error: 207.9493 - val_loss: 110.0409 - val_mean_squared_error: 110.0409\n",
            "Epoch 24/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 199.1705 - mean_squared_error: 199.1705 - val_loss: 111.5250 - val_mean_squared_error: 111.5250\n",
            "Epoch 25/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 180.5134 - mean_squared_error: 180.5134 - val_loss: 98.0222 - val_mean_squared_error: 98.0222\n",
            "Epoch 26/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 195.9780 - mean_squared_error: 195.9780 - val_loss: 102.2600 - val_mean_squared_error: 102.2600\n",
            "Epoch 27/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 174.6703 - mean_squared_error: 174.6703 - val_loss: 94.5039 - val_mean_squared_error: 94.5039\n",
            "Epoch 28/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 154.4827 - mean_squared_error: 154.4827 - val_loss: 98.8822 - val_mean_squared_error: 98.8822\n",
            "Epoch 29/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 166.3798 - mean_squared_error: 166.3798 - val_loss: 86.1655 - val_mean_squared_error: 86.1655\n",
            "Epoch 30/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 179.0404 - mean_squared_error: 179.0404 - val_loss: 87.0875 - val_mean_squared_error: 87.0875\n",
            "Epoch 31/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 163.4458 - mean_squared_error: 163.4458 - val_loss: 87.1502 - val_mean_squared_error: 87.1502\n",
            "Epoch 32/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 142.6323 - mean_squared_error: 142.6323 - val_loss: 97.4514 - val_mean_squared_error: 97.4514\n",
            "Epoch 33/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 162.8153 - mean_squared_error: 162.8153 - val_loss: 78.9346 - val_mean_squared_error: 78.9346\n",
            "Epoch 34/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 145.9624 - mean_squared_error: 145.9624 - val_loss: 74.8770 - val_mean_squared_error: 74.8770\n",
            "Epoch 35/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 161.7510 - mean_squared_error: 161.7510 - val_loss: 79.2300 - val_mean_squared_error: 79.2300\n",
            "Epoch 36/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 160.4971 - mean_squared_error: 160.4971 - val_loss: 75.8700 - val_mean_squared_error: 75.8700\n",
            "Epoch 37/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.6657 - mean_squared_error: 140.6657 - val_loss: 78.4672 - val_mean_squared_error: 78.4672\n",
            "Epoch 38/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 141.7326 - mean_squared_error: 141.7326 - val_loss: 82.5089 - val_mean_squared_error: 82.5089\n",
            "Epoch 39/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 141.5583 - mean_squared_error: 141.5583 - val_loss: 72.0815 - val_mean_squared_error: 72.0815\n",
            "Epoch 40/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 134.3567 - mean_squared_error: 134.3567 - val_loss: 90.8911 - val_mean_squared_error: 90.8911\n",
            "Epoch 41/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 128.3554 - mean_squared_error: 128.3554 - val_loss: 73.3918 - val_mean_squared_error: 73.3918\n",
            "Epoch 42/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 119.1616 - mean_squared_error: 119.1616 - val_loss: 74.0491 - val_mean_squared_error: 74.0491\n",
            "Epoch 43/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.4342 - mean_squared_error: 147.4342 - val_loss: 75.1633 - val_mean_squared_error: 75.1633\n",
            "Epoch 44/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 138.1320 - mean_squared_error: 138.1320 - val_loss: 100.4224 - val_mean_squared_error: 100.4224\n",
            "Epoch 45/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 159.3138 - mean_squared_error: 159.3138 - val_loss: 80.3289 - val_mean_squared_error: 80.3289\n",
            "Epoch 46/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 139.7144 - mean_squared_error: 139.7144 - val_loss: 77.4493 - val_mean_squared_error: 77.4493\n",
            "Epoch 47/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 145.0599 - mean_squared_error: 145.0599 - val_loss: 66.2124 - val_mean_squared_error: 66.2124\n",
            "Epoch 48/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 129.3851 - mean_squared_error: 129.3851 - val_loss: 83.9581 - val_mean_squared_error: 83.9581\n",
            "Epoch 49/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 119.6542 - mean_squared_error: 119.6542 - val_loss: 75.9586 - val_mean_squared_error: 75.9586\n",
            "Epoch 50/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 135.2038 - mean_squared_error: 135.2038 - val_loss: 73.0883 - val_mean_squared_error: 73.0883\n",
            "Epoch 51/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.3425 - mean_squared_error: 122.3425 - val_loss: 71.3963 - val_mean_squared_error: 71.3963\n",
            "Epoch 52/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.5953 - mean_squared_error: 124.5953 - val_loss: 72.7875 - val_mean_squared_error: 72.7875\n",
            "Epoch 53/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.5240 - mean_squared_error: 104.5240 - val_loss: 77.3291 - val_mean_squared_error: 77.3291\n",
            "Epoch 54/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.8398 - mean_squared_error: 107.8398 - val_loss: 68.0826 - val_mean_squared_error: 68.0826\n",
            "Epoch 55/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 115.0193 - mean_squared_error: 115.0193 - val_loss: 79.0634 - val_mean_squared_error: 79.0634\n",
            "Epoch 56/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 133.0854 - mean_squared_error: 133.0854 - val_loss: 68.7508 - val_mean_squared_error: 68.7508\n",
            "Epoch 57/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.2257 - mean_squared_error: 109.2257 - val_loss: 70.3489 - val_mean_squared_error: 70.3489\n",
            "Epoch 58/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.4126 - mean_squared_error: 102.4126 - val_loss: 71.9964 - val_mean_squared_error: 71.9964\n",
            "Epoch 59/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.2859 - mean_squared_error: 112.2859 - val_loss: 68.8019 - val_mean_squared_error: 68.8019\n",
            "Epoch 60/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.4695 - mean_squared_error: 130.4695 - val_loss: 70.7057 - val_mean_squared_error: 70.7057\n",
            "Epoch 61/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.6179 - mean_squared_error: 112.6179 - val_loss: 70.4469 - val_mean_squared_error: 70.4469\n",
            "Epoch 62/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.8959 - mean_squared_error: 108.8959 - val_loss: 67.9922 - val_mean_squared_error: 67.9922\n",
            "Epoch 63/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 104.0567 - mean_squared_error: 104.0567 - val_loss: 68.9400 - val_mean_squared_error: 68.9400\n",
            "Epoch 64/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 129.3898 - mean_squared_error: 129.3898 - val_loss: 71.2145 - val_mean_squared_error: 71.2145\n",
            "Epoch 65/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 115.7399 - mean_squared_error: 115.7399 - val_loss: 69.3321 - val_mean_squared_error: 69.3321\n",
            "Epoch 66/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 100.1419 - mean_squared_error: 100.1419 - val_loss: 74.8751 - val_mean_squared_error: 74.8751\n",
            "Epoch 67/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 116.9410 - mean_squared_error: 116.9410 - val_loss: 66.3288 - val_mean_squared_error: 66.3288\n",
            "Epoch 68/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 85.5962 - mean_squared_error: 85.5962 - val_loss: 68.3953 - val_mean_squared_error: 68.3953\n",
            "Epoch 69/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 98.3985 - mean_squared_error: 98.3985 - val_loss: 69.5447 - val_mean_squared_error: 69.5447\n",
            "Epoch 70/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 97.7138 - mean_squared_error: 97.7138 - val_loss: 68.6275 - val_mean_squared_error: 68.6275\n",
            "Epoch 71/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.4030 - mean_squared_error: 109.4030 - val_loss: 67.2510 - val_mean_squared_error: 67.2510\n",
            "Epoch 72/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.2311 - mean_squared_error: 84.2311 - val_loss: 66.3988 - val_mean_squared_error: 66.3988\n",
            "Epoch 73/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96.6267 - mean_squared_error: 96.6267 - val_loss: 64.2183 - val_mean_squared_error: 64.2183\n",
            "Epoch 74/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.5626 - mean_squared_error: 102.5626 - val_loss: 67.3315 - val_mean_squared_error: 67.3315\n",
            "Epoch 75/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.9809 - mean_squared_error: 109.9809 - val_loss: 67.7534 - val_mean_squared_error: 67.7534\n",
            "Epoch 76/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93.3527 - mean_squared_error: 93.3527 - val_loss: 67.0715 - val_mean_squared_error: 67.0715\n",
            "Epoch 77/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.9415 - mean_squared_error: 105.9415 - val_loss: 64.1106 - val_mean_squared_error: 64.1106\n",
            "Epoch 78/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.3214 - mean_squared_error: 107.3214 - val_loss: 69.2986 - val_mean_squared_error: 69.2986\n",
            "Epoch 79/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91.9617 - mean_squared_error: 91.9617 - val_loss: 69.5424 - val_mean_squared_error: 69.5424\n",
            "Epoch 80/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.6000 - mean_squared_error: 106.6000 - val_loss: 69.7002 - val_mean_squared_error: 69.7002\n",
            "Epoch 81/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.7899 - mean_squared_error: 106.7899 - val_loss: 69.2166 - val_mean_squared_error: 69.2166\n",
            "Epoch 82/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.4167 - mean_squared_error: 102.4167 - val_loss: 70.5927 - val_mean_squared_error: 70.5927\n",
            "Epoch 83/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 95.5200 - mean_squared_error: 95.5200 - val_loss: 65.0818 - val_mean_squared_error: 65.0818\n",
            "Epoch 84/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.5823 - mean_squared_error: 106.5823 - val_loss: 66.4758 - val_mean_squared_error: 66.4758\n",
            "Epoch 85/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91.6169 - mean_squared_error: 91.6169 - val_loss: 69.4969 - val_mean_squared_error: 69.4969\n",
            "Epoch 86/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.8029 - mean_squared_error: 102.8029 - val_loss: 70.5745 - val_mean_squared_error: 70.5745\n",
            "Epoch 87/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.9235 - mean_squared_error: 102.9235 - val_loss: 65.6570 - val_mean_squared_error: 65.6570\n",
            "Epoch 88/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.9960 - mean_squared_error: 102.9960 - val_loss: 65.1789 - val_mean_squared_error: 65.1789\n",
            "Epoch 89/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 97.0482 - mean_squared_error: 97.0482 - val_loss: 66.2977 - val_mean_squared_error: 66.2977\n",
            "Epoch 90/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.4422 - mean_squared_error: 88.4422 - val_loss: 65.5791 - val_mean_squared_error: 65.5791\n",
            "Epoch 91/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 77.4373 - mean_squared_error: 77.4373 - val_loss: 67.0981 - val_mean_squared_error: 67.0981\n",
            "Epoch 92/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85.7461 - mean_squared_error: 85.7461 - val_loss: 65.9242 - val_mean_squared_error: 65.9242\n",
            "Epoch 93/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.6894 - mean_squared_error: 116.6894 - val_loss: 65.2917 - val_mean_squared_error: 65.2917\n",
            "Epoch 94/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.8946 - mean_squared_error: 103.8946 - val_loss: 68.3963 - val_mean_squared_error: 68.3963\n",
            "Epoch 95/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.5880 - mean_squared_error: 86.5880 - val_loss: 65.0577 - val_mean_squared_error: 65.0577\n",
            "Epoch 96/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.0767 - mean_squared_error: 100.0767 - val_loss: 73.5836 - val_mean_squared_error: 73.5836\n",
            "Epoch 97/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.5789 - mean_squared_error: 111.5789 - val_loss: 68.9490 - val_mean_squared_error: 68.9490\n",
            "Epoch 98/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.9632 - mean_squared_error: 109.9632 - val_loss: 63.3785 - val_mean_squared_error: 63.3785\n",
            "Epoch 99/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.0308 - mean_squared_error: 94.0308 - val_loss: 69.4014 - val_mean_squared_error: 69.4014\n",
            "Epoch 100/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88.7501 - mean_squared_error: 88.7501 - val_loss: 66.4863 - val_mean_squared_error: 66.4863\n",
            "Epoch 101/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 81.0455 - mean_squared_error: 81.0455 - val_loss: 68.6858 - val_mean_squared_error: 68.6858\n",
            "Epoch 102/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86.9137 - mean_squared_error: 86.9137 - val_loss: 67.6912 - val_mean_squared_error: 67.6912\n",
            "Epoch 103/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 90.6054 - mean_squared_error: 90.6054 - val_loss: 67.6896 - val_mean_squared_error: 67.6896\n",
            "Epoch 104/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 91.1674 - mean_squared_error: 91.1674 - val_loss: 69.9438 - val_mean_squared_error: 69.9438\n",
            "Epoch 105/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90.5051 - mean_squared_error: 90.5051 - val_loss: 63.1148 - val_mean_squared_error: 63.1148\n",
            "Epoch 106/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 94.9166 - mean_squared_error: 94.9166 - val_loss: 67.2267 - val_mean_squared_error: 67.2267\n",
            "Epoch 107/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 88.9159 - mean_squared_error: 88.9159 - val_loss: 72.0188 - val_mean_squared_error: 72.0188\n",
            "Epoch 108/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 81.2156 - mean_squared_error: 81.2156 - val_loss: 64.4194 - val_mean_squared_error: 64.4194\n",
            "Epoch 109/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 110.3202 - mean_squared_error: 110.3202 - val_loss: 64.4110 - val_mean_squared_error: 64.4110\n",
            "Epoch 110/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82.6951 - mean_squared_error: 82.6951 - val_loss: 67.8297 - val_mean_squared_error: 67.8297\n",
            "Epoch 111/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.4475 - mean_squared_error: 84.4475 - val_loss: 66.6470 - val_mean_squared_error: 66.6470\n",
            "Epoch 112/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 92.5001 - mean_squared_error: 92.5001 - val_loss: 64.1719 - val_mean_squared_error: 64.1719\n",
            "Epoch 113/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 83.5260 - mean_squared_error: 83.5260 - val_loss: 70.3938 - val_mean_squared_error: 70.3938\n",
            "Epoch 114/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.0846 - mean_squared_error: 86.0846 - val_loss: 66.7374 - val_mean_squared_error: 66.7374\n",
            "Epoch 115/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.7338 - mean_squared_error: 109.7338 - val_loss: 64.6390 - val_mean_squared_error: 64.6390\n",
            "Epoch 116/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.0467 - mean_squared_error: 70.0467 - val_loss: 65.8397 - val_mean_squared_error: 65.8397\n",
            "Epoch 117/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.5991 - mean_squared_error: 109.5991 - val_loss: 66.5969 - val_mean_squared_error: 66.5969\n",
            "Epoch 118/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96.4563 - mean_squared_error: 96.4563 - val_loss: 67.2439 - val_mean_squared_error: 67.2439\n",
            "Epoch 119/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.0790 - mean_squared_error: 79.0790 - val_loss: 65.1500 - val_mean_squared_error: 65.1500\n",
            "Epoch 120/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99.0968 - mean_squared_error: 99.0968 - val_loss: 67.5520 - val_mean_squared_error: 67.5520\n",
            "Epoch 121/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 75.5168 - mean_squared_error: 75.5168 - val_loss: 68.3475 - val_mean_squared_error: 68.3475\n",
            "Epoch 122/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.6533 - mean_squared_error: 79.6533 - val_loss: 66.8884 - val_mean_squared_error: 66.8884\n",
            "Epoch 123/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101.1766 - mean_squared_error: 101.1766 - val_loss: 68.9525 - val_mean_squared_error: 68.9525\n",
            "Epoch 124/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.4758 - mean_squared_error: 89.4758 - val_loss: 65.1042 - val_mean_squared_error: 65.1042\n",
            "Epoch 125/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.4417 - mean_squared_error: 69.4417 - val_loss: 63.6820 - val_mean_squared_error: 63.6820\n",
            "Epoch 126/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.9524 - mean_squared_error: 73.9524 - val_loss: 67.3189 - val_mean_squared_error: 67.3189\n",
            "Epoch 127/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.5154 - mean_squared_error: 102.5154 - val_loss: 66.5695 - val_mean_squared_error: 66.5695\n",
            "Epoch 128/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89.0030 - mean_squared_error: 89.0030 - val_loss: 65.3168 - val_mean_squared_error: 65.3168\n",
            "Epoch 129/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.5533 - mean_squared_error: 75.5533 - val_loss: 66.0311 - val_mean_squared_error: 66.0311\n",
            "Epoch 130/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.2586 - mean_squared_error: 85.2586 - val_loss: 67.1480 - val_mean_squared_error: 67.1480\n",
            "Epoch 131/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.9453 - mean_squared_error: 102.9453 - val_loss: 63.7756 - val_mean_squared_error: 63.7756\n",
            "Epoch 132/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 78.0191 - mean_squared_error: 78.0191 - val_loss: 63.6657 - val_mean_squared_error: 63.6657\n",
            "Epoch 133/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.3920 - mean_squared_error: 79.3920 - val_loss: 64.2616 - val_mean_squared_error: 64.2616\n",
            "Epoch 134/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82.1856 - mean_squared_error: 82.1856 - val_loss: 64.3007 - val_mean_squared_error: 64.3007\n",
            "Epoch 135/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96.6331 - mean_squared_error: 96.6331 - val_loss: 63.3021 - val_mean_squared_error: 63.3021\n",
            "Epoch 136/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.3403 - mean_squared_error: 100.3403 - val_loss: 66.2361 - val_mean_squared_error: 66.2361\n",
            "Epoch 137/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.2460 - mean_squared_error: 88.2460 - val_loss: 66.9632 - val_mean_squared_error: 66.9632\n",
            "Epoch 138/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91.8527 - mean_squared_error: 91.8527 - val_loss: 65.8331 - val_mean_squared_error: 65.8331\n",
            "Epoch 139/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.6031 - mean_squared_error: 100.6031 - val_loss: 66.3905 - val_mean_squared_error: 66.3905\n",
            "Epoch 140/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 75.8230 - mean_squared_error: 75.8230 - val_loss: 76.6065 - val_mean_squared_error: 76.6065\n",
            "Epoch 141/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 83.5872 - mean_squared_error: 83.5872 - val_loss: 64.9732 - val_mean_squared_error: 64.9732\n",
            "Epoch 142/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 98.3365 - mean_squared_error: 98.3365 - val_loss: 65.7130 - val_mean_squared_error: 65.7130\n",
            "Epoch 143/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 80.2829 - mean_squared_error: 80.2829 - val_loss: 69.2303 - val_mean_squared_error: 69.2303\n",
            "Epoch 144/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 86.5176 - mean_squared_error: 86.5176 - val_loss: 64.7438 - val_mean_squared_error: 64.7438\n",
            "Epoch 145/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 75.8051 - mean_squared_error: 75.8051 - val_loss: 63.5792 - val_mean_squared_error: 63.5792\n",
            "Epoch 146/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.3741 - mean_squared_error: 89.3741 - val_loss: 63.5741 - val_mean_squared_error: 63.5741\n",
            "Epoch 147/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87.3794 - mean_squared_error: 87.3794 - val_loss: 67.0287 - val_mean_squared_error: 67.0287\n",
            "Epoch 148/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.2769 - mean_squared_error: 85.2769 - val_loss: 65.3741 - val_mean_squared_error: 65.3741\n",
            "Epoch 149/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.2139 - mean_squared_error: 88.2139 - val_loss: 65.9241 - val_mean_squared_error: 65.9241\n",
            "Epoch 150/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83.8408 - mean_squared_error: 83.8408 - val_loss: 63.7185 - val_mean_squared_error: 63.7185\n",
            "Epoch 151/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 73.5241 - mean_squared_error: 73.5241 - val_loss: 63.3164 - val_mean_squared_error: 63.3164\n",
            "Epoch 152/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.3921 - mean_squared_error: 71.3921 - val_loss: 63.9942 - val_mean_squared_error: 63.9942\n",
            "Epoch 153/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.0980 - mean_squared_error: 66.0980 - val_loss: 65.4722 - val_mean_squared_error: 65.4722\n",
            "Epoch 154/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.6501 - mean_squared_error: 74.6501 - val_loss: 65.4310 - val_mean_squared_error: 65.4310\n",
            "Epoch 155/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.6362 - mean_squared_error: 62.6362 - val_loss: 64.2943 - val_mean_squared_error: 64.2943\n",
            "Epoch 156/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 75.5858 - mean_squared_error: 75.5858 - val_loss: 65.4029 - val_mean_squared_error: 65.4029\n",
            "Epoch 157/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 79.6485 - mean_squared_error: 79.6485 - val_loss: 64.5754 - val_mean_squared_error: 64.5754\n",
            "Epoch 158/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 80.9598 - mean_squared_error: 80.9598 - val_loss: 64.7166 - val_mean_squared_error: 64.7166\n",
            "Epoch 159/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.6635 - mean_squared_error: 79.6635 - val_loss: 65.7475 - val_mean_squared_error: 65.7475\n",
            "Epoch 160/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.5061 - mean_squared_error: 83.5061 - val_loss: 65.0899 - val_mean_squared_error: 65.0899\n",
            "Epoch 161/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 73.6177 - mean_squared_error: 73.6177 - val_loss: 63.6178 - val_mean_squared_error: 63.6178\n",
            "Epoch 162/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92.3538 - mean_squared_error: 92.3538 - val_loss: 64.2546 - val_mean_squared_error: 64.2546\n",
            "Epoch 163/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 68.8857 - mean_squared_error: 68.8857 - val_loss: 65.8579 - val_mean_squared_error: 65.8579\n",
            "Epoch 164/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92.1776 - mean_squared_error: 92.1776 - val_loss: 63.4602 - val_mean_squared_error: 63.4602\n",
            "Epoch 165/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 82.4105 - mean_squared_error: 82.4105 - val_loss: 66.5350 - val_mean_squared_error: 66.5350\n",
            "Epoch 166/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76.8281 - mean_squared_error: 76.8281 - val_loss: 65.4362 - val_mean_squared_error: 65.4362\n",
            "Epoch 167/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86.5127 - mean_squared_error: 86.5127 - val_loss: 65.0312 - val_mean_squared_error: 65.0312\n",
            "Epoch 168/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.2537 - mean_squared_error: 74.2537 - val_loss: 65.4081 - val_mean_squared_error: 65.4081\n",
            "Epoch 169/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.0377 - mean_squared_error: 70.0377 - val_loss: 64.7264 - val_mean_squared_error: 64.7264\n",
            "Epoch 170/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.2255 - mean_squared_error: 83.2255 - val_loss: 65.2398 - val_mean_squared_error: 65.2398\n",
            "Epoch 171/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 81.6095 - mean_squared_error: 81.6095 - val_loss: 66.2250 - val_mean_squared_error: 66.2250\n",
            "Epoch 172/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.2405 - mean_squared_error: 80.2405 - val_loss: 63.5574 - val_mean_squared_error: 63.5574\n",
            "Epoch 173/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90.8853 - mean_squared_error: 90.8853 - val_loss: 64.6491 - val_mean_squared_error: 64.6491\n",
            "Epoch 174/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90.4062 - mean_squared_error: 90.4062 - val_loss: 63.7746 - val_mean_squared_error: 63.7746\n",
            "Epoch 175/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 69.2621 - mean_squared_error: 69.2621 - val_loss: 64.5482 - val_mean_squared_error: 64.5482\n",
            "Epoch 176/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 72.9143 - mean_squared_error: 72.9143 - val_loss: 63.5256 - val_mean_squared_error: 63.5256\n",
            "Epoch 177/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 62.0419 - mean_squared_error: 62.0419 - val_loss: 64.2563 - val_mean_squared_error: 64.2563\n",
            "Epoch 178/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 64.6768 - mean_squared_error: 64.6768 - val_loss: 64.4768 - val_mean_squared_error: 64.4768\n",
            "Epoch 179/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 85.6081 - mean_squared_error: 85.6081 - val_loss: 64.6687 - val_mean_squared_error: 64.6687\n",
            "Epoch 180/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 83.5269 - mean_squared_error: 83.5269 - val_loss: 65.0386 - val_mean_squared_error: 65.0386\n",
            "Epoch 181/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 75.5208 - mean_squared_error: 75.5208 - val_loss: 62.4979 - val_mean_squared_error: 62.4979\n",
            "Epoch 182/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 87.4117 - mean_squared_error: 87.4117 - val_loss: 65.3612 - val_mean_squared_error: 65.3612\n",
            "Epoch 183/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 69.4482 - mean_squared_error: 69.4482 - val_loss: 68.1691 - val_mean_squared_error: 68.1691\n",
            "Epoch 184/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 93.9466 - mean_squared_error: 93.9466 - val_loss: 64.8585 - val_mean_squared_error: 64.8585\n",
            "Epoch 185/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 77.9600 - mean_squared_error: 77.9600 - val_loss: 69.5709 - val_mean_squared_error: 69.5709\n",
            "Epoch 186/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 87.0070 - mean_squared_error: 87.0070 - val_loss: 64.2093 - val_mean_squared_error: 64.2093\n",
            "Epoch 187/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 84.0359 - mean_squared_error: 84.0359 - val_loss: 64.6494 - val_mean_squared_error: 64.6494\n",
            "Epoch 188/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 85.5406 - mean_squared_error: 85.5406 - val_loss: 65.6792 - val_mean_squared_error: 65.6792\n",
            "Epoch 189/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 75.1775 - mean_squared_error: 75.1775 - val_loss: 65.1581 - val_mean_squared_error: 65.1581\n",
            "Epoch 190/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 78.8050 - mean_squared_error: 78.8050 - val_loss: 65.2167 - val_mean_squared_error: 65.2167\n",
            "Epoch 191/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86.7245 - mean_squared_error: 86.7245 - val_loss: 65.3818 - val_mean_squared_error: 65.3818\n",
            "Epoch 192/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 75.2513 - mean_squared_error: 75.2513 - val_loss: 66.9149 - val_mean_squared_error: 66.9149\n",
            "Epoch 193/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 86.5629 - mean_squared_error: 86.5629 - val_loss: 63.4886 - val_mean_squared_error: 63.4886\n",
            "Epoch 194/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 61.9833 - mean_squared_error: 61.9833 - val_loss: 64.3623 - val_mean_squared_error: 64.3623\n",
            "Epoch 195/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.2585 - mean_squared_error: 68.2585 - val_loss: 65.0301 - val_mean_squared_error: 65.0301\n",
            "Epoch 196/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.0937 - mean_squared_error: 80.0937 - val_loss: 64.7400 - val_mean_squared_error: 64.7400\n",
            "Epoch 197/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 66.5021 - mean_squared_error: 66.5021 - val_loss: 65.2105 - val_mean_squared_error: 65.2105\n",
            "Epoch 198/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 66.6579 - mean_squared_error: 66.6579 - val_loss: 64.5047 - val_mean_squared_error: 64.5047\n",
            "Epoch 199/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 86.8465 - mean_squared_error: 86.8465 - val_loss: 64.1572 - val_mean_squared_error: 64.1572\n",
            "Epoch 200/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 83.0623 - mean_squared_error: 83.0623 - val_loss: 65.7525 - val_mean_squared_error: 65.7525\n",
            "Epoch 201/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 68.2243 - mean_squared_error: 68.2243 - val_loss: 64.6460 - val_mean_squared_error: 64.6460\n",
            "Epoch 202/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63.9843 - mean_squared_error: 63.9843 - val_loss: 64.4299 - val_mean_squared_error: 64.4299\n",
            "Epoch 203/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.2820 - mean_squared_error: 80.2820 - val_loss: 63.3613 - val_mean_squared_error: 63.3613\n",
            "Epoch 204/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.5352 - mean_squared_error: 67.5352 - val_loss: 63.9098 - val_mean_squared_error: 63.9098\n",
            "Epoch 205/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97.8478 - mean_squared_error: 97.8478 - val_loss: 64.0488 - val_mean_squared_error: 64.0488\n",
            "Epoch 206/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76.6653 - mean_squared_error: 76.6653 - val_loss: 63.3252 - val_mean_squared_error: 63.3252\n",
            "Epoch 207/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.1423 - mean_squared_error: 55.1423 - val_loss: 63.9612 - val_mean_squared_error: 63.9612\n",
            "Epoch 208/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.1921 - mean_squared_error: 86.1921 - val_loss: 64.9711 - val_mean_squared_error: 64.9711\n",
            "Epoch 209/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.5735 - mean_squared_error: 83.5735 - val_loss: 64.9747 - val_mean_squared_error: 64.9747\n",
            "Epoch 210/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 77.6286 - mean_squared_error: 77.6286 - val_loss: 64.1990 - val_mean_squared_error: 64.1990\n",
            "Epoch 211/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.5773 - mean_squared_error: 88.5773 - val_loss: 64.5605 - val_mean_squared_error: 64.5605\n",
            "Epoch 212/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 80.0713 - mean_squared_error: 80.0713 - val_loss: 65.1928 - val_mean_squared_error: 65.1928\n",
            "Epoch 213/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.3932 - mean_squared_error: 71.3932 - val_loss: 64.6857 - val_mean_squared_error: 64.6857\n",
            "Epoch 214/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 72.0721 - mean_squared_error: 72.0721 - val_loss: 64.5980 - val_mean_squared_error: 64.5980\n",
            "Epoch 215/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 75.1009 - mean_squared_error: 75.1009 - val_loss: 64.2606 - val_mean_squared_error: 64.2606\n",
            "Epoch 216/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.3842 - mean_squared_error: 84.3842 - val_loss: 63.6002 - val_mean_squared_error: 63.6002\n",
            "Epoch 217/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 80.8667 - mean_squared_error: 80.8667 - val_loss: 64.2021 - val_mean_squared_error: 64.2021\n",
            "Epoch 218/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.3303 - mean_squared_error: 64.3303 - val_loss: 63.9704 - val_mean_squared_error: 63.9704\n",
            "Epoch 219/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.3436 - mean_squared_error: 71.3436 - val_loss: 65.1347 - val_mean_squared_error: 65.1347\n",
            "Epoch 220/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 78.7837 - mean_squared_error: 78.7837 - val_loss: 65.2423 - val_mean_squared_error: 65.2423\n",
            "Epoch 221/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78.5268 - mean_squared_error: 78.5268 - val_loss: 64.3979 - val_mean_squared_error: 64.3979\n",
            "Epoch 222/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 95.4546 - mean_squared_error: 95.4546 - val_loss: 66.6488 - val_mean_squared_error: 66.6488\n",
            "Epoch 223/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62.5946 - mean_squared_error: 62.5946 - val_loss: 64.0056 - val_mean_squared_error: 64.0056\n",
            "Epoch 224/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 75.1832 - mean_squared_error: 75.1832 - val_loss: 65.5924 - val_mean_squared_error: 65.5924\n",
            "Epoch 225/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.9385 - mean_squared_error: 64.9385 - val_loss: 65.1604 - val_mean_squared_error: 65.1604\n",
            "Epoch 226/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.7961 - mean_squared_error: 68.7961 - val_loss: 64.5063 - val_mean_squared_error: 64.5063\n",
            "Epoch 227/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.7205 - mean_squared_error: 63.7205 - val_loss: 65.7540 - val_mean_squared_error: 65.7540\n",
            "Epoch 228/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.8085 - mean_squared_error: 69.8085 - val_loss: 64.8700 - val_mean_squared_error: 64.8700\n",
            "Epoch 229/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57.2733 - mean_squared_error: 57.2733 - val_loss: 66.2939 - val_mean_squared_error: 66.2939\n",
            "Epoch 230/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.2125 - mean_squared_error: 72.2125 - val_loss: 64.9305 - val_mean_squared_error: 64.9305\n",
            "Epoch 231/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90.6293 - mean_squared_error: 90.6293 - val_loss: 66.6210 - val_mean_squared_error: 66.6210\n",
            "Epoch 232/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.4968 - mean_squared_error: 55.4968 - val_loss: 67.1844 - val_mean_squared_error: 67.1844\n",
            "Epoch 233/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 73.9044 - mean_squared_error: 73.9044 - val_loss: 65.5999 - val_mean_squared_error: 65.5999\n",
            "Epoch 234/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 71.0589 - mean_squared_error: 71.0589 - val_loss: 63.7715 - val_mean_squared_error: 63.7715\n",
            "Epoch 235/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 57.3383 - mean_squared_error: 57.3383 - val_loss: 64.7031 - val_mean_squared_error: 64.7031\n",
            "Epoch 236/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 74.0956 - mean_squared_error: 74.0956 - val_loss: 65.2694 - val_mean_squared_error: 65.2694\n",
            "Epoch 237/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 65.8738 - mean_squared_error: 65.8738 - val_loss: 65.1554 - val_mean_squared_error: 65.1554\n",
            "Epoch 238/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 66.4336 - mean_squared_error: 66.4336 - val_loss: 64.4956 - val_mean_squared_error: 64.4956\n",
            "Epoch 239/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.9378 - mean_squared_error: 64.9378 - val_loss: 64.8865 - val_mean_squared_error: 64.8865\n",
            "Epoch 240/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.1152 - mean_squared_error: 80.1152 - val_loss: 64.4374 - val_mean_squared_error: 64.4374\n",
            "Epoch 241/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.5996 - mean_squared_error: 73.5996 - val_loss: 66.3011 - val_mean_squared_error: 66.3011\n",
            "Epoch 242/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.9521 - mean_squared_error: 58.9521 - val_loss: 65.3860 - val_mean_squared_error: 65.3860\n",
            "Epoch 243/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.3485 - mean_squared_error: 83.3485 - val_loss: 62.2805 - val_mean_squared_error: 62.2805\n",
            "Epoch 244/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.0836 - mean_squared_error: 80.0836 - val_loss: 65.1521 - val_mean_squared_error: 65.1521\n",
            "Epoch 245/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.1039 - mean_squared_error: 84.1039 - val_loss: 65.3279 - val_mean_squared_error: 65.3279\n",
            "Epoch 246/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82.2405 - mean_squared_error: 82.2405 - val_loss: 65.8914 - val_mean_squared_error: 65.8914\n",
            "Epoch 247/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 54.8266 - mean_squared_error: 54.8266 - val_loss: 64.7924 - val_mean_squared_error: 64.7924\n",
            "Epoch 248/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 85.7699 - mean_squared_error: 85.7699 - val_loss: 65.9602 - val_mean_squared_error: 65.9602\n",
            "Epoch 249/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53.0939 - mean_squared_error: 53.0939 - val_loss: 63.9906 - val_mean_squared_error: 63.9906\n",
            "Epoch 250/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56.5370 - mean_squared_error: 56.5370 - val_loss: 64.0081 - val_mean_squared_error: 64.0081\n",
            "Epoch 251/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.3948 - mean_squared_error: 72.3948 - val_loss: 63.8635 - val_mean_squared_error: 63.8635\n",
            "Epoch 252/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 82.7692 - mean_squared_error: 82.7692 - val_loss: 66.1251 - val_mean_squared_error: 66.1251\n",
            "Epoch 253/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76.2779 - mean_squared_error: 76.2779 - val_loss: 64.5204 - val_mean_squared_error: 64.5204\n",
            "Epoch 254/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.2136 - mean_squared_error: 80.2136 - val_loss: 64.9526 - val_mean_squared_error: 64.9526\n",
            "Epoch 255/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 77.9043 - mean_squared_error: 77.9043 - val_loss: 67.7287 - val_mean_squared_error: 67.7287\n",
            "Epoch 256/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 73.8037 - mean_squared_error: 73.8037 - val_loss: 64.1540 - val_mean_squared_error: 64.1540\n",
            "Epoch 257/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.1572 - mean_squared_error: 64.1572 - val_loss: 65.2159 - val_mean_squared_error: 65.2159\n",
            "Epoch 258/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.0752 - mean_squared_error: 72.0752 - val_loss: 63.7229 - val_mean_squared_error: 63.7229\n",
            "Epoch 259/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.8069 - mean_squared_error: 80.8069 - val_loss: 64.0805 - val_mean_squared_error: 64.0805\n",
            "Epoch 260/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.6851 - mean_squared_error: 71.6851 - val_loss: 65.3573 - val_mean_squared_error: 65.3573\n",
            "Epoch 261/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.8458 - mean_squared_error: 79.8458 - val_loss: 65.2090 - val_mean_squared_error: 65.2090\n",
            "Epoch 262/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76.0997 - mean_squared_error: 76.0997 - val_loss: 64.3074 - val_mean_squared_error: 64.3074\n",
            "Epoch 263/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62.6697 - mean_squared_error: 62.6697 - val_loss: 65.0128 - val_mean_squared_error: 65.0128\n",
            "Epoch 264/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76.9998 - mean_squared_error: 76.9998 - val_loss: 61.8997 - val_mean_squared_error: 61.8997\n",
            "Epoch 265/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.3385 - mean_squared_error: 86.3385 - val_loss: 63.8628 - val_mean_squared_error: 63.8628\n",
            "Epoch 266/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.6330 - mean_squared_error: 71.6330 - val_loss: 64.7507 - val_mean_squared_error: 64.7507\n",
            "Epoch 267/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.1373 - mean_squared_error: 59.1373 - val_loss: 64.7294 - val_mean_squared_error: 64.7294\n",
            "Epoch 268/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.2299 - mean_squared_error: 57.2299 - val_loss: 66.2478 - val_mean_squared_error: 66.2478\n",
            "Epoch 269/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 77.3423 - mean_squared_error: 77.3423 - val_loss: 66.7857 - val_mean_squared_error: 66.7857\n",
            "Epoch 270/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71.9972 - mean_squared_error: 71.9972 - val_loss: 65.5460 - val_mean_squared_error: 65.5460\n",
            "Epoch 271/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 71.2751 - mean_squared_error: 71.2751 - val_loss: 66.4528 - val_mean_squared_error: 66.4528\n",
            "Epoch 272/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 65.3769 - mean_squared_error: 65.3769 - val_loss: 65.0220 - val_mean_squared_error: 65.0220\n",
            "Epoch 273/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71.7888 - mean_squared_error: 71.7888 - val_loss: 64.7340 - val_mean_squared_error: 64.7340\n",
            "Epoch 274/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 84.1553 - mean_squared_error: 84.1553 - val_loss: 64.2496 - val_mean_squared_error: 64.2496\n",
            "Epoch 275/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 73.5286 - mean_squared_error: 73.5286 - val_loss: 65.5053 - val_mean_squared_error: 65.5053\n",
            "Epoch 276/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 80.3054 - mean_squared_error: 80.3054 - val_loss: 65.2977 - val_mean_squared_error: 65.2977\n",
            "Epoch 277/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 67.8055 - mean_squared_error: 67.8055 - val_loss: 64.9809 - val_mean_squared_error: 64.9809\n",
            "Epoch 278/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 74.7352 - mean_squared_error: 74.7352 - val_loss: 63.5871 - val_mean_squared_error: 63.5871\n",
            "Epoch 279/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 79.2580 - mean_squared_error: 79.2580 - val_loss: 63.7095 - val_mean_squared_error: 63.7095\n",
            "Epoch 280/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 78.9493 - mean_squared_error: 78.9493 - val_loss: 63.8274 - val_mean_squared_error: 63.8274\n",
            "Epoch 281/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71.7629 - mean_squared_error: 71.7629 - val_loss: 63.8448 - val_mean_squared_error: 63.8448\n",
            "Epoch 282/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 68.8013 - mean_squared_error: 68.8013 - val_loss: 63.4426 - val_mean_squared_error: 63.4426\n",
            "Epoch 283/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.8015 - mean_squared_error: 55.8015 - val_loss: 63.7134 - val_mean_squared_error: 63.7134\n",
            "Epoch 284/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82.6612 - mean_squared_error: 82.6612 - val_loss: 64.1512 - val_mean_squared_error: 64.1512\n",
            "Epoch 285/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92.5761 - mean_squared_error: 92.5761 - val_loss: 63.5234 - val_mean_squared_error: 63.5234\n",
            "Epoch 286/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.2228 - mean_squared_error: 68.2228 - val_loss: 64.7726 - val_mean_squared_error: 64.7726\n",
            "Epoch 287/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 89.5318 - mean_squared_error: 89.5318 - val_loss: 65.1706 - val_mean_squared_error: 65.1706\n",
            "Epoch 288/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.2645 - mean_squared_error: 65.2645 - val_loss: 64.8441 - val_mean_squared_error: 64.8441\n",
            "Epoch 289/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62.4897 - mean_squared_error: 62.4897 - val_loss: 64.0243 - val_mean_squared_error: 64.0243\n",
            "Epoch 290/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 86.7364 - mean_squared_error: 86.7364 - val_loss: 63.7698 - val_mean_squared_error: 63.7698\n",
            "Epoch 291/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.1439 - mean_squared_error: 69.1439 - val_loss: 64.0931 - val_mean_squared_error: 64.0931\n",
            "Epoch 292/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87.3332 - mean_squared_error: 87.3332 - val_loss: 65.0925 - val_mean_squared_error: 65.0925\n",
            "Epoch 293/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 67.8992 - mean_squared_error: 67.8992 - val_loss: 65.5392 - val_mean_squared_error: 65.5392\n",
            "Epoch 294/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.2574 - mean_squared_error: 64.2574 - val_loss: 63.9664 - val_mean_squared_error: 63.9664\n",
            "Epoch 295/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.1638 - mean_squared_error: 73.1638 - val_loss: 65.7443 - val_mean_squared_error: 65.7443\n",
            "Epoch 296/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.6986 - mean_squared_error: 64.6986 - val_loss: 63.3679 - val_mean_squared_error: 63.3679\n",
            "Epoch 297/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.5042 - mean_squared_error: 64.5042 - val_loss: 62.6763 - val_mean_squared_error: 62.6763\n",
            "Epoch 298/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 74.7176 - mean_squared_error: 74.7176 - val_loss: 63.3862 - val_mean_squared_error: 63.3862\n",
            "Epoch 299/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 68.0573 - mean_squared_error: 68.0573 - val_loss: 63.4184 - val_mean_squared_error: 63.4184\n",
            "Epoch 300/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90.9136 - mean_squared_error: 90.9136 - val_loss: 63.0210 - val_mean_squared_error: 63.0210\n",
            "Epoch 301/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 80.3783 - mean_squared_error: 80.3783 - val_loss: 64.7088 - val_mean_squared_error: 64.7088\n",
            "Epoch 302/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78.5064 - mean_squared_error: 78.5064 - val_loss: 66.3256 - val_mean_squared_error: 66.3256\n",
            "Epoch 303/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.8944 - mean_squared_error: 74.8944 - val_loss: 62.7156 - val_mean_squared_error: 62.7156\n",
            "Epoch 304/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 81.3949 - mean_squared_error: 81.3949 - val_loss: 62.3910 - val_mean_squared_error: 62.3910\n",
            "Epoch 305/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 92.0122 - mean_squared_error: 92.0122 - val_loss: 68.0587 - val_mean_squared_error: 68.0587\n",
            "Epoch 306/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 109.9894 - mean_squared_error: 109.9894 - val_loss: 63.8483 - val_mean_squared_error: 63.8483\n",
            "Epoch 307/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 75.8526 - mean_squared_error: 75.8526 - val_loss: 64.6788 - val_mean_squared_error: 64.6788\n",
            "Epoch 308/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 72.0815 - mean_squared_error: 72.0815 - val_loss: 65.4074 - val_mean_squared_error: 65.4074\n",
            "Epoch 309/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 81.9259 - mean_squared_error: 81.9259 - val_loss: 63.7668 - val_mean_squared_error: 63.7668\n",
            "Epoch 310/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.3369 - mean_squared_error: 67.3369 - val_loss: 63.5141 - val_mean_squared_error: 63.5141\n",
            "Epoch 311/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 44.5131 - mean_squared_error: 44.5131 - val_loss: 63.2841 - val_mean_squared_error: 63.2841\n",
            "Epoch 312/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.1804 - mean_squared_error: 65.1804 - val_loss: 64.1944 - val_mean_squared_error: 64.1944\n",
            "Epoch 313/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 78.7117 - mean_squared_error: 78.7117 - val_loss: 62.8536 - val_mean_squared_error: 62.8536\n",
            "Epoch 314/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.0650 - mean_squared_error: 65.0650 - val_loss: 66.1977 - val_mean_squared_error: 66.1977\n",
            "Epoch 315/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.1475 - mean_squared_error: 84.1475 - val_loss: 64.0953 - val_mean_squared_error: 64.0953\n",
            "Epoch 316/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.4328 - mean_squared_error: 54.4328 - val_loss: 62.6452 - val_mean_squared_error: 62.6452\n",
            "Epoch 317/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64.5748 - mean_squared_error: 64.5748 - val_loss: 64.2448 - val_mean_squared_error: 64.2448\n",
            "Epoch 318/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 74.1912 - mean_squared_error: 74.1912 - val_loss: 63.9198 - val_mean_squared_error: 63.9198\n",
            "Epoch 319/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.2714 - mean_squared_error: 71.2714 - val_loss: 63.0821 - val_mean_squared_error: 63.0821\n",
            "Epoch 320/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.0895 - mean_squared_error: 54.0895 - val_loss: 64.2884 - val_mean_squared_error: 64.2884\n",
            "Epoch 321/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58.3027 - mean_squared_error: 58.3027 - val_loss: 65.0045 - val_mean_squared_error: 65.0045\n",
            "Epoch 322/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76.5912 - mean_squared_error: 76.5912 - val_loss: 62.1151 - val_mean_squared_error: 62.1151\n",
            "Epoch 323/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.2755 - mean_squared_error: 71.2755 - val_loss: 63.6542 - val_mean_squared_error: 63.6542\n",
            "Epoch 324/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.5414 - mean_squared_error: 59.5414 - val_loss: 64.7036 - val_mean_squared_error: 64.7036\n",
            "Epoch 325/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63.2551 - mean_squared_error: 63.2551 - val_loss: 64.3398 - val_mean_squared_error: 64.3398\n",
            "Epoch 326/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 53.8448 - mean_squared_error: 53.8448 - val_loss: 64.1013 - val_mean_squared_error: 64.1013\n",
            "Epoch 327/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83.3339 - mean_squared_error: 83.3339 - val_loss: 64.7799 - val_mean_squared_error: 64.7799\n",
            "Epoch 328/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76.3497 - mean_squared_error: 76.3497 - val_loss: 60.6394 - val_mean_squared_error: 60.6394\n",
            "Epoch 329/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.6111 - mean_squared_error: 70.6111 - val_loss: 62.4450 - val_mean_squared_error: 62.4450\n",
            "Epoch 330/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69.7116 - mean_squared_error: 69.7116 - val_loss: 63.3044 - val_mean_squared_error: 63.3044\n",
            "Epoch 331/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.9893 - mean_squared_error: 71.9893 - val_loss: 65.4572 - val_mean_squared_error: 65.4572\n",
            "Epoch 332/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.6342 - mean_squared_error: 59.6342 - val_loss: 63.2210 - val_mean_squared_error: 63.2210\n",
            "Epoch 333/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 72.5645 - mean_squared_error: 72.5645 - val_loss: 65.5858 - val_mean_squared_error: 65.5858\n",
            "Epoch 334/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.1200 - mean_squared_error: 71.1200 - val_loss: 64.0526 - val_mean_squared_error: 64.0526\n",
            "Epoch 335/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57.9436 - mean_squared_error: 57.9436 - val_loss: 63.3723 - val_mean_squared_error: 63.3723\n",
            "Epoch 336/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.7475 - mean_squared_error: 75.7475 - val_loss: 61.8007 - val_mean_squared_error: 61.8007\n",
            "Epoch 337/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.2950 - mean_squared_error: 65.2950 - val_loss: 62.7562 - val_mean_squared_error: 62.7562\n",
            "Epoch 338/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.7078 - mean_squared_error: 59.7078 - val_loss: 63.9185 - val_mean_squared_error: 63.9185\n",
            "Epoch 339/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.8293 - mean_squared_error: 83.8293 - val_loss: 63.1491 - val_mean_squared_error: 63.1491\n",
            "Epoch 340/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 58.1439 - mean_squared_error: 58.1439 - val_loss: 62.4571 - val_mean_squared_error: 62.4571\n",
            "Epoch 341/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 76.2081 - mean_squared_error: 76.2081 - val_loss: 63.5866 - val_mean_squared_error: 63.5866\n",
            "Epoch 342/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 68.2116 - mean_squared_error: 68.2116 - val_loss: 61.8604 - val_mean_squared_error: 61.8604\n",
            "Epoch 343/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 78.3350 - mean_squared_error: 78.3350 - val_loss: 62.6617 - val_mean_squared_error: 62.6617\n",
            "Epoch 344/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 56.4016 - mean_squared_error: 56.4016 - val_loss: 63.7128 - val_mean_squared_error: 63.7128\n",
            "Epoch 345/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.3982 - mean_squared_error: 73.3982 - val_loss: 63.3500 - val_mean_squared_error: 63.3500\n",
            "Epoch 346/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.6636 - mean_squared_error: 62.6636 - val_loss: 63.0411 - val_mean_squared_error: 63.0411\n",
            "Epoch 347/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.1769 - mean_squared_error: 66.1769 - val_loss: 63.3725 - val_mean_squared_error: 63.3725\n",
            "Epoch 348/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.1244 - mean_squared_error: 67.1244 - val_loss: 64.5709 - val_mean_squared_error: 64.5709\n",
            "Epoch 349/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83.7950 - mean_squared_error: 83.7950 - val_loss: 63.4630 - val_mean_squared_error: 63.4630\n",
            "Epoch 350/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 65.0847 - mean_squared_error: 65.0847 - val_loss: 62.6845 - val_mean_squared_error: 62.6845\n",
            "Epoch 351/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 70.0928 - mean_squared_error: 70.0928 - val_loss: 63.4736 - val_mean_squared_error: 63.4736\n",
            "Epoch 352/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 71.0396 - mean_squared_error: 71.0396 - val_loss: 63.4656 - val_mean_squared_error: 63.4656\n",
            "Epoch 353/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.0274 - mean_squared_error: 85.0274 - val_loss: 63.0792 - val_mean_squared_error: 63.0792\n",
            "Epoch 354/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90.5484 - mean_squared_error: 90.5484 - val_loss: 63.8680 - val_mean_squared_error: 63.8680\n",
            "Epoch 355/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 59.8662 - mean_squared_error: 59.8662 - val_loss: 63.7920 - val_mean_squared_error: 63.7920\n",
            "Epoch 356/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 79.0679 - mean_squared_error: 79.0679 - val_loss: 62.9341 - val_mean_squared_error: 62.9341\n",
            "Epoch 357/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.0209 - mean_squared_error: 70.0209 - val_loss: 63.9069 - val_mean_squared_error: 63.9069\n",
            "Epoch 358/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 53.6215 - mean_squared_error: 53.6215 - val_loss: 62.8962 - val_mean_squared_error: 62.8962\n",
            "Epoch 359/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 73.6937 - mean_squared_error: 73.6937 - val_loss: 62.9433 - val_mean_squared_error: 62.9433\n",
            "Epoch 360/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61.3259 - mean_squared_error: 61.3259 - val_loss: 63.1185 - val_mean_squared_error: 63.1185\n",
            "Epoch 361/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78.6992 - mean_squared_error: 78.6992 - val_loss: 62.7805 - val_mean_squared_error: 62.7805\n",
            "Epoch 362/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56.3399 - mean_squared_error: 56.3399 - val_loss: 62.4548 - val_mean_squared_error: 62.4548\n",
            "Epoch 363/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.0486 - mean_squared_error: 63.0486 - val_loss: 62.2136 - val_mean_squared_error: 62.2136\n",
            "Epoch 364/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.3386 - mean_squared_error: 84.3386 - val_loss: 62.6229 - val_mean_squared_error: 62.6229\n",
            "Epoch 365/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87.7503 - mean_squared_error: 87.7503 - val_loss: 63.0384 - val_mean_squared_error: 63.0384\n",
            "Epoch 366/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60.5506 - mean_squared_error: 60.5506 - val_loss: 62.7480 - val_mean_squared_error: 62.7480\n",
            "Epoch 367/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.9429 - mean_squared_error: 57.9429 - val_loss: 64.2566 - val_mean_squared_error: 64.2566\n",
            "Epoch 368/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.2721 - mean_squared_error: 70.2721 - val_loss: 63.7505 - val_mean_squared_error: 63.7505\n",
            "Epoch 369/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62.3549 - mean_squared_error: 62.3549 - val_loss: 62.7622 - val_mean_squared_error: 62.7622\n",
            "Epoch 370/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.8757 - mean_squared_error: 59.8757 - val_loss: 62.6712 - val_mean_squared_error: 62.6712\n",
            "Epoch 371/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 59.3886 - mean_squared_error: 59.3886 - val_loss: 64.3773 - val_mean_squared_error: 64.3773\n",
            "Epoch 372/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 56.4020 - mean_squared_error: 56.4020 - val_loss: 64.5556 - val_mean_squared_error: 64.5556\n",
            "Epoch 373/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 51.9508 - mean_squared_error: 51.9508 - val_loss: 61.8574 - val_mean_squared_error: 61.8574\n",
            "Epoch 374/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 84.6222 - mean_squared_error: 84.6222 - val_loss: 61.7452 - val_mean_squared_error: 61.7452\n",
            "Epoch 375/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 70.7927 - mean_squared_error: 70.7927 - val_loss: 61.6835 - val_mean_squared_error: 61.6835\n",
            "Epoch 376/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 70.5705 - mean_squared_error: 70.5705 - val_loss: 62.2585 - val_mean_squared_error: 62.2585\n",
            "Epoch 377/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 57.5253 - mean_squared_error: 57.5253 - val_loss: 63.4902 - val_mean_squared_error: 63.4902\n",
            "Epoch 378/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 79.1033 - mean_squared_error: 79.1033 - val_loss: 63.1801 - val_mean_squared_error: 63.1801\n",
            "Epoch 379/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 69.6955 - mean_squared_error: 69.6955 - val_loss: 62.2291 - val_mean_squared_error: 62.2291\n",
            "Epoch 380/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 49.6493 - mean_squared_error: 49.6493 - val_loss: 63.3556 - val_mean_squared_error: 63.3556\n",
            "Epoch 381/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 84.7813 - mean_squared_error: 84.7813 - val_loss: 63.0013 - val_mean_squared_error: 63.0013\n",
            "Epoch 382/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 62.4757 - mean_squared_error: 62.4757 - val_loss: 62.9892 - val_mean_squared_error: 62.9892\n",
            "Epoch 383/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 79.9320 - mean_squared_error: 79.9320 - val_loss: 62.6366 - val_mean_squared_error: 62.6366\n",
            "Epoch 384/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 74.7035 - mean_squared_error: 74.7035 - val_loss: 62.9568 - val_mean_squared_error: 62.9568\n",
            "Epoch 385/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 68.2946 - mean_squared_error: 68.2946 - val_loss: 62.7395 - val_mean_squared_error: 62.7395\n",
            "Epoch 386/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76.8250 - mean_squared_error: 76.8250 - val_loss: 63.3823 - val_mean_squared_error: 63.3823\n",
            "Epoch 387/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 64.2670 - mean_squared_error: 64.2670 - val_loss: 64.4963 - val_mean_squared_error: 64.4963\n",
            "Epoch 388/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 65.1526 - mean_squared_error: 65.1526 - val_loss: 63.4136 - val_mean_squared_error: 63.4136\n",
            "Epoch 389/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 53.8261 - mean_squared_error: 53.8261 - val_loss: 63.8992 - val_mean_squared_error: 63.8992\n",
            "Epoch 390/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 74.6130 - mean_squared_error: 74.6130 - val_loss: 63.4410 - val_mean_squared_error: 63.4410\n",
            "Epoch 391/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 46.4630 - mean_squared_error: 46.4630 - val_loss: 64.1805 - val_mean_squared_error: 64.1805\n",
            "Epoch 392/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 76.9551 - mean_squared_error: 76.9551 - val_loss: 63.4165 - val_mean_squared_error: 63.4165\n",
            "Epoch 393/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 70.6432 - mean_squared_error: 70.6432 - val_loss: 62.7363 - val_mean_squared_error: 62.7363\n",
            "Epoch 394/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 79.2313 - mean_squared_error: 79.2313 - val_loss: 62.9839 - val_mean_squared_error: 62.9839\n",
            "Epoch 395/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 81.9820 - mean_squared_error: 81.9820 - val_loss: 61.8369 - val_mean_squared_error: 61.8369\n",
            "Epoch 396/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 54.0477 - mean_squared_error: 54.0477 - val_loss: 62.5722 - val_mean_squared_error: 62.5722\n",
            "Epoch 397/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 76.3587 - mean_squared_error: 76.3587 - val_loss: 62.5951 - val_mean_squared_error: 62.5951\n",
            "Epoch 398/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 64.2002 - mean_squared_error: 64.2002 - val_loss: 62.7684 - val_mean_squared_error: 62.7684\n",
            "Epoch 399/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65.8392 - mean_squared_error: 65.8392 - val_loss: 62.1484 - val_mean_squared_error: 62.1484\n",
            "Epoch 400/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 70.4080 - mean_squared_error: 70.4080 - val_loss: 62.9672 - val_mean_squared_error: 62.9672\n",
            "Epoch 401/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.0359 - mean_squared_error: 64.0359 - val_loss: 61.9670 - val_mean_squared_error: 61.9670\n",
            "Epoch 402/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 63.7744 - mean_squared_error: 63.7744 - val_loss: 63.0677 - val_mean_squared_error: 63.0677\n",
            "Epoch 403/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 71.2014 - mean_squared_error: 71.2014 - val_loss: 62.4737 - val_mean_squared_error: 62.4737\n",
            "Epoch 404/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 76.3186 - mean_squared_error: 76.3186 - val_loss: 62.7394 - val_mean_squared_error: 62.7394\n",
            "Epoch 405/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 61.6026 - mean_squared_error: 61.6026 - val_loss: 62.7108 - val_mean_squared_error: 62.7108\n",
            "Epoch 406/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 71.9629 - mean_squared_error: 71.9629 - val_loss: 62.2401 - val_mean_squared_error: 62.2401\n",
            "Epoch 407/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 70.9063 - mean_squared_error: 70.9063 - val_loss: 63.9746 - val_mean_squared_error: 63.9746\n",
            "Epoch 408/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 74.1983 - mean_squared_error: 74.1983 - val_loss: 63.0084 - val_mean_squared_error: 63.0084\n",
            "Epoch 409/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 51.1294 - mean_squared_error: 51.1294 - val_loss: 63.6831 - val_mean_squared_error: 63.6831\n",
            "Epoch 410/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 78.1313 - mean_squared_error: 78.1313 - val_loss: 64.3334 - val_mean_squared_error: 64.3334\n",
            "Epoch 411/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 62.5910 - mean_squared_error: 62.5910 - val_loss: 63.8487 - val_mean_squared_error: 63.8487\n",
            "Epoch 412/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 52.6446 - mean_squared_error: 52.6446 - val_loss: 63.5464 - val_mean_squared_error: 63.5464\n",
            "Epoch 413/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 80.9766 - mean_squared_error: 80.9766 - val_loss: 61.9854 - val_mean_squared_error: 61.9854\n",
            "Epoch 414/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 66.9067 - mean_squared_error: 66.9067 - val_loss: 62.6239 - val_mean_squared_error: 62.6239\n",
            "Epoch 415/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 67.7435 - mean_squared_error: 67.7435 - val_loss: 62.8860 - val_mean_squared_error: 62.8860\n",
            "Epoch 416/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 54.7448 - mean_squared_error: 54.7448 - val_loss: 63.5830 - val_mean_squared_error: 63.5830\n",
            "Epoch 417/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 60.0581 - mean_squared_error: 60.0581 - val_loss: 64.0074 - val_mean_squared_error: 64.0074\n",
            "Epoch 418/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.2863 - mean_squared_error: 70.2863 - val_loss: 64.0117 - val_mean_squared_error: 64.0117\n",
            "Epoch 419/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63.4763 - mean_squared_error: 63.4763 - val_loss: 62.2520 - val_mean_squared_error: 62.2520\n",
            "Epoch 420/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48.5272 - mean_squared_error: 48.5272 - val_loss: 63.8674 - val_mean_squared_error: 63.8674\n",
            "Epoch 421/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 70.4752 - mean_squared_error: 70.4752 - val_loss: 62.5461 - val_mean_squared_error: 62.5461\n",
            "Epoch 422/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.1603 - mean_squared_error: 68.1603 - val_loss: 64.5896 - val_mean_squared_error: 64.5896\n",
            "Epoch 423/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 58.7793 - mean_squared_error: 58.7793 - val_loss: 62.0975 - val_mean_squared_error: 62.0975\n",
            "Epoch 424/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.7220 - mean_squared_error: 55.7220 - val_loss: 62.6068 - val_mean_squared_error: 62.6068\n",
            "Epoch 425/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.9154 - mean_squared_error: 64.9154 - val_loss: 62.1782 - val_mean_squared_error: 62.1782\n",
            "Epoch 426/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.9461 - mean_squared_error: 54.9461 - val_loss: 62.2543 - val_mean_squared_error: 62.2543\n",
            "Epoch 427/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.8603 - mean_squared_error: 65.8603 - val_loss: 62.8453 - val_mean_squared_error: 62.8453\n",
            "Epoch 428/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 79.6915 - mean_squared_error: 79.6915 - val_loss: 62.5562 - val_mean_squared_error: 62.5562\n",
            "Epoch 429/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 63.7353 - mean_squared_error: 63.7353 - val_loss: 61.6882 - val_mean_squared_error: 61.6882\n",
            "Epoch 430/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.5847 - mean_squared_error: 80.5847 - val_loss: 62.1065 - val_mean_squared_error: 62.1065\n",
            "Epoch 431/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.8142 - mean_squared_error: 57.8142 - val_loss: 62.5780 - val_mean_squared_error: 62.5780\n",
            "Epoch 432/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76.2958 - mean_squared_error: 76.2958 - val_loss: 62.8846 - val_mean_squared_error: 62.8846\n",
            "Epoch 433/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.6990 - mean_squared_error: 73.6990 - val_loss: 63.0697 - val_mean_squared_error: 63.0697\n",
            "Epoch 434/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 49.1055 - mean_squared_error: 49.1055 - val_loss: 61.7372 - val_mean_squared_error: 61.7372\n",
            "Epoch 435/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 110.8637 - mean_squared_error: 110.8637 - val_loss: 97.5403 - val_mean_squared_error: 97.5403\n",
            "Epoch 436/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 122.2066 - mean_squared_error: 122.2066 - val_loss: 63.5612 - val_mean_squared_error: 63.5612\n",
            "Epoch 437/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 121.9263 - mean_squared_error: 121.9263 - val_loss: 61.5625 - val_mean_squared_error: 61.5625\n",
            "Epoch 438/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 70.9286 - mean_squared_error: 70.9286 - val_loss: 62.5979 - val_mean_squared_error: 62.5979\n",
            "Epoch 439/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 103.1382 - mean_squared_error: 103.1382 - val_loss: 61.2564 - val_mean_squared_error: 61.2564\n",
            "Epoch 440/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 83.5304 - mean_squared_error: 83.5304 - val_loss: 63.1253 - val_mean_squared_error: 63.1253\n",
            "Epoch 441/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.6951 - mean_squared_error: 74.6951 - val_loss: 62.1676 - val_mean_squared_error: 62.1676\n",
            "Epoch 442/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 61.7430 - mean_squared_error: 61.7430 - val_loss: 62.7517 - val_mean_squared_error: 62.7517\n",
            "Epoch 443/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.3749 - mean_squared_error: 70.3749 - val_loss: 63.5873 - val_mean_squared_error: 63.5873\n",
            "Epoch 444/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 75.8834 - mean_squared_error: 75.8834 - val_loss: 63.2210 - val_mean_squared_error: 63.2210\n",
            "Epoch 445/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 77.2829 - mean_squared_error: 77.2829 - val_loss: 63.3110 - val_mean_squared_error: 63.3110\n",
            "Epoch 446/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60.7268 - mean_squared_error: 60.7268 - val_loss: 61.6136 - val_mean_squared_error: 61.6136\n",
            "Epoch 447/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.0112 - mean_squared_error: 68.0112 - val_loss: 62.8331 - val_mean_squared_error: 62.8331\n",
            "Epoch 448/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.1536 - mean_squared_error: 68.1536 - val_loss: 62.2249 - val_mean_squared_error: 62.2249\n",
            "Epoch 449/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.6418 - mean_squared_error: 84.6418 - val_loss: 63.0933 - val_mean_squared_error: 63.0933\n",
            "Epoch 450/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 73.0828 - mean_squared_error: 73.0828 - val_loss: 62.7428 - val_mean_squared_error: 62.7428\n",
            "Epoch 451/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.6921 - mean_squared_error: 70.6921 - val_loss: 62.8318 - val_mean_squared_error: 62.8318\n",
            "Epoch 452/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.9918 - mean_squared_error: 65.9918 - val_loss: 63.2560 - val_mean_squared_error: 63.2560\n",
            "Epoch 453/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 66.3053 - mean_squared_error: 66.3053 - val_loss: 62.8981 - val_mean_squared_error: 62.8981\n",
            "Epoch 454/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 49.3919 - mean_squared_error: 49.3919 - val_loss: 62.5057 - val_mean_squared_error: 62.5057\n",
            "Epoch 455/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 81.8688 - mean_squared_error: 81.8688 - val_loss: 61.8630 - val_mean_squared_error: 61.8630\n",
            "Epoch 456/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 60.6031 - mean_squared_error: 60.6031 - val_loss: 62.1306 - val_mean_squared_error: 62.1306\n",
            "Epoch 457/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.6905 - mean_squared_error: 58.6905 - val_loss: 62.5191 - val_mean_squared_error: 62.5191\n",
            "Epoch 458/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.2145 - mean_squared_error: 54.2145 - val_loss: 62.3658 - val_mean_squared_error: 62.3658\n",
            "Epoch 459/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71.7167 - mean_squared_error: 71.7167 - val_loss: 63.2676 - val_mean_squared_error: 63.2676\n",
            "Epoch 460/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 81.5192 - mean_squared_error: 81.5192 - val_loss: 63.6890 - val_mean_squared_error: 63.6890\n",
            "Epoch 461/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 60.9749 - mean_squared_error: 60.9749 - val_loss: 64.0467 - val_mean_squared_error: 64.0467\n",
            "Epoch 462/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 61.3386 - mean_squared_error: 61.3386 - val_loss: 62.1775 - val_mean_squared_error: 62.1775\n",
            "Epoch 463/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64.5067 - mean_squared_error: 64.5067 - val_loss: 63.2311 - val_mean_squared_error: 63.2311\n",
            "Epoch 464/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.8579 - mean_squared_error: 75.8579 - val_loss: 65.1156 - val_mean_squared_error: 65.1156\n",
            "Epoch 465/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 64.5055 - mean_squared_error: 64.5055 - val_loss: 63.7499 - val_mean_squared_error: 63.7499\n",
            "Epoch 466/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 60.2519 - mean_squared_error: 60.2519 - val_loss: 62.9972 - val_mean_squared_error: 62.9972\n",
            "Epoch 467/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 68.6307 - mean_squared_error: 68.6307 - val_loss: 63.6155 - val_mean_squared_error: 63.6155\n",
            "Epoch 468/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 52.0284 - mean_squared_error: 52.0284 - val_loss: 61.1730 - val_mean_squared_error: 61.1730\n",
            "Epoch 469/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 64.8732 - mean_squared_error: 64.8732 - val_loss: 62.9342 - val_mean_squared_error: 62.9342\n",
            "Epoch 470/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 80.3432 - mean_squared_error: 80.3432 - val_loss: 62.0905 - val_mean_squared_error: 62.0905\n",
            "Epoch 471/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 73.5863 - mean_squared_error: 73.5863 - val_loss: 62.0257 - val_mean_squared_error: 62.0257\n",
            "Epoch 472/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 70.2922 - mean_squared_error: 70.2922 - val_loss: 62.5111 - val_mean_squared_error: 62.5111\n",
            "Epoch 473/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.4662 - mean_squared_error: 68.4662 - val_loss: 62.8473 - val_mean_squared_error: 62.8473\n",
            "Epoch 474/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 80.7169 - mean_squared_error: 80.7169 - val_loss: 63.2145 - val_mean_squared_error: 63.2145\n",
            "Epoch 475/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 54.1463 - mean_squared_error: 54.1463 - val_loss: 62.4685 - val_mean_squared_error: 62.4685\n",
            "Epoch 476/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.8655 - mean_squared_error: 73.8655 - val_loss: 62.5182 - val_mean_squared_error: 62.5182\n",
            "Epoch 477/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92.1555 - mean_squared_error: 92.1555 - val_loss: 61.4220 - val_mean_squared_error: 61.4220\n",
            "Epoch 478/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.1311 - mean_squared_error: 68.1311 - val_loss: 61.9798 - val_mean_squared_error: 61.9798\n",
            "Epoch 479/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 68.9746 - mean_squared_error: 68.9746 - val_loss: 62.4184 - val_mean_squared_error: 62.4184\n",
            "Epoch 480/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 66.6535 - mean_squared_error: 66.6535 - val_loss: 62.7567 - val_mean_squared_error: 62.7567\n",
            "Epoch 481/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70.8488 - mean_squared_error: 70.8488 - val_loss: 62.6042 - val_mean_squared_error: 62.6042\n",
            "Epoch 482/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76.7617 - mean_squared_error: 76.7617 - val_loss: 63.9160 - val_mean_squared_error: 63.9160\n",
            "Epoch 483/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.1077 - mean_squared_error: 67.1077 - val_loss: 63.2166 - val_mean_squared_error: 63.2166\n",
            "Epoch 484/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 83.5333 - mean_squared_error: 83.5333 - val_loss: 62.4451 - val_mean_squared_error: 62.4451\n",
            "Epoch 485/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 75.4829 - mean_squared_error: 75.4829 - val_loss: 62.2367 - val_mean_squared_error: 62.2367\n",
            "Epoch 486/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 63.6835 - mean_squared_error: 63.6835 - val_loss: 64.4066 - val_mean_squared_error: 64.4066\n",
            "Epoch 487/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.7850 - mean_squared_error: 55.7850 - val_loss: 64.5954 - val_mean_squared_error: 64.5954\n",
            "Epoch 488/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 110.8588 - mean_squared_error: 110.8588 - val_loss: 64.3703 - val_mean_squared_error: 64.3703\n",
            "Epoch 489/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 61.6390 - mean_squared_error: 61.6390 - val_loss: 65.9976 - val_mean_squared_error: 65.9976\n",
            "Epoch 490/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.0018 - mean_squared_error: 68.0018 - val_loss: 63.2635 - val_mean_squared_error: 63.2635\n",
            "Epoch 491/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 75.9633 - mean_squared_error: 75.9633 - val_loss: 64.4918 - val_mean_squared_error: 64.4918\n",
            "Epoch 492/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.7830 - mean_squared_error: 94.7830 - val_loss: 62.4318 - val_mean_squared_error: 62.4318\n",
            "Epoch 493/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.3296 - mean_squared_error: 70.3296 - val_loss: 63.5926 - val_mean_squared_error: 63.5926\n",
            "Epoch 494/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 81.9662 - mean_squared_error: 81.9662 - val_loss: 62.4726 - val_mean_squared_error: 62.4726\n",
            "Epoch 495/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 53.3442 - mean_squared_error: 53.3442 - val_loss: 62.1902 - val_mean_squared_error: 62.1902\n",
            "Epoch 496/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 66.6802 - mean_squared_error: 66.6802 - val_loss: 62.6707 - val_mean_squared_error: 62.6707\n",
            "Epoch 497/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 76.9821 - mean_squared_error: 76.9821 - val_loss: 62.6624 - val_mean_squared_error: 62.6624\n",
            "Epoch 498/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 96.3965 - mean_squared_error: 96.3965 - val_loss: 63.3124 - val_mean_squared_error: 63.3124\n",
            "Epoch 499/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 64.8949 - mean_squared_error: 64.8949 - val_loss: 61.9866 - val_mean_squared_error: 61.9866\n",
            "Epoch 500/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 68.3574 - mean_squared_error: 68.3574 - val_loss: 62.9871 - val_mean_squared_error: 62.9871\n",
            "Epoch 501/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 67.8205 - mean_squared_error: 67.8205 - val_loss: 62.1890 - val_mean_squared_error: 62.1890\n",
            "Epoch 502/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 68.5480 - mean_squared_error: 68.5480 - val_loss: 61.6782 - val_mean_squared_error: 61.6782\n",
            "Epoch 503/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63.2337 - mean_squared_error: 63.2337 - val_loss: 62.6941 - val_mean_squared_error: 62.6941\n",
            "Epoch 504/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91.7274 - mean_squared_error: 91.7274 - val_loss: 62.5630 - val_mean_squared_error: 62.5630\n",
            "Epoch 505/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.8680 - mean_squared_error: 71.8680 - val_loss: 62.5181 - val_mean_squared_error: 62.5181\n",
            "Epoch 506/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.0578 - mean_squared_error: 73.0578 - val_loss: 61.4317 - val_mean_squared_error: 61.4317\n",
            "Epoch 507/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.4443 - mean_squared_error: 84.4443 - val_loss: 62.0929 - val_mean_squared_error: 62.0929\n",
            "Epoch 508/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.4202 - mean_squared_error: 65.4202 - val_loss: 61.8157 - val_mean_squared_error: 61.8157\n",
            "Epoch 509/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 61.1466 - mean_squared_error: 61.1466 - val_loss: 61.4764 - val_mean_squared_error: 61.4764\n",
            "Epoch 510/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.6413 - mean_squared_error: 72.6413 - val_loss: 63.1497 - val_mean_squared_error: 63.1497\n",
            "Epoch 511/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78.7846 - mean_squared_error: 78.7846 - val_loss: 63.0766 - val_mean_squared_error: 63.0766\n",
            "Epoch 512/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 98.1980 - mean_squared_error: 98.1980 - val_loss: 62.6766 - val_mean_squared_error: 62.6766\n",
            "Epoch 513/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 75.4726 - mean_squared_error: 75.4726 - val_loss: 63.6275 - val_mean_squared_error: 63.6275\n",
            "Epoch 514/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 80.3793 - mean_squared_error: 80.3793 - val_loss: 63.0997 - val_mean_squared_error: 63.0997\n",
            "Epoch 515/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 81.7708 - mean_squared_error: 81.7708 - val_loss: 62.3469 - val_mean_squared_error: 62.3469\n",
            "Epoch 516/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 87.7481 - mean_squared_error: 87.7481 - val_loss: 62.1315 - val_mean_squared_error: 62.1315\n",
            "Epoch 517/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67.8652 - mean_squared_error: 67.8652 - val_loss: 61.5435 - val_mean_squared_error: 61.5435\n",
            "Epoch 518/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 65.8333 - mean_squared_error: 65.8333 - val_loss: 62.1458 - val_mean_squared_error: 62.1458\n",
            "Epoch 519/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 73.1355 - mean_squared_error: 73.1355 - val_loss: 61.4637 - val_mean_squared_error: 61.4637\n",
            "Epoch 520/520\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 80.9964 - mean_squared_error: 80.9964 - val_loss: 62.2996 - val_mean_squared_error: 62.2996\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 64.05738886774597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I only print df where room is bedroom\n",
        "print(df[df['bedroom'] == 1][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBn0QJjq9jAO",
        "outputId": "0b6c6eb4-aa64-411e-e37b-efb86788d36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    hour     room  light_level  next_light_level  light_level_lag_1  \\\n",
            "1      4  bedroom            0               0.0                0.0   \n",
            "3      5  bedroom            0               0.0                0.0   \n",
            "5      6  bedroom            0               0.0                0.0   \n",
            "7      7  bedroom            0               0.0                0.0   \n",
            "9      8  bedroom            0              93.0                0.0   \n",
            "11     9  bedroom           93              92.0                0.0   \n",
            "13    10  bedroom           92              92.0               93.0   \n",
            "15    11  bedroom           92              91.0               92.0   \n",
            "17    12  bedroom           91              89.0               92.0   \n",
            "19    13  bedroom           89              91.0               91.0   \n",
            "21    14  bedroom           91              93.0               89.0   \n",
            "23    15  bedroom           93              93.0               91.0   \n",
            "25    16  bedroom           93               4.0               93.0   \n",
            "27    17  bedroom            4              10.0               93.0   \n",
            "29    18  bedroom           10               6.0                4.0   \n",
            "31    19  bedroom            6               9.0               10.0   \n",
            "33    20  bedroom            9               7.0                6.0   \n",
            "35    21  bedroom            7               9.0                9.0   \n",
            "37    22  bedroom            9               4.0                7.0   \n",
            "39    23  bedroom            4               0.0                9.0   \n",
            "41     0  bedroom            0               0.0                4.0   \n",
            "43     1  bedroom            0               0.0                0.0   \n",
            "45     2  bedroom            0               0.0                0.0   \n",
            "47     3  bedroom            0               0.0                0.0   \n",
            "49     4  bedroom            0               0.0                0.0   \n",
            "51     5  bedroom            0               0.0                0.0   \n",
            "53     6  bedroom            0               0.0                0.0   \n",
            "55     7  bedroom            0               0.0                0.0   \n",
            "57     8  bedroom            0              93.0                0.0   \n",
            "59     9  bedroom           93              89.0                0.0   \n",
            "\n",
            "    light_level_lag_2  light_level_lag_3  balcony  bedroom  \n",
            "1                 0.0                0.0      0.0      1.0  \n",
            "3                 0.0                0.0      0.0      1.0  \n",
            "5                 0.0                0.0      0.0      1.0  \n",
            "7                 0.0                0.0      0.0      1.0  \n",
            "9                 0.0                0.0      0.0      1.0  \n",
            "11                0.0                0.0      0.0      1.0  \n",
            "13                0.0                0.0      0.0      1.0  \n",
            "15               93.0                0.0      0.0      1.0  \n",
            "17               92.0               93.0      0.0      1.0  \n",
            "19               92.0               92.0      0.0      1.0  \n",
            "21               91.0               92.0      0.0      1.0  \n",
            "23               89.0               91.0      0.0      1.0  \n",
            "25               91.0               89.0      0.0      1.0  \n",
            "27               93.0               91.0      0.0      1.0  \n",
            "29               93.0               93.0      0.0      1.0  \n",
            "31                4.0               93.0      0.0      1.0  \n",
            "33               10.0                4.0      0.0      1.0  \n",
            "35                6.0               10.0      0.0      1.0  \n",
            "37                9.0                6.0      0.0      1.0  \n",
            "39                7.0                9.0      0.0      1.0  \n",
            "41                9.0                7.0      0.0      1.0  \n",
            "43                4.0                9.0      0.0      1.0  \n",
            "45                0.0                4.0      0.0      1.0  \n",
            "47                0.0                0.0      0.0      1.0  \n",
            "49                0.0                0.0      0.0      1.0  \n",
            "51                0.0                0.0      0.0      1.0  \n",
            "53                0.0                0.0      0.0      1.0  \n",
            "55                0.0                0.0      0.0      1.0  \n",
            "57                0.0                0.0      0.0      1.0  \n",
            "59                0.0                0.0      0.0      1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jzolXAa8YJ0",
        "outputId": "09108092-6a24-4786-e977-97b3975e188f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Room       | Current Light Level | Real Next Light | Predicted Next Light\n",
            "living_room | 75.73              | 73.57            | 77.03\n",
            "bedroom    | 94.07              | 73.35            | 71.16\n",
            "living_room | 74.45              | 63.63            | 77.20\n",
            "balcony    | 95.92              | 61.15            | 75.59\n",
            "bedroom    | 65.77              | 84.99            | 74.36\n",
            "living_room | 99.43              | 69.35            | 79.59\n",
            "balcony    | 76.47              | 80.85            | 81.44\n",
            "living_room | 98.60              | 61.18            | 72.31\n",
            "living_room | 90.85              | 62.39            | 82.77\n",
            "living_room | 69.94              | 75.34            | 81.83\n",
            "living_room | 67.89              | 88.79            | 71.26\n",
            "living_room | 63.18              | 82.80            | 76.29\n",
            "balcony    | 98.57              | 97.10            | 77.11\n",
            "living_room | 61.33              | 83.53            | 74.99\n",
            "bedroom    | 63.86              | 93.35            | 83.27\n",
            "bedroom    | 86.32              | 62.09            | 82.00\n",
            "living_room | 90.07              | 89.31            | 79.54\n",
            "bedroom    | 89.01              | 79.01            | 75.53\n",
            "living_room | 83.97              | 71.05            | 81.31\n",
            "bedroom    | 82.75              | 68.04            | 75.14\n",
            "balcony    | 80.06              | 78.56            | 78.05\n",
            "bedroom    | 63.30              | 64.69            | 81.73\n",
            "balcony    | 95.30              | 65.20            | 80.42\n",
            "balcony    | 80.92              | 60.80            | 79.98\n",
            "bedroom    | 87.65              | 83.97            | 76.65\n",
            "bedroom    | 87.06              | 60.36            | 84.71\n",
            "bedroom    | 75.14              | 82.31            | 79.46\n",
            "living_room | 78.47              | 61.19            | 79.61\n",
            "balcony    | 89.08              | 84.89            | 83.15\n",
            "bedroom    | 61.82              | 85.20            | 78.56\n",
            "balcony    | 89.44              | 72.83            | 82.82\n",
            "living_room | 81.00              | 67.21            | 80.22\n",
            "bedroom    | 82.49              | 74.91            | 77.16\n",
            "living_room | 89.99              | 94.10            | 81.66\n",
            "bedroom    | 65.95              | 63.17            | 76.38\n",
            "bedroom    | 76.53              | 61.23            | 77.91\n",
            "balcony    | 66.16              | 70.15            | 78.85\n",
            "living_room | 63.26              | 88.81            | 79.38\n",
            "bedroom    | 64.87              | 62.76            | 75.25\n",
            "balcony    | 69.92              | 87.69            | 80.31\n",
            "bedroom    | 90.04              | 80.98            | 82.81\n",
            "living_room | 79.23              | 96.71            | 76.31\n",
            "balcony    | 60.64              | 74.65            | 75.64\n",
            "bedroom    | 93.82              | 73.60            | 85.76\n",
            "balcony    | 68.47              | 72.88            | 77.37\n",
            "balcony    | 80.85              | 98.01            | 79.75\n",
            "bedroom    | 64.58              | 64.87            | 73.87\n",
            "bedroom    | 92.37              | 71.04            | 80.02\n",
            "living_room | 96.59              | 79.34            | 88.27\n",
            "bedroom    | 93.50              | 67.00            | 88.85\n",
            "living_room | 66.35              | 93.13            | 76.99\n",
            "bedroom    | 62.88              | 79.75            | 75.28\n",
            "living_room | 71.92              | 90.02            | 80.90\n",
            "balcony    | 60.12              | 92.05            | 76.40\n",
            "living_room | 87.26              | 79.92            | 78.26\n",
            "living_room | 68.96              | 80.73            | 85.97\n",
            "living_room | 65.27              | 68.17            | 77.36\n",
            "bedroom    | 96.40              | 89.13            | 83.44\n",
            "balcony    | 66.10              | 92.93            | 81.46\n",
            "balcony    | 64.04              | 67.25            | 81.86\n",
            "balcony    | 72.83              | 64.40            | 80.82\n",
            "bedroom    | 84.06              | 89.68            | 73.67\n",
            "living_room | 88.65              | 82.28            | 82.45\n",
            "bedroom    | 92.59              | 65.09            | 79.60\n",
            "living_room | 79.34              | 88.14            | 77.81\n",
            "living_room | 92.88              | 68.28            | 81.16\n",
            "living_room | 95.58              | 68.50            | 82.43\n",
            "living_room | 87.31              | 80.89            | 79.31\n",
            "living_room | 84.18              | 91.01            | 82.56\n",
            "living_room | 60.17              | 93.98            | 83.43\n",
            "balcony    | 79.89              | 95.56            | 77.38\n",
            "living_room | 71.70              | 66.63            | 83.11\n",
            "living_room | 78.11              | 62.68            | 78.81\n",
            "living_room | 84.29              | 96.57            | 79.13\n",
            "balcony    | 80.81              | 99.93            | 83.59\n",
            "balcony    | 96.97              | 80.28            | 82.03\n",
            "bedroom    | 70.89              | 84.49            | 80.39\n",
            "balcony    | 77.23              | 61.35            | 80.91\n",
            "living_room | 98.72              | 62.90            | 76.63\n",
            "balcony    | 74.62              | 71.17            | 81.26\n",
            "living_room | 83.04              | 73.86            | 79.74\n",
            "bedroom    | 82.70              | 89.08            | 74.11\n",
            "balcony    | 95.62              | 91.47            | 75.58\n",
            "bedroom    | 77.49              | 64.77            | 78.08\n",
            "living_room | 67.15              | 95.08            | 82.34\n",
            "bedroom    | 98.06              | 91.20            | 81.30\n",
            "living_room | 98.71              | 78.48            | 71.80\n",
            "living_room | 77.71              | 97.59            | 78.21\n",
            "bedroom    | 64.57              | 74.25            | 73.87\n",
            "living_room | 87.11              | 73.69            | 78.26\n",
            "living_room | 91.89              | 91.78            | 81.99\n",
            "living_room | 86.57              | 85.57            | 78.90\n",
            "living_room | 78.91              | 98.03            | 83.15\n",
            "balcony    | 66.47              | 87.71            | 79.32\n",
            "living_room | 75.20              | 99.29            | 79.83\n",
            "living_room | 86.13              | 95.02            | 78.48\n",
            "living_room | 97.07              | 84.16            | 81.64\n",
            "living_room | 90.68              | 76.85            | 83.08\n",
            "balcony    | 99.81              | 64.43            | 90.94\n",
            "living_room | 77.84              | 67.81            | 78.44\n",
            "balcony    | 63.73              | 78.24            | 79.87\n",
            "balcony    | 93.43              | 81.92            | 78.42\n",
            "bedroom    | 62.67              | 82.89            | 78.64\n",
            "bedroom    | 60.54              | 84.15            | 81.67\n",
            "living_room | 65.81              | 93.65            | 74.59\n",
            "living_room | 78.34              | 97.16            | 76.30\n",
            "living_room | 98.25              | 69.52            | 75.57\n",
            "living_room | 97.61              | 80.26            | 76.72\n",
            "living_room | 89.97              | 72.99            | 80.27\n",
            "balcony    | 93.03              | 87.55            | 84.25\n",
            "living_room | 64.63              | 95.55            | 79.68\n",
            "balcony    | 64.37              | 60.80            | 78.59\n",
            "bedroom    | 86.90              | 87.32            | 80.16\n",
            "living_room | 67.29              | 99.85            | 76.30\n",
            "balcony    | 73.64              | 82.61            | 78.42\n",
            "balcony    | 83.99              | 87.38            | 83.02\n",
            "living_room | 82.73              | 65.74            | 74.15\n",
            "balcony    | 98.43              | 70.65            | 82.59\n",
            "living_room | 73.68              | 68.12            | 76.05\n",
            "living_room | 69.35              | 97.04            | 76.40\n",
            "bedroom    | 76.38              | 94.66            | 78.16\n",
            "living_room | 77.16              | 90.61            | 77.97\n",
            "living_room | 61.46              | 94.88            | 79.05\n",
            "living_room | 65.91              | 77.32            | 76.61\n",
            "bedroom    | 62.80              | 76.21            | 73.65\n",
            "balcony    | 68.51              | 61.72            | 78.08\n",
            "balcony    | 97.33              | 68.80            | 91.96\n",
            "balcony    | 82.70              | 82.21            | 74.26\n",
            "bedroom    | 64.33              | 66.03            | 76.26\n",
            "balcony    | 64.96              | 79.25            | 81.87\n",
            "balcony    | 99.02              | 63.47            | 77.74\n",
            "bedroom    | 69.47              | 62.97            | 78.72\n",
            "balcony    | 69.50              | 77.10            | 78.76\n",
            "living_room | 69.29              | 96.59            | 79.69\n",
            "bedroom    | 96.91              | 77.40            | 74.17\n",
            "bedroom    | 60.15              | 78.74            | 94.55\n",
            "bedroom    | 66.39              | 70.54            | 78.23\n",
            "bedroom    | 83.21              | 71.89            | 79.21\n",
            "balcony    | 80.38              | 81.43            | 76.69\n",
            "living_room | 62.50              | 79.70            | 83.15\n",
            "balcony    | 89.33              | 63.42            | 82.29\n",
            "bedroom    | 74.19              | 92.37            | 80.34\n",
            "balcony    | 63.44              | 74.50            | 83.07\n",
            "bedroom    | 71.42              | 71.42            | 84.44\n",
            "balcony    | 97.62              | 91.24            | 81.69\n",
            "bedroom    | 70.50              | 81.37            | 77.78\n",
            "balcony    | 62.16              | 93.29            | 79.74\n",
            "bedroom    | 77.76              | 70.09            | 82.16\n",
            "living_room | 89.39              | 60.48            | 86.23\n",
            "balcony    | 66.02              | 64.45            | 81.28\n",
            "living_room | 85.27              | 97.05            | 77.27\n",
            "bedroom    | 61.39              | 79.40            | 81.25\n",
            "balcony    | 89.66              | 60.88            | 83.71\n",
            "bedroom    | 80.12              | 82.45            | 82.22\n",
            "living_room | 88.08              | 70.86            | 76.92\n",
            "bedroom    | 92.37              | 64.57            | 89.17\n",
            "balcony    | 63.27              | 88.79            | 82.70\n",
            "balcony    | 60.38              | 83.14            | 84.36\n",
            "living_room | 91.12              | 88.72            | 82.38\n",
            "living_room | 84.92              | 68.69            | 81.50\n",
            "bedroom    | 90.52              | 71.88            | 79.66\n",
            "balcony    | 90.46              | 95.10            | 79.27\n",
            "bedroom    | 97.25              | 85.81            | 66.26\n",
            "living_room | 91.60              | 98.88            | 80.28\n",
            "bedroom    | 69.91              | 83.26            | 80.18\n",
            "bedroom    | 81.69              | 85.48            | 79.68\n",
            "balcony    | 71.66              | 78.60            | 81.01\n",
            "balcony    | 68.58              | 81.51            | 80.76\n",
            "bedroom    | 90.07              | 63.31            | 79.07\n",
            "bedroom    | 95.39              | 87.10            | 70.86\n",
            "bedroom    | 72.94              | 80.92            | 74.33\n",
            "balcony    | 61.93              | 92.44            | 76.70\n",
            "balcony    | 68.83              | 72.16            | 78.08\n",
            "balcony    | 68.54              | 84.53            | 83.59\n",
            "balcony    | 79.38              | 64.83            | 84.57\n",
            "balcony    | 61.76              | 94.46            | 86.33\n",
            "balcony    | 70.41              | 76.89            | 79.77\n",
            "bedroom    | 83.91              | 68.39            | 80.63\n",
            "bedroom    | 97.17              | 99.44            | 78.29\n",
            "living_room | 76.08              | 93.95            | 77.97\n",
            "living_room | 64.33              | 96.18            | 76.90\n",
            "living_room | 68.94              | 90.85            | 79.47\n",
            "balcony    | 98.75              | 72.42            | 91.40\n",
            "living_room | 98.87              | 79.86            | 75.70\n",
            "balcony    | 95.46              | 61.05            | 83.36\n",
            "bedroom    | 68.46              | 80.12            | 76.73\n",
            "living_room | 87.03              | 80.94            | 78.26\n",
            "bedroom    | 95.78              | 82.45            | 85.94\n",
            "balcony    | 68.79              | 75.62            | 81.76\n",
            "balcony    | 92.93              | 82.08            | 79.61\n",
            "balcony    | 66.08              | 80.58            | 81.46\n",
            "balcony    | 76.75              | 89.42            | 80.12\n",
            "balcony    | 89.35              | 70.92            | 91.19\n",
            "balcony    | 85.62              | 80.20            | 81.82\n",
            "bedroom    | 78.39              | 87.51            | 76.59\n",
            "bedroom    | 99.77              | 68.20            | 87.04\n",
            "living_room | 93.55              | 97.68            | 81.01\n",
            "bedroom    | 86.59              | 60.20            | 73.06\n",
            "balcony    | 60.69              | 97.67            | 73.36\n",
            "living_room | 97.00              | 81.10            | 82.83\n",
            "bedroom    | 66.88              | 61.21            | 80.90\n",
            "balcony    | 97.12              | 99.07            | 79.29\n",
            "living_room | 92.84              | 79.23            | 81.09\n",
            "balcony    | 86.91              | 78.23            | 80.23\n",
            "living_room | 61.45              | 64.70            | 82.36\n",
            "balcony    | 60.80              | 96.28            | 78.28\n",
            "bedroom    | 85.81              | 85.66            | 83.56\n",
            "balcony    | 71.71              | 70.81            | 81.01\n",
            "living_room | 81.62              | 98.87            | 75.65\n",
            "bedroom    | 80.93              | 70.28            | 80.19\n",
            "bedroom    | 90.83              | 65.88            | 89.26\n",
            "bedroom    | 77.05              | 92.58            | 77.04\n",
            "bedroom    | 86.01              | 94.52            | 83.00\n",
            "balcony    | 72.97              | 97.50            | 75.60\n",
            "balcony    | 93.96              | 66.85            | 80.01\n",
            "bedroom    | 83.60              | 67.14            | 77.28\n",
            "balcony    | 84.52              | 82.31            | 79.40\n",
            "bedroom    | 69.64              | 78.17            | 80.18\n",
            "balcony    | 83.47              | 91.89            | 80.08\n",
            "living_room | 68.31              | 89.15            | 79.40\n",
            "living_room | 79.70              | 75.69            | 79.21\n",
            "balcony    | 98.37              | 84.96            | 75.20\n",
            "balcony    | 89.21              | 80.35            | 80.92\n",
            "bedroom    | 60.27              | 87.45            | 89.01\n",
            "living_room | 76.58              | 64.59            | 77.97\n",
            "bedroom    | 97.94              | 68.14            | 74.30\n",
            "living_room | 97.73              | 70.55            | 78.96\n",
            "living_room | 82.08              | 80.25            | 80.22\n",
            "balcony    | 80.70              | 60.25            | 79.68\n",
            "bedroom    | 95.80              | 80.41            | 84.54\n",
            "bedroom    | 91.47              | 97.94            | 80.94\n",
            "living_room | 61.62              | 66.62            | 77.22\n",
            "living_room | 89.54              | 97.66            | 83.91\n",
            "balcony    | 70.81              | 88.57            | 83.03\n",
            "living_room | 94.92              | 93.55            | 81.25\n",
            "bedroom    | 79.04              | 85.98            | 83.81\n",
            "living_room | 73.25              | 97.87            | 80.95\n",
            "living_room | 83.82              | 78.85            | 83.15\n",
            "living_room | 83.83              | 60.17            | 81.31\n",
            "living_room | 62.11              | 74.82            | 79.66\n",
            "balcony    | 84.83              | 79.43            | 80.88\n",
            "balcony    | 70.20              | 91.84            | 78.08\n",
            "bedroom    | 72.48              | 88.03            | 82.28\n",
            "balcony    | 79.17              | 96.62            | 77.28\n",
            "bedroom    | 80.12              | 69.56            | 81.70\n",
            "balcony    | 88.35              | 92.89            | 82.03\n",
            "living_room | 96.57              | 80.51            | 80.83\n",
            "balcony    | 85.20              | 89.35            | 80.02\n",
            "balcony    | 91.75              | 79.94            | 82.66\n",
            "bedroom    | 93.16              | 74.77            | 85.87\n",
            "living_room | 66.47              | 69.61            | 69.63\n",
            "balcony    | 85.46              | 70.45            | 83.66\n",
            "bedroom    | 71.88              | 66.39            | 81.01\n",
            "living_room | 67.65              | 94.23            | 86.41\n",
            "living_room | 92.76              | 81.94            | 78.36\n",
            "living_room | 89.69              | 94.72            | 82.35\n",
            "balcony    | 98.05              | 66.45            | 86.84\n",
            "balcony    | 77.34              | 63.02            | 80.03\n",
            "balcony    | 72.79              | 97.73            | 80.82\n",
            "balcony    | 84.87              | 68.40            | 82.87\n",
            "living_room | 75.29              | 71.02            | 76.22\n",
            "living_room | 98.03              | 62.11            | 76.57\n",
            "balcony    | 64.30              | 91.77            | 78.60\n",
            "living_room | 89.15              | 88.40            | 83.57\n",
            "balcony    | 81.43              | 70.81            | 79.68\n",
            "living_room | 68.10              | 62.08            | 78.59\n",
            "balcony    | 86.09              | 78.00            | 83.03\n",
            "living_room | 98.04              | 91.92            | 76.57\n",
            "balcony    | 83.69              | 70.53            | 82.99\n",
            "living_room | 85.09              | 62.99            | 82.59\n",
            "balcony    | 62.96              | 88.19            | 83.48\n",
            "living_room | 81.11              | 64.72            | 80.24\n",
            "balcony    | 95.33              | 70.87            | 75.88\n",
            "bedroom    | 66.38              | 91.26            | 78.30\n",
            "living_room | 94.49              | 65.88            | 84.86\n",
            "balcony    | 68.66              | 72.83            | 83.59\n",
            "living_room | 97.87              | 75.29            | 81.81\n",
            "living_room | 63.75              | 68.16            | 78.00\n",
            "living_room | 65.85              | 90.40            | 77.45\n",
            "living_room | 73.52              | 62.81            | 78.41\n",
            "bedroom    | 63.71              | 82.11            | 76.99\n",
            "balcony    | 73.87              | 70.57            | 79.34\n",
            "bedroom    | 96.55              | 61.82            | 77.18\n",
            "balcony    | 73.71              | 63.24            | 78.42\n",
            "balcony    | 70.03              | 84.64            | 84.52\n",
            "bedroom    | 61.21              | 79.04            | 79.04\n",
            "living_room | 69.68              | 82.33            | 79.52\n",
            "bedroom    | 76.03              | 94.47            | 79.01\n",
            "living_room | 68.84              | 80.80            | 80.01\n",
            "balcony    | 97.46              | 91.95            | 83.86\n",
            "balcony    | 68.95              | 60.49            | 81.76\n",
            "living_room | 64.93              | 69.80            | 78.19\n",
            "living_room | 78.72              | 95.92            | 80.30\n",
            "balcony    | 97.55              | 84.98            | 82.28\n",
            "balcony    | 97.93              | 91.88            | 77.06\n",
            "bedroom    | 78.29              | 88.18            | 76.24\n",
            "balcony    | 67.25              | 64.98            | 79.40\n",
            "balcony    | 93.46              | 87.59            | 80.46\n",
            "living_room | 76.42              | 83.76            | 78.74\n",
            "living_room | 74.09              | 90.24            | 77.64\n",
            "balcony    | 82.35              | 85.55            | 83.00\n",
            "balcony    | 77.36              | 72.50            | 79.91\n",
            "bedroom    | 93.48              | 67.81            | 88.85\n",
            "living_room | 75.28              | 81.00            | 83.74\n",
            "balcony    | 86.08              | 92.50            | 82.44\n",
            "bedroom    | 87.01              | 85.20            | 80.09\n",
            "living_room | 87.75              | 86.19            | 78.26\n",
            "bedroom    | 83.58              | 99.67            | 79.24\n",
            "bedroom    | 81.24              | 61.69            | 80.19\n",
            "balcony    | 79.69              | 62.00            | 80.36\n",
            "bedroom    | 67.67              | 90.03            | 80.36\n",
            "living_room | 61.79              | 97.84            | 82.08\n",
            "bedroom    | 91.48              | 84.18            | 84.67\n",
            "living_room | 89.41              | 89.56            | 90.29\n",
            "living_room | 89.91              | 67.89            | 82.49\n",
            "balcony    | 64.29              | 84.22            | 79.18\n",
            "bedroom    | 60.53              | 85.17            | 79.70\n",
            "balcony    | 82.20              | 75.15            | 79.68\n",
            "balcony    | 82.61              | 70.49            | 85.91\n",
            "bedroom    | 89.09              | 78.16            | 76.73\n",
            "balcony    | 61.59              | 87.26            | 82.14\n",
            "bedroom    | 74.05              | 70.76            | 74.83\n",
            "living_room | 89.70              | 87.05            | 82.83\n",
            "living_room | 88.43              | 68.22            | 83.50\n",
            "bedroom    | 78.17              | 64.66            | 78.84\n",
            "living_room | 92.48              | 83.04            | 82.17\n",
            "living_room | 98.02              | 83.52            | 74.51\n",
            "bedroom    | 97.34              | 78.70            | 82.48\n",
            "balcony    | 75.11              | 85.01            | 80.16\n",
            "bedroom    | 64.24              | 76.58            | 74.81\n",
            "bedroom    | 65.64              | 97.02            | 75.00\n",
            "living_room | 64.70              | 83.66            | 82.69\n",
            "bedroom    | 80.95              | 64.01            | 80.19\n",
            "bedroom    | 64.66              | 77.23            | 76.93\n",
            "bedroom    | 81.89              | 71.42            | 79.84\n",
            "living_room | 74.34              | 70.01            | 79.27\n",
            "balcony    | 76.90              | 91.91            | 77.64\n",
            "bedroom    | 63.09              | 96.40            | 77.01\n",
            "bedroom    | 99.44              | 74.43            | 75.91\n",
            "bedroom    | 87.47              | 84.73            | 76.17\n",
            "living_room | 99.30              | 60.55            | 85.74\n",
            "bedroom    | 65.68              | 95.39            | 81.26\n",
            "bedroom    | 68.89              | 95.96            | 79.24\n",
            "living_room | 60.01              | 69.99            | 79.99\n",
            "balcony    | 86.51              | 91.59            | 79.01\n",
            "living_room | 94.17              | 65.92            | 85.90\n",
            "living_room | 62.10              | 67.29            | 86.44\n",
            "bedroom    | 96.34              | 94.07            | 78.47\n",
            "bedroom    | 89.19              | 89.01            | 77.03\n",
            "bedroom    | 75.43              | 60.51            | 78.60\n",
            "bedroom    | 67.93              | 70.09            | 74.32\n",
            "bedroom    | 92.72              | 69.63            | 80.20\n",
            "living_room | 73.58              | 69.20            | 78.29\n",
            "bedroom    | 68.03              | 88.02            | 74.20\n",
            "living_room | 61.15              | 68.39            | 78.05\n",
            "balcony    | 67.46              | 76.48            | 73.45\n",
            "balcony    | 95.60              | 72.44            | 79.74\n",
            "living_room | 70.59              | 76.79            | 75.31\n",
            "balcony    | 95.72              | 65.20            | 86.68\n",
            "living_room | 81.26              | 99.92            | 80.20\n",
            "bedroom    | 73.72              | 69.42            | 75.83\n",
            "living_room | 92.29              | 99.44            | 75.66\n",
            "living_room | 78.49              | 84.13            | 80.21\n",
            "balcony    | 96.52              | 63.44            | 82.69\n",
            "balcony    | 85.62              | 62.20            | 80.93\n",
            "living_room | 85.89              | 78.50            | 82.03\n",
            "balcony    | 64.75              | 88.17            | 79.21\n",
            "balcony    | 61.86              | 87.38            | 84.34\n",
            "living_room | 94.70              | 73.31            | 81.82\n",
            "living_room | 83.53              | 91.07            | 81.19\n",
            "bedroom    | 86.89              | 89.92            | 79.66\n",
            "living_room | 64.87              | 80.22            | 72.22\n",
            "living_room | 84.08              | 81.28            | 81.31\n",
            "balcony    | 74.21              | 86.34            | 82.74\n",
            "balcony    | 77.74              | 64.70            | 77.88\n",
            "bedroom    | 96.75              | 95.46            | 79.83\n",
            "living_room | 91.42              | 73.08            | 80.95\n",
            "living_room | 67.07              | 97.78            | 75.50\n",
            "balcony    | 77.04              | 89.56            | 76.54\n",
            "bedroom    | 99.78              | 62.56            | 82.84\n",
            "living_room | 99.06              | 65.03            | 71.24\n",
            "balcony    | 98.01              | 92.98            | 80.53\n",
            "balcony    | 97.41              | 99.08            | 82.27\n",
            "living_room | 81.94              | 67.80            | 80.20\n",
            "bedroom    | 84.08              | 84.66            | 80.63\n",
            "bedroom    | 61.41              | 95.65            | 72.59\n",
            "balcony    | 79.49              | 93.46            | 84.27\n",
            "balcony    | 71.17              | 77.08            | 83.56\n",
            "bedroom    | 68.87              | 90.46            | 78.72\n",
            "bedroom    | 87.51              | 61.17            | 78.51\n",
            "living_room | 65.94              | 76.15            | 76.61\n",
            "balcony    | 68.91              | 81.38            | 80.09\n",
            "bedroom    | 74.48              | 81.51            | 83.11\n",
            "balcony    | 99.78              | 71.21            | 86.32\n",
            "bedroom    | 61.52              | 99.92            | 79.04\n",
            "bedroom    | 60.83              | 71.64            | 79.69\n",
            "living_room | 65.29              | 83.14            | 75.38\n",
            "living_room | 98.87              | 91.88            | 73.92\n",
            "balcony    | 63.72              | 84.89            | 89.33\n",
            "balcony    | 69.58              | 79.38            | 80.18\n",
            "living_room | 90.03              | 68.62            | 80.65\n",
            "bedroom    | 67.32              | 97.79            | 80.78\n",
            "balcony    | 69.76              | 86.58            | 79.51\n",
            "living_room | 60.65              | 90.07            | 81.16\n",
            "bedroom    | 93.10              | 86.24            | 95.51\n",
            "bedroom    | 88.28              | 99.46            | 81.88\n",
            "balcony    | 77.14              | 94.26            | 81.44\n",
            "balcony    | 81.17              | 61.04            | 80.33\n",
            "balcony    | 79.25              | 64.62            | 80.33\n",
            "bedroom    | 84.27              | 68.42            | 78.33\n",
            "balcony    | 60.72              | 82.47            | 74.58\n",
            "bedroom    | 70.19              | 86.49            | 80.18\n",
            "living_room | 84.13              | 69.29            | 81.21\n",
            "balcony    | 79.43              | 64.62            | 84.27\n",
            "living_room | 84.03              | 60.35            | 81.16\n",
            "bedroom    | 89.85              | 63.09            | 88.95\n",
            "bedroom    | 84.01              | 90.17            | 82.52\n",
            "balcony    | 96.23              | 95.61            | 78.38\n",
            "living_room | 88.93              | 73.61            | 76.84\n",
            "living_room | 82.83              | 73.53            | 76.11\n",
            "balcony    | 96.97              | 74.17            | 74.70\n",
            "balcony    | 89.23              | 85.01            | 80.22\n",
            "living_room | 62.99              | 74.07            | 79.57\n",
            "bedroom    | 70.28              | 97.67            | 79.77\n",
            "bedroom    | 74.16              | 63.96            | 77.56\n",
            "bedroom    | 69.66              | 89.69            | 79.77\n",
            "balcony    | 84.73              | 84.61            | 88.05\n",
            "balcony    | 73.69              | 72.54            | 78.05\n",
            "balcony    | 68.20              | 67.86            | 74.52\n",
            "bedroom    | 65.94              | 88.42            | 76.19\n",
            "balcony    | 79.29              | 76.01            | 78.23\n",
            "living_room | 68.53              | 86.13            | 79.81\n",
            "bedroom    | 68.70              | 90.97            | 77.54\n",
            "balcony    | 67.19              | 61.82            | 78.19\n",
            "balcony    | 68.96              | 78.48            | 79.24\n",
            "living_room | 63.93              | 63.00            | 77.25\n",
            "bedroom    | 97.51              | 65.94            | 83.07\n",
            "balcony    | 71.09              | 96.00            | 83.56\n",
            "living_room | 78.85              | 76.40            | 81.99\n",
            "living_room | 63.39              | 90.74            | 76.87\n",
            "bedroom    | 64.94              | 76.17            | 76.69\n",
            "bedroom    | 94.20              | 80.08            | 71.16\n",
            "bedroom    | 76.30              | 88.84            | 81.92\n",
            "balcony    | 82.01              | 73.77            | 80.19\n",
            "living_room | 83.34              | 79.38            | 81.19\n",
            "living_room | 92.45              | 86.60            | 78.77\n",
            "balcony    | 96.09              | 62.05            | 85.72\n",
            "balcony    | 98.58              | 87.64            | 77.16\n",
            "living_room | 80.37              | 85.27            | 81.11\n",
            "bedroom    | 73.42              | 92.18            | 77.46\n",
            "balcony    | 85.84              | 65.33            | 81.78\n",
            "bedroom    | 98.92              | 61.91            | 70.99\n",
            "bedroom    | 99.92              | 99.05            | 77.83\n",
            "bedroom    | 96.56              | 91.04            | 77.98\n",
            "balcony    | 62.57              | 94.40            | 88.26\n",
            "living_room | 63.63              | 94.70            | 76.81\n",
            "balcony    | 64.18              | 95.60            | 79.18\n",
            "living_room | 74.93              | 84.92            | 80.21\n",
            "bedroom    | 82.11              | 79.04            | 79.92\n",
            "bedroom    | 87.89              | 78.78            | 84.17\n",
            "bedroom    | 92.97              | 93.36            | 78.40\n",
            "balcony    | 93.24              | 85.51            | 86.94\n",
            "bedroom    | 79.05              | 68.61            | 82.34\n",
            "bedroom    | 86.24              | 74.49            | 84.67\n",
            "living_room | 60.48              | 71.56            | 80.82\n",
            "living_room | 88.83              | 78.91            | 86.11\n",
            "balcony    | 63.69              | 94.14            | 78.59\n",
            "balcony    | 60.92              | 95.51            | 76.72\n",
            "balcony    | 97.59              | 83.02            | 76.74\n",
            "balcony    | 85.65              | 90.26            | 80.58\n",
            "balcony    | 71.21              | 98.52            | 79.58\n",
            "balcony    | 90.31              | 92.10            | 83.73\n",
            "balcony    | 64.44              | 63.27            | 79.50\n",
            "bedroom    | 71.42              | 72.94            | 77.48\n",
            "balcony    | 63.84              | 97.41            | 78.94\n",
            "balcony    | 76.89              | 99.02            | 81.44\n",
            "bedroom    | 82.11              | 78.78            | 78.81\n",
            "balcony    | 82.15              | 66.28            | 77.43\n",
            "bedroom    | 85.09              | 83.45            | 83.51\n",
            "bedroom    | 98.87              | 73.55            | 76.45\n",
            "bedroom    | 98.83              | 76.02            | 88.54\n",
            "living_room | 64.72              | 97.51            | 80.83\n",
            "balcony    | 63.87              | 89.38            | 79.36\n",
            "balcony    | 65.76              | 93.93            | 81.39\n",
            "bedroom    | 90.35              | 79.76            | 81.28\n",
            "balcony    | 88.38              | 86.08            | 82.03\n",
            "bedroom    | 96.87              | 84.79            | 80.59\n",
            "balcony    | 76.03              | 73.97            | 77.64\n",
            "bedroom    | 88.03              | 89.27            | 76.65\n",
            "bedroom    | 75.62              | 89.94            | 81.43\n",
            "bedroom    | 96.27              | 82.92            | 90.44\n",
            "living_room | 69.97              | 73.64            | 81.24\n",
            "living_room | 77.84              | 63.93            | 78.21\n",
            "living_room | 62.14              | 91.79            | 78.31\n",
            "living_room | 66.47              | 69.68            | 78.28\n",
            "balcony    | 61.15              | 62.82            | 88.87\n",
            "bedroom    | 82.87              | 74.74            | 79.86\n",
            "living_room | 78.92              | 70.03            | 76.54\n",
            "living_room | 73.81              | 62.81            | 78.29\n",
            "bedroom    | 77.35              | 69.88            | 78.16\n",
            "balcony    | 91.12              | 80.70            | 82.59\n",
            "bedroom    | 87.66              | 98.23            | 80.07\n",
            "balcony    | 99.93              | 67.25            | 82.79\n",
            "bedroom    | 81.29              | 65.83            | 79.68\n",
            "bedroom    | 76.76              | 84.43            | 78.16\n",
            "living_room | 83.69              | 62.41            | 81.16\n",
            "living_room | 85.84              | 85.86            | 82.11\n",
            "bedroom    | 79.55              | 92.62            | 81.70\n",
            "balcony    | 74.01              | 88.65            | 79.34\n",
            "balcony    | 93.47              | 78.65            | 78.42\n",
            "living_room | 69.00              | 87.31            | 79.69\n",
            "living_room | 95.62              | 66.82            | 76.73\n",
            "living_room | 99.92              | 67.81            | 84.37\n",
            "living_room | 72.20              | 75.17            | 80.90\n",
            "balcony    | 64.57              | 62.16            | 81.78\n",
            "living_room | 68.17              | 78.53            | 77.10\n",
            "bedroom    | 92.38              | 62.91            | 79.68\n",
            "balcony    | 98.03              | 90.31            | 86.66\n",
            "bedroom    | 63.67              | 73.19            | 76.26\n",
            "living_room | 98.83              | 75.99            | 77.32\n",
            "balcony    | 85.08              | 68.93            | 86.45\n",
            "balcony    | 94.70              | 83.03            | 87.16\n",
            "bedroom    | 96.20              | 96.59            | 77.49\n",
            "bedroom    | 79.01              | 87.41            | 83.96\n",
            "bedroom    | 68.79              | 79.45            | 87.03\n",
            "bedroom    | 94.66              | 85.95            | 85.27\n",
            "balcony    | 72.47              | 98.44            | 81.32\n",
            "bedroom    | 75.37              | 92.37            | 75.20\n",
            "bedroom    | 93.91              | 83.29            | 83.39\n",
            "living_room | 70.86              | 67.19            | 84.27\n",
            "bedroom    | 97.00              | 87.67            | 70.58\n",
            "balcony    | 98.34              | 75.00            | 78.62\n",
            "living_room | 90.07              | 60.82            | 82.59\n",
            "balcony    | 76.52              | 92.09            | 75.51\n",
            "balcony    | 98.93              | 70.39            | 91.29\n",
            "bedroom    | 92.19              | 94.95            | 80.75\n",
            "balcony    | 98.14              | 80.81            | 84.91\n",
            "bedroom    | 62.71              | 91.73            | 81.90\n",
            "bedroom    | 81.87              | 91.83            | 76.97\n",
            "bedroom    | 78.16              | 67.87            | 76.95\n",
            "bedroom    | 87.02              | 67.43            | 81.26\n",
            "bedroom    | 79.40              | 99.51            | 78.30\n",
            "living_room | 63.89              | 70.59            | 71.51\n",
            "living_room | 92.31              | 78.45            | 89.32\n",
            "bedroom    | 88.68              | 68.31            | 84.11\n",
            "living_room | 99.26              | 84.65            | 85.74\n",
            "bedroom    | 85.14              | 82.28            | 85.15\n",
            "bedroom    | 91.14              | 62.31            | 84.85\n",
            "bedroom    | 82.15              | 64.64            | 81.21\n",
            "balcony    | 86.19              | 82.64            | 81.79\n",
            "bedroom    | 88.60              | 93.01            | 85.54\n",
            "living_room | 69.20              | 60.74            | 79.02\n",
            "living_room | 83.51              | 88.75            | 81.19\n",
            "balcony    | 97.79              | 68.20            | 81.40\n",
            "living_room | 61.30              | 85.95            | 77.03\n",
            "balcony    | 99.54              | 86.67            | 87.97\n",
            "balcony    | 67.22              | 90.28            | 76.07\n",
            "balcony    | 63.71              | 89.66            | 79.36\n",
            "living_room | 60.41              | 84.76            | 82.82\n",
            "bedroom    | 65.83              | 61.90            | 73.58\n",
            "bedroom    | 89.01              | 99.92            | 77.03\n",
            "bedroom    | 92.92              | 86.83            | 86.70\n",
            "balcony    | 91.07              | 85.65            | 80.55\n",
            "bedroom    | 77.92              | 84.80            | 76.95\n",
            "living_room | 63.95              | 84.58            | 81.37\n",
            "balcony    | 62.82              | 85.20            | 86.06\n",
            "bedroom    | 68.28              | 78.16            | 77.41\n",
            "balcony    | 74.58              | 70.65            | 83.21\n",
            "balcony    | 98.70              | 81.08            | 77.16\n",
            "balcony    | 88.46              | 66.26            | 86.84\n",
            "bedroom    | 85.66              | 97.51            | 73.49\n",
            "bedroom    | 91.62              | 78.91            | 80.76\n",
            "bedroom    | 78.07              | 93.90            | 78.40\n",
            "balcony    | 94.70              | 64.03            | 87.16\n",
            "living_room | 91.07              | 95.41            | 79.07\n",
            "bedroom    | 88.79              | 92.92            | 83.73\n",
            "bedroom    | 87.62              | 71.33            | 76.17\n",
            "living_room | 93.56              | 78.98            | 81.01\n",
            "bedroom    | 92.99              | 92.97            | 82.47\n",
            "living_room | 74.05              | 69.92            | 74.72\n",
            "living_room | 75.17              | 83.40            | 80.21\n",
            "balcony    | 92.09              | 74.46            | 71.75\n",
            "living_room | 96.67              | 69.75            | 70.27\n",
            "bedroom    | 74.00              | 76.64            | 74.83\n",
            "living_room | 73.03              | 83.82            | 80.97\n",
            "living_room | 64.59              | 90.07            | 76.08\n",
            "living_room | 81.28              | 60.13            | 78.27\n",
            "bedroom    | 93.88              | 78.61            | 81.44\n",
            "living_room | 72.99              | 90.12            | 81.81\n",
            "living_room | 90.80              | 81.01            | 81.94\n",
            "living_room | 74.28              | 69.14            | 78.18\n",
            "balcony    | 96.18              | 93.99            | 84.70\n",
            "bedroom    | 93.01              | 75.40            | 85.87\n",
            "bedroom    | 68.21              | 68.97            | 82.55\n",
            "living_room | 92.98              | 100.00            | 77.23\n",
            "balcony    | 74.65              | 69.17            | 79.56\n",
            "bedroom    | 60.20              | 86.92            | 95.27\n",
            "balcony    | 78.65              | 64.76            | 77.67\n",
            "bedroom    | 67.50              | 97.21            | 85.51\n",
            "balcony    | 65.32              | 99.53            | 82.13\n",
            "balcony    | 66.26              | 89.24            | 83.19\n",
            "living_room | 95.05              | 64.19            | 72.67\n",
            "balcony    | 87.65              | 63.94            | 77.84\n",
            "bedroom    | 76.20              | 72.58            | 78.08\n",
            "balcony    | 61.96              | 75.42            | 85.72\n",
            "living_room | 78.68              | 91.36            | 84.22\n",
            "living_room | 63.00              | 97.00            | 79.57\n",
            "balcony    | 89.37              | 60.76            | 82.29\n",
            "bedroom    | 81.00              | 74.05            | 79.69\n",
            "bedroom    | 90.55              | 96.31            | 80.63\n",
            "living_room | 94.88              | 75.98            | 87.02\n",
            "bedroom    | 75.26              | 77.52            | 76.24\n",
            "balcony    | 86.59              | 93.43            | 80.13\n",
            "living_room | 88.72              | 89.97            | 82.45\n",
            "bedroom    | 67.81              | 87.65            | 76.57\n",
            "bedroom    | 77.45              | 65.98            | 78.08\n",
            "balcony    | 87.08              | 61.17            | 81.39\n",
            "bedroom    | 84.43              | 88.60            | 80.63\n",
            "living_room | 86.93              | 84.13            | 80.42\n",
            "balcony    | 94.20              | 86.57            | 75.66\n",
            "balcony    | 89.65              | 91.07            | 81.05\n",
            "balcony    | 85.51              | 94.80            | 77.38\n",
            "bedroom    | 63.05              | 77.55            | 73.09\n",
            "living_room | 64.70              | 91.17            | 77.67\n",
            "bedroom    | 67.00              | 64.32            | 80.15\n",
            "bedroom    | 78.60              | 88.39            | 75.53\n",
            "balcony    | 70.65              | 83.36            | 77.06\n",
            "living_room | 73.96              | 71.01            | 77.34\n",
            "living_room | 94.56              | 99.42            | 82.81\n",
            "balcony    | 77.94              | 60.51            | 80.03\n",
            "bedroom    | 91.26              | 70.12            | 83.03\n",
            "balcony    | 70.45              | 89.49            | 79.77\n",
            "bedroom    | 92.51              | 91.75            | 80.19\n"
          ]
        }
      ],
      "source": [
        "def predict_and_print(model, X_data, y_data):\n",
        "    \"\"\"\n",
        "    This function takes a trained model, input features (X_data), and real target values (y_data),\n",
        "    and prints the room, current light level, real light level, and predicted light level.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained XGBoost model\n",
        "    - X_data: Input features (e.g., test data or any new data)\n",
        "    - y_data: Actual light levels (real values) to compare predictions against\n",
        "    \"\"\"\n",
        "    # Make predictions on the input data\n",
        "    y_pred = model.predict(xgb.DMatrix(X_data))\n",
        "\n",
        "    # Iterate over the data and print the room, current light level, real, and predicted light levels\n",
        "    print(f'Room       | Current Light Level | Real Next Light | Predicted Next Light')\n",
        "\n",
        "    for i, (real, pred) in enumerate(zip(y_data, y_pred)):\n",
        "        # Extract the current light level from the 'light_level' column of X_data\n",
        "        current_light = X_data.iloc[i]['light_level']\n",
        "\n",
        "        # Determine the room by checking which column has a value of 1.0\n",
        "        room = X_data.iloc[i][['balcony', 'bedroom', 'living_room']].idxmax()\n",
        "\n",
        "        print(f'{room:<10} | {current_light:.2f}              | {real:.2f}            | {pred:.2f}')\n",
        "\n",
        "# Example usage:\n",
        "# Predict and print the results for the test data\n",
        "predict_and_print(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ORo7ptxP8-To",
        "outputId": "c56d0b5d-3a20-4bc9-be53-0da180da95c1"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9a728a23096e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_arraylike_not_scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_numpy_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_preserve_dia_indices_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_isfinite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFiniteStatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy_isfinite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0m_NUMPY_NAMESPACE_NAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array_api_compat.numpy\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    604\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 606\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_measurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmilp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearConstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m from scipy._lib._util import (check_random_state, MapWrapper, _get_nan,\n\u001b[1;32m     44\u001b[0m                               \u001b[0mrng_integers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rename_parameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_contains_nan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_optimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_minimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_root_scalar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_minpack_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_root.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_optimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemoizeJac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizeResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_check_unknown_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_minpack_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_root_hybr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleastsq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_spectral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_root_df_sane\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_nonlin\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_code_to_timestamp_pyc\u001b[0;34m(code, mtime, source_size)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Prepare the data as before\n",
        "df = pd.read_csv(\"light_data_with_hour_only.csv\")\n",
        "\n",
        "# Shift the light level to get the next light level\n",
        "df['next_light_level'] = df.groupby('room')['light_level'].shift(-1)\n",
        "df = df.dropna(subset=['next_light_level'])\n",
        "\n",
        "# One-hot encode 'room' column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_rooms = encoder.fit_transform(df[['room']])\n",
        "encoded_rooms_df = pd.DataFrame(encoded_rooms, columns=encoder.categories_[0])\n",
        "\n",
        "# Merge everything into a final DataFrame\n",
        "df = pd.concat([df, encoded_rooms_df], axis=1)\n",
        "X = df[['timestamp'] + list(encoded_rooms_df.columns) + ['light_level']]\n",
        "y = df['next_light_level']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the neural network model using TensorFlow (Keras)\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Input layer: First, we define the input shape, which is the number of features in X\n",
        "model.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
        "\n",
        "# Hidden layer 1: A dense layer with 64 neurons and ReLU activation function\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Hidden layer 2: A dense layer with 32 neurons and ReLU activation function\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "\n",
        "# Output layer: A single neuron to predict the next light level\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "# Compile the model: We'll use Mean Squared Error loss and Adam optimizer\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define the ModelCheckpoint callback to save only the best weights based on validation loss\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'NN_model.weights.keras',  # File path to save the best weights\n",
        "    monitor='val_loss',          # Monitor validation loss\n",
        "    save_best_only=True,         # Only save the weights when the model improves\n",
        "    mode='min',                  # Minimize the validation loss\n",
        "    verbose=1                    # Display a message when saving the model\n",
        ")\n",
        "\n",
        "# Train the model with the ModelCheckpoint callback\n",
        "model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Function to print real and predicted light levels\n",
        "def predict_and_print_nn(model, X_data, y_data):\n",
        "    \"\"\"\n",
        "    This function takes a trained model, input features (X_data), and real target values (y_data),\n",
        "    and prints the room, current light level, real light level, and predicted light level.\n",
        "    \"\"\"\n",
        "    # Make predictions on the input data\n",
        "    y_pred = model.predict(X_data)\n",
        "\n",
        "    # Iterate over the data and print the room, current light level, real, and predicted light levels\n",
        "    print(f'Room       | Current Light Level | Real Light Level | Predicted Light Level')\n",
        "\n",
        "    for i, (real, pred) in enumerate(zip(y_data, y_pred)):\n",
        "        # Extract the current light level from the 'light_level' column of X_data\n",
        "        current_light = X_data.iloc[i]['light_level']\n",
        "\n",
        "        # Determine the room by checking which column has a value of 1.0\n",
        "        room = X_data.iloc[i][['balcony', 'bedroom', 'living_room']].idxmax()\n",
        "\n",
        "        print(f'{room:<10} | {current_light:.2f}              | {real:.2f}            | {pred[0]:.2f}')\n",
        "\n",
        "# Example usage:\n",
        "# Predict and print the results for the test data\n",
        "predict_and_print_nn(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnnFxL7_Bv-N",
        "outputId": "cea19c8a-c314-4835-aa51-1156456e6297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "XGBoost Model Mean Squared Error: 39.7638\n",
            "Neural Network Model Mean Squared Error: 51.0926\n",
            "XGBoost performs better.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"light_data_with_hour_only.csv\")\n",
        "\n",
        "# Shift the light level to get the next light level\n",
        "df['next_light_level'] = df.groupby('room')['light_level'].shift(-1)\n",
        "df = df.dropna(subset=['next_light_level'])\n",
        "\n",
        "# One-hot encode 'room' column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_rooms = encoder.fit_transform(df[['room']])\n",
        "encoded_rooms_df = pd.DataFrame(encoded_rooms, columns=encoder.categories_[0])\n",
        "\n",
        "# Merge everything into a final DataFrame\n",
        "df = pd.concat([df, encoded_rooms_df], axis=1)\n",
        "X = df[['timestamp'] + list(encoded_rooms_df.columns) + ['light_level']]\n",
        "y = df['next_light_level']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------\n",
        "# Load the XGBoost model\n",
        "# ------------------------\n",
        "xgb_model = xgb.Booster()\n",
        "xgb_model.load_model('xgb_model.json')  # Load the XGBoost model\n",
        "\n",
        "# ------------------------\n",
        "# Load the Neural Network model\n",
        "# ------------------------\n",
        "nn_model = tf.keras.models.load_model('best_NN_model.weights.keras')  # Load the NN model\n",
        "\n",
        "# ------------------------\n",
        "# Make predictions using both models\n",
        "# ------------------------\n",
        "\n",
        "# XGBoost predictions\n",
        "dtest = xgb.DMatrix(X_test)\n",
        "xgb_predictions = xgb_model.predict(dtest)\n",
        "\n",
        "# Neural Network predictions\n",
        "nn_predictions = nn_model.predict(X_test)\n",
        "\n",
        "# ------------------------\n",
        "# Evaluate the performance using Mean Squared Error (MSE)\n",
        "# ------------------------\n",
        "\n",
        "# Calculate MSE for XGBoost\n",
        "xgb_mse = mean_squared_error(y_test, xgb_predictions)\n",
        "\n",
        "# Calculate MSE for Neural Network\n",
        "nn_mse = mean_squared_error(y_test, nn_predictions)\n",
        "\n",
        "# ------------------------\n",
        "# Compare the performances\n",
        "# ------------------------\n",
        "\n",
        "print(f'XGBoost Model Mean Squared Error: {xgb_mse:.4f}')\n",
        "print(f'Neural Network Model Mean Squared Error: {nn_mse:.4f}')\n",
        "\n",
        "# Additional information: which model performs better\n",
        "if xgb_mse < nn_mse:\n",
        "    print(\"XGBoost performs better.\")\n",
        "else:\n",
        "    print(\"Neural Network performs better.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GKvPeI5OJsmL",
        "outputId": "11c1c5e4-789c-4bc1-f4e2-31cc7ac80ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1574.5667\n",
            "Epoch 1: val_loss improved from inf to 965.01324, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 1551.5646 - val_loss: 965.0132\n",
            "Epoch 2/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1085.7258\n",
            "Epoch 2: val_loss improved from 965.01324 to 637.13733, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1095.5575 - val_loss: 637.1373\n",
            "Epoch 3/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 920.9799\n",
            "Epoch 3: val_loss improved from 637.13733 to 308.48425, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 882.6878 - val_loss: 308.4843\n",
            "Epoch 4/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 443.3070\n",
            "Epoch 4: val_loss improved from 308.48425 to 236.47534, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 439.7584 - val_loss: 236.4753\n",
            "Epoch 5/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 257.5950\n",
            "Epoch 5: val_loss did not improve from 236.47534\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 258.5898 - val_loss: 240.1864\n",
            "Epoch 6/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 273.1192\n",
            "Epoch 6: val_loss improved from 236.47534 to 221.44588, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 273.7050 - val_loss: 221.4459\n",
            "Epoch 7/500\n",
            "\u001b[1m24/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 274.6467   \n",
            "Epoch 7: val_loss improved from 221.44588 to 211.27470, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264.0356 - val_loss: 211.2747\n",
            "Epoch 8/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 249.8839 \n",
            "Epoch 8: val_loss improved from 211.27470 to 207.77213, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 257.9157 - val_loss: 207.7721\n",
            "Epoch 9/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 257.3724\n",
            "Epoch 9: val_loss improved from 207.77213 to 204.04260, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 256.2911 - val_loss: 204.0426\n",
            "Epoch 10/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 246.5763 \n",
            "Epoch 10: val_loss did not improve from 204.04260\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 247.5523 - val_loss: 205.7769\n",
            "Epoch 11/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 256.5042 \n",
            "Epoch 11: val_loss improved from 204.04260 to 203.40883, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 252.6171 - val_loss: 203.4088\n",
            "Epoch 12/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 243.9269\n",
            "Epoch 12: val_loss improved from 203.40883 to 196.79738, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 243.9051 - val_loss: 196.7974\n",
            "Epoch 13/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 300.8684 \n",
            "Epoch 13: val_loss improved from 196.79738 to 189.30885, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 279.7027 - val_loss: 189.3089\n",
            "Epoch 14/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 257.6431\n",
            "Epoch 14: val_loss improved from 189.30885 to 184.19769, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 254.3913 - val_loss: 184.1977\n",
            "Epoch 15/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 225.7998\n",
            "Epoch 15: val_loss improved from 184.19769 to 183.70609, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 225.7459 - val_loss: 183.7061\n",
            "Epoch 16/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 191.8079\n",
            "Epoch 16: val_loss improved from 183.70609 to 178.41719, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 192.9582 - val_loss: 178.4172\n",
            "Epoch 17/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 199.3366\n",
            "Epoch 17: val_loss improved from 178.41719 to 173.10898, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 200.8908 - val_loss: 173.1090\n",
            "Epoch 18/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 194.7040 \n",
            "Epoch 18: val_loss did not improve from 173.10898\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 199.2385 - val_loss: 181.7794\n",
            "Epoch 19/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 201.2177  \n",
            "Epoch 19: val_loss improved from 173.10898 to 169.13898, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 211.0952 - val_loss: 169.1390\n",
            "Epoch 20/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 187.8862  \n",
            "Epoch 20: val_loss did not improve from 169.13898\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 192.9922 - val_loss: 175.6451\n",
            "Epoch 21/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 196.7528\n",
            "Epoch 21: val_loss improved from 169.13898 to 162.75661, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 198.6844 - val_loss: 162.7566\n",
            "Epoch 22/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 193.2556 \n",
            "Epoch 22: val_loss improved from 162.75661 to 158.56253, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 192.5283 - val_loss: 158.5625\n",
            "Epoch 23/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 189.2621  \n",
            "Epoch 23: val_loss did not improve from 158.56253\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 190.5534 - val_loss: 163.6459\n",
            "Epoch 24/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 192.9402 \n",
            "Epoch 24: val_loss did not improve from 158.56253\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 193.6437 - val_loss: 160.6539\n",
            "Epoch 25/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 209.5197 \n",
            "Epoch 25: val_loss improved from 158.56253 to 154.92155, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 202.3557 - val_loss: 154.9216\n",
            "Epoch 26/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 212.4546  \n",
            "Epoch 26: val_loss improved from 154.92155 to 152.87193, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 205.9995 - val_loss: 152.8719\n",
            "Epoch 27/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 201.5050 \n",
            "Epoch 27: val_loss did not improve from 152.87193\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 194.0781 - val_loss: 156.2224\n",
            "Epoch 28/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 195.0348  \n",
            "Epoch 28: val_loss improved from 152.87193 to 150.95679, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 193.5560 - val_loss: 150.9568\n",
            "Epoch 29/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 184.4995\n",
            "Epoch 29: val_loss did not improve from 150.95679\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 184.0970 - val_loss: 154.7075\n",
            "Epoch 30/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 147.6306\n",
            "Epoch 30: val_loss did not improve from 150.95679\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 163.6711 - val_loss: 154.3930\n",
            "Epoch 31/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 162.8498 \n",
            "Epoch 31: val_loss did not improve from 150.95679\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 167.4240 - val_loss: 153.7390\n",
            "Epoch 32/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 183.8812 \n",
            "Epoch 32: val_loss improved from 150.95679 to 143.56030, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 179.7876 - val_loss: 143.5603\n",
            "Epoch 33/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 183.9183\n",
            "Epoch 33: val_loss did not improve from 143.56030\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 183.5325 - val_loss: 154.1074\n",
            "Epoch 34/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 185.1558\n",
            "Epoch 34: val_loss did not improve from 143.56030\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 185.9192 - val_loss: 149.7189\n",
            "Epoch 35/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 173.4251  \n",
            "Epoch 35: val_loss improved from 143.56030 to 141.50388, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 174.7528 - val_loss: 141.5039\n",
            "Epoch 36/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 170.7926\n",
            "Epoch 36: val_loss improved from 141.50388 to 139.30743, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 170.6646 - val_loss: 139.3074\n",
            "Epoch 37/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 208.4214\n",
            "Epoch 37: val_loss improved from 139.30743 to 139.01398, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 201.2174 - val_loss: 139.0140\n",
            "Epoch 38/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 162.9587 \n",
            "Epoch 38: val_loss did not improve from 139.01398\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 166.5335 - val_loss: 142.3808\n",
            "Epoch 39/500\n",
            "\u001b[1m24/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 149.0531 \n",
            "Epoch 39: val_loss did not improve from 139.01398\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 154.5126 - val_loss: 148.8678\n",
            "Epoch 40/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 174.5700 \n",
            "Epoch 40: val_loss did not improve from 139.01398\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 174.3441 - val_loss: 139.3044\n",
            "Epoch 41/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 154.2744\n",
            "Epoch 41: val_loss improved from 139.01398 to 135.97092, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 155.1714 - val_loss: 135.9709\n",
            "Epoch 42/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 161.1452\n",
            "Epoch 42: val_loss improved from 135.97092 to 134.58574, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 160.4781 - val_loss: 134.5857\n",
            "Epoch 43/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.2562\n",
            "Epoch 43: val_loss did not improve from 134.58574\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 143.0076 - val_loss: 136.3577\n",
            "Epoch 44/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 156.9532  \n",
            "Epoch 44: val_loss improved from 134.58574 to 133.60370, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 159.7691 - val_loss: 133.6037\n",
            "Epoch 45/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 172.2708\n",
            "Epoch 45: val_loss did not improve from 133.60370\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 169.0257 - val_loss: 133.9359\n",
            "Epoch 46/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 154.8071\n",
            "Epoch 46: val_loss did not improve from 133.60370\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 153.8198 - val_loss: 135.7173\n",
            "Epoch 47/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 160.1787\n",
            "Epoch 47: val_loss improved from 133.60370 to 130.35260, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 158.5166 - val_loss: 130.3526\n",
            "Epoch 48/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 149.8924\n",
            "Epoch 48: val_loss improved from 130.35260 to 129.95113, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 149.3798 - val_loss: 129.9511\n",
            "Epoch 49/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 169.6059\n",
            "Epoch 49: val_loss improved from 129.95113 to 127.36660, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 169.1093 - val_loss: 127.3666\n",
            "Epoch 50/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 168.0339\n",
            "Epoch 50: val_loss did not improve from 127.36660\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 166.5644 - val_loss: 133.2534\n",
            "Epoch 51/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 151.3277\n",
            "Epoch 51: val_loss did not improve from 127.36660\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 151.3913 - val_loss: 131.5395\n",
            "Epoch 52/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 134.6870\n",
            "Epoch 52: val_loss did not improve from 127.36660\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 135.6458 - val_loss: 138.6783\n",
            "Epoch 53/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 159.9464\n",
            "Epoch 53: val_loss improved from 127.36660 to 124.86911, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 159.1332 - val_loss: 124.8691\n",
            "Epoch 54/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 159.6210\n",
            "Epoch 54: val_loss did not improve from 124.86911\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 158.0845 - val_loss: 127.3956\n",
            "Epoch 55/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 166.8545 \n",
            "Epoch 55: val_loss improved from 124.86911 to 121.30103, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 158.1482 - val_loss: 121.3010\n",
            "Epoch 56/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 125.1896\n",
            "Epoch 56: val_loss improved from 121.30103 to 120.47294, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 125.5227 - val_loss: 120.4729\n",
            "Epoch 57/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 136.1915  \n",
            "Epoch 57: val_loss improved from 120.47294 to 119.76430, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 138.8283 - val_loss: 119.7643\n",
            "Epoch 58/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.7533  \n",
            "Epoch 58: val_loss improved from 119.76430 to 119.01923, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 140.4415 - val_loss: 119.0192\n",
            "Epoch 59/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 125.2252\n",
            "Epoch 59: val_loss did not improve from 119.01923\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 136.8751 - val_loss: 119.8466\n",
            "Epoch 60/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 145.6046\n",
            "Epoch 60: val_loss did not improve from 119.01923\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 145.3319 - val_loss: 122.5143\n",
            "Epoch 61/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.9237 \n",
            "Epoch 61: val_loss did not improve from 119.01923\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 141.0764 - val_loss: 119.3685\n",
            "Epoch 62/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 121.5045\n",
            "Epoch 62: val_loss did not improve from 119.01923\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 127.2002 - val_loss: 130.1153\n",
            "Epoch 63/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 143.1123 \n",
            "Epoch 63: val_loss did not improve from 119.01923\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 141.3667 - val_loss: 119.0876\n",
            "Epoch 64/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 161.4035 \n",
            "Epoch 64: val_loss improved from 119.01923 to 117.53023, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 154.6042 - val_loss: 117.5302\n",
            "Epoch 65/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 125.9814\n",
            "Epoch 65: val_loss did not improve from 117.53023\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 126.8413 - val_loss: 125.0974\n",
            "Epoch 66/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 159.4697 \n",
            "Epoch 66: val_loss did not improve from 117.53023\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 157.4931 - val_loss: 123.3129\n",
            "Epoch 67/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.0206\n",
            "Epoch 67: val_loss did not improve from 117.53023\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 138.2960 - val_loss: 118.6155\n",
            "Epoch 68/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 130.6871 \n",
            "Epoch 68: val_loss did not improve from 117.53023\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 132.7837 - val_loss: 136.7887\n",
            "Epoch 69/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 145.8981\n",
            "Epoch 69: val_loss did not improve from 117.53023\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 146.7860 - val_loss: 118.8560\n",
            "Epoch 70/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 146.6818\n",
            "Epoch 70: val_loss did not improve from 117.53023\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 146.7499 - val_loss: 120.5098\n",
            "Epoch 71/500\n",
            "\u001b[1m24/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 132.9276 \n",
            "Epoch 71: val_loss did not improve from 117.53023\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 142.8185 - val_loss: 119.9939\n",
            "Epoch 72/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 144.8459\n",
            "Epoch 72: val_loss did not improve from 117.53023\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 144.9325 - val_loss: 120.6465\n",
            "Epoch 73/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.7542\n",
            "Epoch 73: val_loss improved from 117.53023 to 116.24376, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 125.0242 - val_loss: 116.2438\n",
            "Epoch 74/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 150.4229\n",
            "Epoch 74: val_loss did not improve from 116.24376\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 150.1668 - val_loss: 116.5237\n",
            "Epoch 75/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 161.4693 \n",
            "Epoch 75: val_loss did not improve from 116.24376\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 150.2508 - val_loss: 128.1083\n",
            "Epoch 76/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 139.8611  \n",
            "Epoch 76: val_loss improved from 116.24376 to 110.73362, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 137.9062 - val_loss: 110.7336\n",
            "Epoch 77/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 125.2186\n",
            "Epoch 77: val_loss did not improve from 110.73362\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 126.4371 - val_loss: 124.0973\n",
            "Epoch 78/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 147.9510 \n",
            "Epoch 78: val_loss did not improve from 110.73362\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.0461 - val_loss: 113.1785\n",
            "Epoch 79/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 137.1192\n",
            "Epoch 79: val_loss did not improve from 110.73362\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 137.4147 - val_loss: 114.8589\n",
            "Epoch 80/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 125.5142 \n",
            "Epoch 80: val_loss did not improve from 110.73362\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 129.1364 - val_loss: 117.7887\n",
            "Epoch 81/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 138.9026\n",
            "Epoch 81: val_loss did not improve from 110.73362\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 138.4830 - val_loss: 112.3576\n",
            "Epoch 82/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 144.0042\n",
            "Epoch 82: val_loss improved from 110.73362 to 109.37783, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 141.7307 - val_loss: 109.3778\n",
            "Epoch 83/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 126.6212\n",
            "Epoch 83: val_loss did not improve from 109.37783\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 130.7334 - val_loss: 116.1486\n",
            "Epoch 84/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 147.5479  \n",
            "Epoch 84: val_loss did not improve from 109.37783\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 145.2498 - val_loss: 117.2173\n",
            "Epoch 85/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.0998\n",
            "Epoch 85: val_loss did not improve from 109.37783\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.7957 - val_loss: 121.6468\n",
            "Epoch 86/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 157.2477\n",
            "Epoch 86: val_loss did not improve from 109.37783\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 156.9622 - val_loss: 120.7948\n",
            "Epoch 87/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 133.2903 \n",
            "Epoch 87: val_loss did not improve from 109.37783\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 135.9161 - val_loss: 111.4965\n",
            "Epoch 88/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.0427\n",
            "Epoch 88: val_loss did not improve from 109.37783\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 131.3575 - val_loss: 112.8031\n",
            "Epoch 89/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 137.0190  \n",
            "Epoch 89: val_loss improved from 109.37783 to 107.89893, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 136.4693 - val_loss: 107.8989\n",
            "Epoch 90/500\n",
            "\u001b[1m19/44\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.3863\n",
            "Epoch 90: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.8768 - val_loss: 115.3306\n",
            "Epoch 91/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119.4097 \n",
            "Epoch 91: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.9291 - val_loss: 109.8415\n",
            "Epoch 92/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.8421\n",
            "Epoch 92: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 145.4024 - val_loss: 112.9972\n",
            "Epoch 93/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 147.4297\n",
            "Epoch 93: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.1789 - val_loss: 112.8585\n",
            "Epoch 94/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 150.0084\n",
            "Epoch 94: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 146.5056 - val_loss: 111.8883\n",
            "Epoch 95/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.3089\n",
            "Epoch 95: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 125.6077 - val_loss: 118.0470\n",
            "Epoch 96/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 144.0454\n",
            "Epoch 96: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 143.3065 - val_loss: 111.4173\n",
            "Epoch 97/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 153.1338\n",
            "Epoch 97: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 152.1109 - val_loss: 113.2028\n",
            "Epoch 98/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 129.4924\n",
            "Epoch 98: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 129.8662 - val_loss: 111.9462\n",
            "Epoch 99/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 139.4502\n",
            "Epoch 99: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 139.1735 - val_loss: 113.5125\n",
            "Epoch 100/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 126.5972\n",
            "Epoch 100: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 128.0246 - val_loss: 115.1241\n",
            "Epoch 101/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 138.2510\n",
            "Epoch 101: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 137.7175 - val_loss: 109.5446\n",
            "Epoch 102/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.5728\n",
            "Epoch 102: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 114.1539 - val_loss: 111.5110\n",
            "Epoch 103/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 128.2670\n",
            "Epoch 103: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 128.7139 - val_loss: 109.2664\n",
            "Epoch 104/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.6849\n",
            "Epoch 104: val_loss did not improve from 107.89893\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 126.2797 - val_loss: 113.8605\n",
            "Epoch 105/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108.2048\n",
            "Epoch 105: val_loss improved from 107.89893 to 106.31997, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.4335 - val_loss: 106.3200\n",
            "Epoch 106/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 132.1196\n",
            "Epoch 106: val_loss did not improve from 106.31997\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 131.4972 - val_loss: 108.0456\n",
            "Epoch 107/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.7606\n",
            "Epoch 107: val_loss did not improve from 106.31997\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.4032 - val_loss: 107.2970\n",
            "Epoch 108/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 136.8009\n",
            "Epoch 108: val_loss did not improve from 106.31997\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 136.7278 - val_loss: 114.7049\n",
            "Epoch 109/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.7318\n",
            "Epoch 109: val_loss did not improve from 106.31997\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 139.7282 - val_loss: 106.5202\n",
            "Epoch 110/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.2630\n",
            "Epoch 110: val_loss did not improve from 106.31997\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123.8883 - val_loss: 106.8516\n",
            "Epoch 111/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108.6960  \n",
            "Epoch 111: val_loss did not improve from 106.31997\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.6372 - val_loss: 134.4010\n",
            "Epoch 112/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 115.7925 \n",
            "Epoch 112: val_loss did not improve from 106.31997\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 117.6634 - val_loss: 119.4803\n",
            "Epoch 113/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.3463\n",
            "Epoch 113: val_loss did not improve from 106.31997\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123.7435 - val_loss: 107.7079\n",
            "Epoch 114/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 145.4944\n",
            "Epoch 114: val_loss improved from 106.31997 to 106.02290, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 142.9324 - val_loss: 106.0229\n",
            "Epoch 115/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 146.8891 \n",
            "Epoch 115: val_loss did not improve from 106.02290\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 142.8270 - val_loss: 106.1350\n",
            "Epoch 116/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.7392\n",
            "Epoch 116: val_loss did not improve from 106.02290\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.0227 - val_loss: 106.4450\n",
            "Epoch 117/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 127.1244\n",
            "Epoch 117: val_loss improved from 106.02290 to 104.02489, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 126.4854 - val_loss: 104.0249\n",
            "Epoch 118/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 135.4358 \n",
            "Epoch 118: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 133.0751 - val_loss: 106.1630\n",
            "Epoch 119/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.3950\n",
            "Epoch 119: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 121.5249 - val_loss: 106.4421\n",
            "Epoch 120/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.0755\n",
            "Epoch 120: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 130.4061 - val_loss: 105.0341\n",
            "Epoch 121/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 125.4855\n",
            "Epoch 121: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 125.1208 - val_loss: 115.9689\n",
            "Epoch 122/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 130.2421\n",
            "Epoch 122: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.3403 - val_loss: 112.9029\n",
            "Epoch 123/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 137.6662 \n",
            "Epoch 123: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 134.5113 - val_loss: 113.1144\n",
            "Epoch 124/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.7103\n",
            "Epoch 124: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.7669 - val_loss: 109.1460\n",
            "Epoch 125/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 129.6491\n",
            "Epoch 125: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 129.6153 - val_loss: 116.6142\n",
            "Epoch 126/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 130.7958 \n",
            "Epoch 126: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 132.2342 - val_loss: 108.5758\n",
            "Epoch 127/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 128.5681\n",
            "Epoch 127: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 128.7090 - val_loss: 110.3379\n",
            "Epoch 128/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 115.3180\n",
            "Epoch 128: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.0683 - val_loss: 124.0022\n",
            "Epoch 129/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119.4039 \n",
            "Epoch 129: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.3625 - val_loss: 107.8783\n",
            "Epoch 130/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 171.5233  \n",
            "Epoch 130: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 158.6772 - val_loss: 111.3134\n",
            "Epoch 131/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 131.5662\n",
            "Epoch 131: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 131.0725 - val_loss: 112.3147\n",
            "Epoch 132/500\n",
            "\u001b[1m24/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 125.9623\n",
            "Epoch 132: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 126.1569 - val_loss: 108.3256\n",
            "Epoch 133/500\n",
            "\u001b[1m24/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 123.2072 \n",
            "Epoch 133: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.6083 - val_loss: 121.0399\n",
            "Epoch 134/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 127.2030 \n",
            "Epoch 134: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 126.1163 - val_loss: 105.8421\n",
            "Epoch 135/500\n",
            "\u001b[1m27/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123.8156\n",
            "Epoch 135: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 121.2374 - val_loss: 111.0845\n",
            "Epoch 136/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.1239\n",
            "Epoch 136: val_loss did not improve from 104.02489\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 120.8082 - val_loss: 124.4922\n",
            "Epoch 137/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112.5274  \n",
            "Epoch 137: val_loss improved from 104.02489 to 103.77788, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 117.3201 - val_loss: 103.7779\n",
            "Epoch 138/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 124.8755\n",
            "Epoch 138: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.5085 - val_loss: 107.9709\n",
            "Epoch 139/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.1974\n",
            "Epoch 139: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 139.7333 - val_loss: 113.0152\n",
            "Epoch 140/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 126.1942\n",
            "Epoch 140: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 126.2579 - val_loss: 107.5908\n",
            "Epoch 141/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.6570\n",
            "Epoch 141: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 121.7512 - val_loss: 106.0099\n",
            "Epoch 142/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.1370\n",
            "Epoch 142: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 137.0515 - val_loss: 107.2795\n",
            "Epoch 143/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 133.2037\n",
            "Epoch 143: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 132.6566 - val_loss: 123.2962\n",
            "Epoch 144/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 150.2342\n",
            "Epoch 144: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 148.5348 - val_loss: 117.9739\n",
            "Epoch 145/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 125.7781\n",
            "Epoch 145: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 126.0190 - val_loss: 105.3998\n",
            "Epoch 146/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 137.9100\n",
            "Epoch 146: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 137.9405 - val_loss: 108.1446\n",
            "Epoch 147/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.0807\n",
            "Epoch 147: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 123.2566 - val_loss: 106.4454\n",
            "Epoch 148/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 127.3550\n",
            "Epoch 148: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 126.9620 - val_loss: 114.9411\n",
            "Epoch 149/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 165.1150   \n",
            "Epoch 149: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 149.7359 - val_loss: 112.2666\n",
            "Epoch 150/500\n",
            "\u001b[1m20/44\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.3422\n",
            "Epoch 150: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.0019 - val_loss: 106.6322\n",
            "Epoch 151/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 126.0623\n",
            "Epoch 151: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 128.7025 - val_loss: 105.3158\n",
            "Epoch 152/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 123.0529\n",
            "Epoch 152: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.0816 - val_loss: 110.3142\n",
            "Epoch 153/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 128.9258\n",
            "Epoch 153: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 128.8853 - val_loss: 107.5582\n",
            "Epoch 154/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.5036\n",
            "Epoch 154: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.6459 - val_loss: 105.8250\n",
            "Epoch 155/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 153.9157\n",
            "Epoch 155: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 151.6694 - val_loss: 110.3139\n",
            "Epoch 156/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123.3054\n",
            "Epoch 156: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 124.7562 - val_loss: 108.7862\n",
            "Epoch 157/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.0743\n",
            "Epoch 157: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.5413 - val_loss: 106.8547\n",
            "Epoch 158/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.9525\n",
            "Epoch 158: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 120.7070 - val_loss: 103.8490\n",
            "Epoch 159/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90.8994\n",
            "Epoch 159: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96.9108 - val_loss: 111.7630\n",
            "Epoch 160/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114.8900\n",
            "Epoch 160: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 115.7744 - val_loss: 117.4595\n",
            "Epoch 161/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 134.1943\n",
            "Epoch 161: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 131.9944 - val_loss: 107.8045\n",
            "Epoch 162/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119.4351\n",
            "Epoch 162: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.6418 - val_loss: 104.4998\n",
            "Epoch 163/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.9757\n",
            "Epoch 163: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 120.7973 - val_loss: 104.7107\n",
            "Epoch 164/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 120.6277\n",
            "Epoch 164: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 120.5593 - val_loss: 104.1421\n",
            "Epoch 165/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.6134\n",
            "Epoch 165: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 118.6940 - val_loss: 115.6753\n",
            "Epoch 166/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 123.3545\n",
            "Epoch 166: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123.9985 - val_loss: 119.5215\n",
            "Epoch 167/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 114.5193\n",
            "Epoch 167: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114.9008 - val_loss: 112.4127\n",
            "Epoch 168/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 127.7410\n",
            "Epoch 168: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 128.3592 - val_loss: 109.0586\n",
            "Epoch 169/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.2244\n",
            "Epoch 169: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.7285 - val_loss: 106.0276\n",
            "Epoch 170/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 121.2289 \n",
            "Epoch 170: val_loss did not improve from 103.77788\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 119.6109 - val_loss: 106.9797\n",
            "Epoch 171/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 114.1367\n",
            "Epoch 171: val_loss improved from 103.77788 to 103.27628, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 115.4572 - val_loss: 103.2763\n",
            "Epoch 172/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 159.0909\n",
            "Epoch 172: val_loss did not improve from 103.27628\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 152.1374 - val_loss: 104.8086\n",
            "Epoch 173/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.7538\n",
            "Epoch 173: val_loss did not improve from 103.27628\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 121.5143 - val_loss: 112.6248\n",
            "Epoch 174/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108.7188 \n",
            "Epoch 174: val_loss did not improve from 103.27628\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.5982 - val_loss: 107.4698\n",
            "Epoch 175/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114.5621\n",
            "Epoch 175: val_loss did not improve from 103.27628\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 114.7806 - val_loss: 112.5495\n",
            "Epoch 176/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 129.5830\n",
            "Epoch 176: val_loss did not improve from 103.27628\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 129.9027 - val_loss: 103.6931\n",
            "Epoch 177/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 114.4278\n",
            "Epoch 177: val_loss did not improve from 103.27628\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.1269 - val_loss: 104.6068\n",
            "Epoch 178/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 138.5141\n",
            "Epoch 178: val_loss did not improve from 103.27628\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 136.4791 - val_loss: 109.5861\n",
            "Epoch 179/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.9941\n",
            "Epoch 179: val_loss improved from 103.27628 to 102.77206, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 113.5556 - val_loss: 102.7721\n",
            "Epoch 180/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 126.9693\n",
            "Epoch 180: val_loss improved from 102.77206 to 101.13637, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 126.2051 - val_loss: 101.1364\n",
            "Epoch 181/500\n",
            "\u001b[1m24/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 129.4942  \n",
            "Epoch 181: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 127.0429 - val_loss: 102.3180\n",
            "Epoch 182/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 136.3608\n",
            "Epoch 182: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 135.9160 - val_loss: 103.3728\n",
            "Epoch 183/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.0754\n",
            "Epoch 183: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.9044 - val_loss: 107.1799\n",
            "Epoch 184/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.8860\n",
            "Epoch 184: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 114.9147 - val_loss: 108.4833\n",
            "Epoch 185/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.1392\n",
            "Epoch 185: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 137.3825 - val_loss: 114.7901\n",
            "Epoch 186/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 111.7191\n",
            "Epoch 186: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 112.5836 - val_loss: 118.6048\n",
            "Epoch 187/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 134.8135\n",
            "Epoch 187: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 134.7858 - val_loss: 108.6603\n",
            "Epoch 188/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 131.8030\n",
            "Epoch 188: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 131.7076 - val_loss: 120.6868\n",
            "Epoch 189/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 136.6846\n",
            "Epoch 189: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 135.7346 - val_loss: 102.8902\n",
            "Epoch 190/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 125.3263\n",
            "Epoch 190: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 125.4793 - val_loss: 105.6116\n",
            "Epoch 191/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 100.4782\n",
            "Epoch 191: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 104.9059 - val_loss: 111.1935\n",
            "Epoch 192/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 148.4196\n",
            "Epoch 192: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 147.0557 - val_loss: 118.3317\n",
            "Epoch 193/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 127.3284\n",
            "Epoch 193: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 126.2401 - val_loss: 115.8108\n",
            "Epoch 194/500\n",
            "\u001b[1m24/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 140.0445  \n",
            "Epoch 194: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.9170 - val_loss: 106.9364\n",
            "Epoch 195/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 118.8438 \n",
            "Epoch 195: val_loss did not improve from 101.13637\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.0041 - val_loss: 112.5075\n",
            "Epoch 196/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.0788\n",
            "Epoch 196: val_loss improved from 101.13637 to 98.97910, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 131.1440 - val_loss: 98.9791\n",
            "Epoch 197/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.0793\n",
            "Epoch 197: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 112.0610 - val_loss: 106.4934\n",
            "Epoch 198/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.4631\n",
            "Epoch 198: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.5135 - val_loss: 109.0350\n",
            "Epoch 199/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 124.3612\n",
            "Epoch 199: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.4294 - val_loss: 117.7220\n",
            "Epoch 200/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.3409\n",
            "Epoch 200: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 121.0035 - val_loss: 105.0451\n",
            "Epoch 201/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 127.0140\n",
            "Epoch 201: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 126.7566 - val_loss: 105.0808\n",
            "Epoch 202/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 133.4713\n",
            "Epoch 202: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 132.6701 - val_loss: 104.5003\n",
            "Epoch 203/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 110.2901\n",
            "Epoch 203: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 110.4938 - val_loss: 105.9211\n",
            "Epoch 204/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 118.6108\n",
            "Epoch 204: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 117.5399 - val_loss: 107.5704\n",
            "Epoch 205/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 123.3288 \n",
            "Epoch 205: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 123.9030 - val_loss: 105.0724\n",
            "Epoch 206/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.4088\n",
            "Epoch 206: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.7564 - val_loss: 101.6044\n",
            "Epoch 207/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.2032\n",
            "Epoch 207: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 120.4071 - val_loss: 101.5413\n",
            "Epoch 208/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 127.1463\n",
            "Epoch 208: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 127.1325 - val_loss: 105.9311\n",
            "Epoch 209/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.2399\n",
            "Epoch 209: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.6622 - val_loss: 98.9878\n",
            "Epoch 210/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 103.6166 \n",
            "Epoch 210: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.8648 - val_loss: 107.9010\n",
            "Epoch 211/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.6552\n",
            "Epoch 211: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.9188 - val_loss: 100.6805\n",
            "Epoch 212/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.5029\n",
            "Epoch 212: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.5693 - val_loss: 101.6627\n",
            "Epoch 213/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111.8288\n",
            "Epoch 213: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 117.4095 - val_loss: 101.8438\n",
            "Epoch 214/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 114.4516\n",
            "Epoch 214: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 115.1338 - val_loss: 100.3291\n",
            "Epoch 215/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.0226\n",
            "Epoch 215: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.4826 - val_loss: 104.6350\n",
            "Epoch 216/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 120.4044\n",
            "Epoch 216: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 117.3040 - val_loss: 107.3358\n",
            "Epoch 217/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.9843  \n",
            "Epoch 217: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.8097 - val_loss: 121.8731\n",
            "Epoch 218/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 145.6082\n",
            "Epoch 218: val_loss did not improve from 98.97910\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 145.0056 - val_loss: 112.3604\n",
            "Epoch 219/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.5084\n",
            "Epoch 219: val_loss improved from 98.97910 to 97.93937, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 121.4880 - val_loss: 97.9394\n",
            "Epoch 220/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 114.3994\n",
            "Epoch 220: val_loss did not improve from 97.93937\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114.6246 - val_loss: 106.9995\n",
            "Epoch 221/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95.2467\n",
            "Epoch 221: val_loss did not improve from 97.93937\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95.9659 - val_loss: 106.1245\n",
            "Epoch 222/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 146.4145\n",
            "Epoch 222: val_loss did not improve from 97.93937\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 142.4929 - val_loss: 101.8923\n",
            "Epoch 223/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 139.0274\n",
            "Epoch 223: val_loss did not improve from 97.93937\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 133.8415 - val_loss: 100.9987\n",
            "Epoch 224/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 118.9992\n",
            "Epoch 224: val_loss improved from 97.93937 to 96.94218, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.0579 - val_loss: 96.9422\n",
            "Epoch 225/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 137.1353\n",
            "Epoch 225: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 136.4333 - val_loss: 101.8664\n",
            "Epoch 226/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.8722\n",
            "Epoch 226: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 124.4215 - val_loss: 107.2978\n",
            "Epoch 227/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 119.0989\n",
            "Epoch 227: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.8466 - val_loss: 103.2150\n",
            "Epoch 228/500\n",
            "\u001b[1m24/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 124.6327 \n",
            "Epoch 228: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.1749 - val_loss: 102.4692\n",
            "Epoch 229/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.0175\n",
            "Epoch 229: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 118.0777 - val_loss: 107.5505\n",
            "Epoch 230/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.6688\n",
            "Epoch 230: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 123.1392 - val_loss: 98.8086\n",
            "Epoch 231/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 107.3139\n",
            "Epoch 231: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 110.5290 - val_loss: 100.7051\n",
            "Epoch 232/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 133.2724\n",
            "Epoch 232: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 131.2799 - val_loss: 104.7644\n",
            "Epoch 233/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 102.3607\n",
            "Epoch 233: val_loss did not improve from 96.94218\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 104.3152 - val_loss: 111.2311\n",
            "Epoch 234/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.5907\n",
            "Epoch 234: val_loss improved from 96.94218 to 96.35349, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 111.3393 - val_loss: 96.3535\n",
            "Epoch 235/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 113.9801\n",
            "Epoch 235: val_loss did not improve from 96.35349\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 113.9247 - val_loss: 98.4678\n",
            "Epoch 236/500\n",
            "\u001b[1m21/44\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 173.1402   \n",
            "Epoch 236: val_loss did not improve from 96.35349\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 159.2283 - val_loss: 101.2280\n",
            "Epoch 237/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 139.8939\n",
            "Epoch 237: val_loss did not improve from 96.35349\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 139.6151 - val_loss: 99.8733\n",
            "Epoch 238/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.4158\n",
            "Epoch 238: val_loss did not improve from 96.35349\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 117.5633 - val_loss: 105.6851\n",
            "Epoch 239/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 115.9803\n",
            "Epoch 239: val_loss did not improve from 96.35349\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.0788 - val_loss: 105.7043\n",
            "Epoch 240/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 126.1967 \n",
            "Epoch 240: val_loss improved from 96.35349 to 96.28719, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.8421 - val_loss: 96.2872\n",
            "Epoch 241/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.6116\n",
            "Epoch 241: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 114.3297 - val_loss: 109.1762\n",
            "Epoch 242/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.8168\n",
            "Epoch 242: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 121.2652 - val_loss: 96.6630\n",
            "Epoch 243/500\n",
            "\u001b[1m20/44\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.0651 \n",
            "Epoch 243: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.4125 - val_loss: 104.2562\n",
            "Epoch 244/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100.3846 \n",
            "Epoch 244: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.6449 - val_loss: 103.2385\n",
            "Epoch 245/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 114.7102\n",
            "Epoch 245: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.1152 - val_loss: 112.3663\n",
            "Epoch 246/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112.6719\n",
            "Epoch 246: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 113.1040 - val_loss: 101.0046\n",
            "Epoch 247/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 135.4759\n",
            "Epoch 247: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 134.6131 - val_loss: 116.4775\n",
            "Epoch 248/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.6593\n",
            "Epoch 248: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 109.4059 - val_loss: 116.4600\n",
            "Epoch 249/500\n",
            "\u001b[1m20/44\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.5622  \n",
            "Epoch 249: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.5407 - val_loss: 125.6625\n",
            "Epoch 250/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 126.7833 \n",
            "Epoch 250: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.7502 - val_loss: 99.2592\n",
            "Epoch 251/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 123.7678\n",
            "Epoch 251: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123.4029 - val_loss: 103.3825\n",
            "Epoch 252/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91.8610\n",
            "Epoch 252: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94.6694 - val_loss: 133.4571\n",
            "Epoch 253/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 106.1605\n",
            "Epoch 253: val_loss did not improve from 96.28719\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.6660 - val_loss: 102.8055\n",
            "Epoch 254/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 110.4950\n",
            "Epoch 254: val_loss improved from 96.28719 to 96.15059, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.4622 - val_loss: 96.1506\n",
            "Epoch 255/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 132.3579\n",
            "Epoch 255: val_loss did not improve from 96.15059\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 128.7534 - val_loss: 100.8913\n",
            "Epoch 256/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.3354\n",
            "Epoch 256: val_loss did not improve from 96.15059\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.1495 - val_loss: 102.1525\n",
            "Epoch 257/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.0654\n",
            "Epoch 257: val_loss improved from 96.15059 to 93.80018, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 120.9454 - val_loss: 93.8002\n",
            "Epoch 258/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108.3833 \n",
            "Epoch 258: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.2759 - val_loss: 102.0200\n",
            "Epoch 259/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102.2684\n",
            "Epoch 259: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.4091 - val_loss: 100.7292\n",
            "Epoch 260/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112.3455  \n",
            "Epoch 260: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.0889 - val_loss: 113.8999\n",
            "Epoch 261/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 110.9575  \n",
            "Epoch 261: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 110.6088 - val_loss: 102.3606\n",
            "Epoch 262/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.9536\n",
            "Epoch 262: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 102.6719 - val_loss: 95.9731\n",
            "Epoch 263/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101.3717 \n",
            "Epoch 263: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.4725 - val_loss: 98.2067\n",
            "Epoch 264/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98.9649\n",
            "Epoch 264: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99.4535 - val_loss: 95.6811\n",
            "Epoch 265/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107.9633\n",
            "Epoch 265: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.0521 - val_loss: 101.2115\n",
            "Epoch 266/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 119.8439\n",
            "Epoch 266: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.3817 - val_loss: 99.1034\n",
            "Epoch 267/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 104.2762\n",
            "Epoch 267: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.5338 - val_loss: 101.5244\n",
            "Epoch 268/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 116.5967\n",
            "Epoch 268: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 115.9890 - val_loss: 98.6293\n",
            "Epoch 269/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 118.2378\n",
            "Epoch 269: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.0451 - val_loss: 95.5441\n",
            "Epoch 270/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.2047\n",
            "Epoch 270: val_loss did not improve from 93.80018\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 106.5245 - val_loss: 95.7869\n",
            "Epoch 271/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.4172\n",
            "Epoch 271: val_loss improved from 93.80018 to 90.65001, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 107.0879 - val_loss: 90.6500\n",
            "Epoch 272/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.4123\n",
            "Epoch 272: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 104.8334 - val_loss: 154.8711\n",
            "Epoch 273/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.7884\n",
            "Epoch 273: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 119.4392 - val_loss: 112.1300\n",
            "Epoch 274/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.1446\n",
            "Epoch 274: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 107.9679 - val_loss: 91.0968\n",
            "Epoch 275/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.4629\n",
            "Epoch 275: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 116.3508 - val_loss: 97.2208\n",
            "Epoch 276/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 96.9321\n",
            "Epoch 276: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 100.0622 - val_loss: 97.0609\n",
            "Epoch 277/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.5258\n",
            "Epoch 277: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 111.5270 - val_loss: 102.3469\n",
            "Epoch 278/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.4956\n",
            "Epoch 278: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 117.9857 - val_loss: 93.5604\n",
            "Epoch 279/500\n",
            "\u001b[1m25/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119.7907   \n",
            "Epoch 279: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.9532 - val_loss: 111.0169\n",
            "Epoch 280/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 109.5286 \n",
            "Epoch 280: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.5388 - val_loss: 100.6390\n",
            "Epoch 281/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 117.4539\n",
            "Epoch 281: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.9654 - val_loss: 100.1303\n",
            "Epoch 282/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 120.6782 \n",
            "Epoch 282: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.0786 - val_loss: 95.9279\n",
            "Epoch 283/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 126.0180\n",
            "Epoch 283: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.7740 - val_loss: 103.7766\n",
            "Epoch 284/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107.0129\n",
            "Epoch 284: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.0967 - val_loss: 96.3219\n",
            "Epoch 285/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 125.9894\n",
            "Epoch 285: val_loss did not improve from 90.65001\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123.8369 - val_loss: 97.3717\n",
            "Epoch 286/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103.8392\n",
            "Epoch 286: val_loss improved from 90.65001 to 89.62542, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 104.4757 - val_loss: 89.6254\n",
            "Epoch 287/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 125.3410\n",
            "Epoch 287: val_loss improved from 89.62542 to 88.27671, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 124.7950 - val_loss: 88.2767\n",
            "Epoch 288/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.3376\n",
            "Epoch 288: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 129.3698 - val_loss: 133.7832\n",
            "Epoch 289/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 110.2657\n",
            "Epoch 289: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.2942 - val_loss: 99.7611\n",
            "Epoch 290/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.9327\n",
            "Epoch 290: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 113.3385 - val_loss: 123.3328\n",
            "Epoch 291/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111.0726\n",
            "Epoch 291: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.1706 - val_loss: 100.9416\n",
            "Epoch 292/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99.1778\n",
            "Epoch 292: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 101.4057 - val_loss: 97.1637\n",
            "Epoch 293/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 123.1058\n",
            "Epoch 293: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.6300 - val_loss: 106.0751\n",
            "Epoch 294/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 110.9414\n",
            "Epoch 294: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.5583 - val_loss: 122.4120\n",
            "Epoch 295/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 126.2802\n",
            "Epoch 295: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.4875 - val_loss: 92.7199\n",
            "Epoch 296/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.1845\n",
            "Epoch 296: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 101.3207 - val_loss: 91.1971\n",
            "Epoch 297/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.1291\n",
            "Epoch 297: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.8810 - val_loss: 99.0632\n",
            "Epoch 298/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 115.6099\n",
            "Epoch 298: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 115.3757 - val_loss: 97.2534\n",
            "Epoch 299/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.4801\n",
            "Epoch 299: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 112.3379 - val_loss: 98.8460\n",
            "Epoch 300/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.8266\n",
            "Epoch 300: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.9800 - val_loss: 92.9487\n",
            "Epoch 301/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.8528\n",
            "Epoch 301: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.9991 - val_loss: 91.4827\n",
            "Epoch 302/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.3982\n",
            "Epoch 302: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103.3960 - val_loss: 91.0541\n",
            "Epoch 303/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 121.8544\n",
            "Epoch 303: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 121.4947 - val_loss: 92.6407\n",
            "Epoch 304/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.9068\n",
            "Epoch 304: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.4834 - val_loss: 99.0735\n",
            "Epoch 305/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.5865\n",
            "Epoch 305: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.6819 - val_loss: 89.8011\n",
            "Epoch 306/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97.1696\n",
            "Epoch 306: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 98.5375 - val_loss: 96.1866\n",
            "Epoch 307/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 113.1558\n",
            "Epoch 307: val_loss did not improve from 88.27671\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.3015 - val_loss: 97.3352\n",
            "Epoch 308/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 95.1221\n",
            "Epoch 308: val_loss improved from 88.27671 to 88.25427, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 97.0749 - val_loss: 88.2543\n",
            "Epoch 309/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.7372\n",
            "Epoch 309: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.5381 - val_loss: 96.5728\n",
            "Epoch 310/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.3601\n",
            "Epoch 310: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 111.2328 - val_loss: 107.9405\n",
            "Epoch 311/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.2721\n",
            "Epoch 311: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95.2208 - val_loss: 103.1210\n",
            "Epoch 312/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105.2395 \n",
            "Epoch 312: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.2754 - val_loss: 90.9401\n",
            "Epoch 313/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.8945\n",
            "Epoch 313: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 105.9510 - val_loss: 95.0238\n",
            "Epoch 314/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94.3658\n",
            "Epoch 314: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 96.8455 - val_loss: 101.4354\n",
            "Epoch 315/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.8375\n",
            "Epoch 315: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 110.7440 - val_loss: 90.6174\n",
            "Epoch 316/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114.5824\n",
            "Epoch 316: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 114.1326 - val_loss: 95.3092\n",
            "Epoch 317/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.6221\n",
            "Epoch 317: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 109.1477 - val_loss: 92.2161\n",
            "Epoch 318/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 129.3850\n",
            "Epoch 318: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 129.5083 - val_loss: 106.1709\n",
            "Epoch 319/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.7287\n",
            "Epoch 319: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 131.8835 - val_loss: 135.2937\n",
            "Epoch 320/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 109.7063\n",
            "Epoch 320: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 112.3738 - val_loss: 98.9578\n",
            "Epoch 321/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 128.1302\n",
            "Epoch 321: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 126.0055 - val_loss: 98.6622\n",
            "Epoch 322/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.0355\n",
            "Epoch 322: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 106.4654 - val_loss: 116.0437\n",
            "Epoch 323/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 126.2469\n",
            "Epoch 323: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 125.0507 - val_loss: 104.2556\n",
            "Epoch 324/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 118.4093\n",
            "Epoch 324: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 117.7206 - val_loss: 96.2634\n",
            "Epoch 325/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 104.1097\n",
            "Epoch 325: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.9495 - val_loss: 94.6750\n",
            "Epoch 326/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.8507\n",
            "Epoch 326: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99.1112 - val_loss: 103.5910\n",
            "Epoch 327/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.6571\n",
            "Epoch 327: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103.6610 - val_loss: 179.2710\n",
            "Epoch 328/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.3594\n",
            "Epoch 328: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.4116 - val_loss: 105.6455\n",
            "Epoch 329/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 116.5943\n",
            "Epoch 329: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 114.1748 - val_loss: 100.6879\n",
            "Epoch 330/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.7409\n",
            "Epoch 330: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.7504 - val_loss: 112.9551\n",
            "Epoch 331/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101.7893\n",
            "Epoch 331: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102.1316 - val_loss: 88.3754\n",
            "Epoch 332/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87.1281\n",
            "Epoch 332: val_loss did not improve from 88.25427\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89.4823 - val_loss: 97.1375\n",
            "Epoch 333/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102.1457\n",
            "Epoch 333: val_loss improved from 88.25427 to 81.97239, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 102.0714 - val_loss: 81.9724\n",
            "Epoch 334/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94.4307    \n",
            "Epoch 334: val_loss did not improve from 81.97239\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 101.6416 - val_loss: 106.5761\n",
            "Epoch 335/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.7220\n",
            "Epoch 335: val_loss improved from 81.97239 to 77.21340, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 106.9748 - val_loss: 77.2134\n",
            "Epoch 336/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91.5363\n",
            "Epoch 336: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93.6517 - val_loss: 89.0962\n",
            "Epoch 337/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 95.0704\n",
            "Epoch 337: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97.5385 - val_loss: 114.1561\n",
            "Epoch 338/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 114.1457\n",
            "Epoch 338: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 114.5000 - val_loss: 102.1925\n",
            "Epoch 339/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 122.6570 \n",
            "Epoch 339: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.5802 - val_loss: 96.5729\n",
            "Epoch 340/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.5937\n",
            "Epoch 340: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 138.5395 - val_loss: 101.3438\n",
            "Epoch 341/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 135.3351\n",
            "Epoch 341: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 135.2227 - val_loss: 107.7675\n",
            "Epoch 342/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 128.2906\n",
            "Epoch 342: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 126.6803 - val_loss: 101.2882\n",
            "Epoch 343/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 122.6239\n",
            "Epoch 343: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.3621 - val_loss: 82.7371\n",
            "Epoch 344/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 125.1103\n",
            "Epoch 344: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 122.7568 - val_loss: 80.5281\n",
            "Epoch 345/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91.1165\n",
            "Epoch 345: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93.4644 - val_loss: 82.1842\n",
            "Epoch 346/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.5276\n",
            "Epoch 346: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.3521 - val_loss: 96.6168\n",
            "Epoch 347/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.9406\n",
            "Epoch 347: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 111.8835 - val_loss: 117.4959\n",
            "Epoch 348/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.3864\n",
            "Epoch 348: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 102.9483 - val_loss: 95.8027\n",
            "Epoch 349/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.7982\n",
            "Epoch 349: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.1482 - val_loss: 98.9758\n",
            "Epoch 350/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.3463\n",
            "Epoch 350: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.3262 - val_loss: 89.7495\n",
            "Epoch 351/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.4713\n",
            "Epoch 351: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.2505 - val_loss: 112.0941\n",
            "Epoch 352/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.3776\n",
            "Epoch 352: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114.0123 - val_loss: 106.9730\n",
            "Epoch 353/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.2656\n",
            "Epoch 353: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 103.9521 - val_loss: 103.6439\n",
            "Epoch 354/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.2674\n",
            "Epoch 354: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.9811 - val_loss: 91.2246\n",
            "Epoch 355/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96.8045\n",
            "Epoch 355: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 95.3259 - val_loss: 79.3064\n",
            "Epoch 356/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 93.7651\n",
            "Epoch 356: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 93.9225 - val_loss: 84.7905\n",
            "Epoch 357/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101.1219\n",
            "Epoch 357: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 100.9934 - val_loss: 79.1343\n",
            "Epoch 358/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 98.1780\n",
            "Epoch 358: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 100.0928 - val_loss: 97.0195\n",
            "Epoch 359/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.4519\n",
            "Epoch 359: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 123.7992 - val_loss: 106.1437\n",
            "Epoch 360/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 82.1253\n",
            "Epoch 360: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 86.1625 - val_loss: 93.1055\n",
            "Epoch 361/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 99.8941\n",
            "Epoch 361: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 99.9244 - val_loss: 94.0384\n",
            "Epoch 362/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.7579\n",
            "Epoch 362: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 110.5887 - val_loss: 97.1936\n",
            "Epoch 363/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.7649\n",
            "Epoch 363: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 118.2788 - val_loss: 92.6680\n",
            "Epoch 364/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 131.7506\n",
            "Epoch 364: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 130.8333 - val_loss: 93.0546\n",
            "Epoch 365/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.8245\n",
            "Epoch 365: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.3874 - val_loss: 97.4623\n",
            "Epoch 366/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85.2159  \n",
            "Epoch 366: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92.1934 - val_loss: 98.9626\n",
            "Epoch 367/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93.9801\n",
            "Epoch 367: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96.2485 - val_loss: 92.1000\n",
            "Epoch 368/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 98.3989\n",
            "Epoch 368: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 100.2377 - val_loss: 98.9273\n",
            "Epoch 369/500\n",
            "\u001b[1m22/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.3275\n",
            "Epoch 369: val_loss did not improve from 77.21340\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.8095 - val_loss: 113.7420\n",
            "Epoch 370/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 117.5613\n",
            "Epoch 370: val_loss improved from 77.21340 to 71.55750, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.6500 - val_loss: 71.5575\n",
            "Epoch 371/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.2531\n",
            "Epoch 371: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 100.0112 - val_loss: 87.9704\n",
            "Epoch 372/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.0299\n",
            "Epoch 372: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.1173 - val_loss: 97.2386\n",
            "Epoch 373/500\n",
            "\u001b[1m23/44\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 116.3553  \n",
            "Epoch 373: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.1120 - val_loss: 75.6085\n",
            "Epoch 374/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.8965\n",
            "Epoch 374: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.6985 - val_loss: 77.8139\n",
            "Epoch 375/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91.0419\n",
            "Epoch 375: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91.5017 - val_loss: 90.5246\n",
            "Epoch 376/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112.4418\n",
            "Epoch 376: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.6082 - val_loss: 92.2875\n",
            "Epoch 377/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105.0644\n",
            "Epoch 377: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.0820 - val_loss: 88.4984\n",
            "Epoch 378/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97.6462\n",
            "Epoch 378: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 97.7409 - val_loss: 95.6769\n",
            "Epoch 379/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.9631\n",
            "Epoch 379: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88.7271 - val_loss: 93.8316\n",
            "Epoch 380/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113.1234\n",
            "Epoch 380: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.6599 - val_loss: 86.0152\n",
            "Epoch 381/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88.2366\n",
            "Epoch 381: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 88.2997 - val_loss: 78.3819\n",
            "Epoch 382/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.9003\n",
            "Epoch 382: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.2336 - val_loss: 95.2840\n",
            "Epoch 383/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 117.2183\n",
            "Epoch 383: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.9518 - val_loss: 103.1427\n",
            "Epoch 384/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100.1354\n",
            "Epoch 384: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.3651 - val_loss: 106.4024\n",
            "Epoch 385/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 118.3517\n",
            "Epoch 385: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 117.1814 - val_loss: 114.1673\n",
            "Epoch 386/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 99.9432\n",
            "Epoch 386: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.3419 - val_loss: 97.6539\n",
            "Epoch 387/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.6412\n",
            "Epoch 387: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.8503 - val_loss: 89.1451\n",
            "Epoch 388/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.6066\n",
            "Epoch 388: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91.2871 - val_loss: 99.7214\n",
            "Epoch 389/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.6482\n",
            "Epoch 389: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.2944 - val_loss: 89.1964\n",
            "Epoch 390/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92.9796\n",
            "Epoch 390: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94.1096 - val_loss: 90.1624\n",
            "Epoch 391/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 114.0800\n",
            "Epoch 391: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 113.3816 - val_loss: 99.1394\n",
            "Epoch 392/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.9576\n",
            "Epoch 392: val_loss did not improve from 71.55750\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 123.6174 - val_loss: 71.7260\n",
            "Epoch 393/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90.3060\n",
            "Epoch 393: val_loss improved from 71.55750 to 70.34013, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 90.7326 - val_loss: 70.3401\n",
            "Epoch 394/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82.5517\n",
            "Epoch 394: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.3266 - val_loss: 76.9361\n",
            "Epoch 395/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 119.2786\n",
            "Epoch 395: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 117.8449 - val_loss: 106.3067\n",
            "Epoch 396/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98.1195\n",
            "Epoch 396: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 98.9499 - val_loss: 88.6828\n",
            "Epoch 397/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.0243\n",
            "Epoch 397: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 99.4757 - val_loss: 90.4903\n",
            "Epoch 398/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 103.2241\n",
            "Epoch 398: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 103.3254 - val_loss: 109.6752\n",
            "Epoch 399/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.2460\n",
            "Epoch 399: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 121.4905 - val_loss: 89.3333\n",
            "Epoch 400/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.8796\n",
            "Epoch 400: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 96.0201 - val_loss: 93.6220\n",
            "Epoch 401/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 94.8152\n",
            "Epoch 401: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 96.5467 - val_loss: 80.9688\n",
            "Epoch 402/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97.3508\n",
            "Epoch 402: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 97.2340 - val_loss: 79.9601\n",
            "Epoch 403/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 81.4763\n",
            "Epoch 403: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 84.2153 - val_loss: 79.2712\n",
            "Epoch 404/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93.0824\n",
            "Epoch 404: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 93.2279 - val_loss: 85.8483\n",
            "Epoch 405/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 99.9161\n",
            "Epoch 405: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.9419 - val_loss: 111.5302\n",
            "Epoch 406/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90.7894\n",
            "Epoch 406: val_loss did not improve from 70.34013\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92.0724 - val_loss: 90.9959\n",
            "Epoch 407/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.4315\n",
            "Epoch 407: val_loss improved from 70.34013 to 67.01360, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 112.5374 - val_loss: 67.0136\n",
            "Epoch 408/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.8167\n",
            "Epoch 408: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 98.0521 - val_loss: 88.9242\n",
            "Epoch 409/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.6450\n",
            "Epoch 409: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103.5810 - val_loss: 88.9004\n",
            "Epoch 410/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.4596\n",
            "Epoch 410: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 116.4532 - val_loss: 96.0225\n",
            "Epoch 411/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92.9112\n",
            "Epoch 411: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94.1460 - val_loss: 89.7661\n",
            "Epoch 412/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.7864\n",
            "Epoch 412: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103.6310 - val_loss: 85.3078\n",
            "Epoch 413/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 106.5247\n",
            "Epoch 413: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102.5980 - val_loss: 79.5025\n",
            "Epoch 414/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95.9138\n",
            "Epoch 414: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 97.6594 - val_loss: 121.9109\n",
            "Epoch 415/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.3474\n",
            "Epoch 415: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.7487 - val_loss: 90.3856\n",
            "Epoch 416/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101.7042\n",
            "Epoch 416: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101.8200 - val_loss: 124.0783\n",
            "Epoch 417/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 99.1238\n",
            "Epoch 417: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99.8024 - val_loss: 93.3719\n",
            "Epoch 418/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 110.9094\n",
            "Epoch 418: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110.6287 - val_loss: 107.1980\n",
            "Epoch 419/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 99.3558\n",
            "Epoch 419: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99.5685 - val_loss: 90.6435\n",
            "Epoch 420/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96.1023\n",
            "Epoch 420: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96.2584 - val_loss: 74.0084\n",
            "Epoch 421/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.3599\n",
            "Epoch 421: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 104.5571 - val_loss: 78.3221\n",
            "Epoch 422/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.8960\n",
            "Epoch 422: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95.1782 - val_loss: 72.9419\n",
            "Epoch 423/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.8524\n",
            "Epoch 423: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90.6805 - val_loss: 73.9960\n",
            "Epoch 424/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 104.5042\n",
            "Epoch 424: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 104.8509 - val_loss: 94.0631\n",
            "Epoch 425/500\n",
            "\u001b[1m32/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90.0715\n",
            "Epoch 425: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 95.5732 - val_loss: 99.5392\n",
            "Epoch 426/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 117.4887\n",
            "Epoch 426: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 117.7257 - val_loss: 108.6098\n",
            "Epoch 427/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.3383\n",
            "Epoch 427: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.1499 - val_loss: 97.2392\n",
            "Epoch 428/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.2983\n",
            "Epoch 428: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.0204 - val_loss: 106.8799\n",
            "Epoch 429/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 111.9544\n",
            "Epoch 429: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.5086 - val_loss: 100.7988\n",
            "Epoch 430/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 106.0438\n",
            "Epoch 430: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 109.2929 - val_loss: 92.6477\n",
            "Epoch 431/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 125.5663\n",
            "Epoch 431: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123.2876 - val_loss: 97.4980\n",
            "Epoch 432/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.2671\n",
            "Epoch 432: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101.8144 - val_loss: 108.9711\n",
            "Epoch 433/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 114.9026\n",
            "Epoch 433: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 114.4950 - val_loss: 94.9727\n",
            "Epoch 434/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 103.8210\n",
            "Epoch 434: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102.7157 - val_loss: 105.4285\n",
            "Epoch 435/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101.6320\n",
            "Epoch 435: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101.8600 - val_loss: 81.8629\n",
            "Epoch 436/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 107.1352\n",
            "Epoch 436: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 105.9398 - val_loss: 78.3603\n",
            "Epoch 437/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 93.7956\n",
            "Epoch 437: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 97.0985 - val_loss: 79.2136\n",
            "Epoch 438/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 105.2271\n",
            "Epoch 438: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 104.2093 - val_loss: 81.0980\n",
            "Epoch 439/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100.6573\n",
            "Epoch 439: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 101.4615 - val_loss: 90.7174\n",
            "Epoch 440/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 110.7411\n",
            "Epoch 440: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 110.6947 - val_loss: 101.0049\n",
            "Epoch 441/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 91.9224\n",
            "Epoch 441: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 96.3738 - val_loss: 94.2748\n",
            "Epoch 442/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 133.7082\n",
            "Epoch 442: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 131.9426 - val_loss: 96.6686\n",
            "Epoch 443/500\n",
            "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 109.3282\n",
            "Epoch 443: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 111.8755 - val_loss: 113.4788\n",
            "Epoch 444/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118.4281\n",
            "Epoch 444: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 117.0559 - val_loss: 89.9750\n",
            "Epoch 445/500\n",
            "\u001b[1m31/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.4761\n",
            "Epoch 445: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 104.7798 - val_loss: 91.0416\n",
            "Epoch 446/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 107.7689\n",
            "Epoch 446: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 107.6631 - val_loss: 97.5084\n",
            "Epoch 447/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.3995\n",
            "Epoch 447: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.5492 - val_loss: 80.8824\n",
            "Epoch 448/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113.7803\n",
            "Epoch 448: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.8969 - val_loss: 84.1094\n",
            "Epoch 449/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.8458\n",
            "Epoch 449: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 109.6500 - val_loss: 87.0533\n",
            "Epoch 450/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101.0509\n",
            "Epoch 450: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101.5691 - val_loss: 90.9706\n",
            "Epoch 451/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 123.7216\n",
            "Epoch 451: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122.6154 - val_loss: 100.0741\n",
            "Epoch 452/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.5339\n",
            "Epoch 452: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109.0459 - val_loss: 94.6275\n",
            "Epoch 453/500\n",
            "\u001b[1m26/44\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85.0135\n",
            "Epoch 453: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 91.5191 - val_loss: 95.8185\n",
            "Epoch 454/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.1113\n",
            "Epoch 454: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114.6925 - val_loss: 91.1232\n",
            "Epoch 455/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.3698\n",
            "Epoch 455: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.2683 - val_loss: 87.5355\n",
            "Epoch 456/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 111.1708\n",
            "Epoch 456: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 110.9905 - val_loss: 92.1205\n",
            "Epoch 457/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.5435\n",
            "Epoch 457: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103.9063 - val_loss: 78.8812\n",
            "Epoch 458/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.3205\n",
            "Epoch 458: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112.4873 - val_loss: 97.8640\n",
            "Epoch 459/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.3280\n",
            "Epoch 459: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 99.6155 - val_loss: 86.3608\n",
            "Epoch 460/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.2655\n",
            "Epoch 460: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86.0957 - val_loss: 77.9897\n",
            "Epoch 461/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.2528\n",
            "Epoch 461: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101.3776 - val_loss: 77.0770\n",
            "Epoch 462/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.9609\n",
            "Epoch 462: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86.6208 - val_loss: 90.5384\n",
            "Epoch 463/500\n",
            "\u001b[1m30/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 98.0560\n",
            "Epoch 463: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 101.5437 - val_loss: 103.5615\n",
            "Epoch 464/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93.5742\n",
            "Epoch 464: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94.0836 - val_loss: 96.4898\n",
            "Epoch 465/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.8944\n",
            "Epoch 465: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103.0705 - val_loss: 74.1367\n",
            "Epoch 466/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.5884\n",
            "Epoch 466: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84.7180 - val_loss: 91.9881\n",
            "Epoch 467/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98.0286\n",
            "Epoch 467: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 98.1417 - val_loss: 82.4427\n",
            "Epoch 468/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.3049\n",
            "Epoch 468: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.5374 - val_loss: 72.7556\n",
            "Epoch 469/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.8338\n",
            "Epoch 469: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 92.2286 - val_loss: 74.3635\n",
            "Epoch 470/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.1779\n",
            "Epoch 470: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 78.4165 - val_loss: 84.6854\n",
            "Epoch 471/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.3320\n",
            "Epoch 471: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88.3525 - val_loss: 76.0016\n",
            "Epoch 472/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93.6057\n",
            "Epoch 472: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94.3849 - val_loss: 97.5673\n",
            "Epoch 473/500\n",
            "\u001b[1m28/44\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.3795\n",
            "Epoch 473: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87.5644 - val_loss: 81.2724\n",
            "Epoch 474/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98.2884\n",
            "Epoch 474: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 98.7348 - val_loss: 74.2371\n",
            "Epoch 475/500\n",
            "\u001b[1m29/44\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.1967\n",
            "Epoch 475: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85.0664 - val_loss: 71.1666\n",
            "Epoch 476/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99.2964\n",
            "Epoch 476: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 99.6829 - val_loss: 103.0218\n",
            "Epoch 477/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108.9090\n",
            "Epoch 477: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.8338 - val_loss: 91.7500\n",
            "Epoch 478/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.2186\n",
            "Epoch 478: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.8898 - val_loss: 88.2171\n",
            "Epoch 479/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.6029\n",
            "Epoch 479: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 106.8392 - val_loss: 89.0963\n",
            "Epoch 480/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 123.0269\n",
            "Epoch 480: val_loss did not improve from 67.01360\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.9083 - val_loss: 95.1991\n",
            "Epoch 481/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101.4904\n",
            "Epoch 481: val_loss improved from 67.01360 to 61.60934, saving model to NN_model_deeper.weights.keras\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 101.1190 - val_loss: 61.6093\n",
            "Epoch 482/500\n",
            "\u001b[1m39/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.4524\n",
            "Epoch 482: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 106.9959 - val_loss: 72.4554\n",
            "Epoch 483/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 77.9429\n",
            "Epoch 483: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 78.9604 - val_loss: 69.3856\n",
            "Epoch 484/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 98.3526\n",
            "Epoch 484: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 99.4305 - val_loss: 111.4769\n",
            "Epoch 485/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104.0838\n",
            "Epoch 485: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 104.2301 - val_loss: 117.9713\n",
            "Epoch 486/500\n",
            "\u001b[1m36/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 115.1793\n",
            "Epoch 486: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 113.7122 - val_loss: 93.6692\n",
            "Epoch 487/500\n",
            "\u001b[1m37/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99.6470\n",
            "Epoch 487: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 100.6217 - val_loss: 106.2858\n",
            "Epoch 488/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.9913\n",
            "Epoch 488: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 108.8492 - val_loss: 99.9799\n",
            "Epoch 489/500\n",
            "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95.8744\n",
            "Epoch 489: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 96.4127 - val_loss: 105.9547\n",
            "Epoch 490/500\n",
            "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95.6851\n",
            "Epoch 490: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95.5396 - val_loss: 77.8159\n",
            "Epoch 491/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88.6393\n",
            "Epoch 491: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88.6045 - val_loss: 81.5298\n",
            "Epoch 492/500\n",
            "\u001b[1m35/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104.6507\n",
            "Epoch 492: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 102.0272 - val_loss: 74.8867\n",
            "Epoch 493/500\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92.8877\n",
            "Epoch 493: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94.1474 - val_loss: 78.3826\n",
            "Epoch 494/500\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105.5212\n",
            "Epoch 494: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.2803 - val_loss: 70.5438\n",
            "Epoch 495/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.9732\n",
            "Epoch 495: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95.4465 - val_loss: 64.1038\n",
            "Epoch 496/500\n",
            "\u001b[1m34/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.1395\n",
            "Epoch 496: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107.0790 - val_loss: 97.9530\n",
            "Epoch 497/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.5944\n",
            "Epoch 497: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114.7974 - val_loss: 68.9549\n",
            "Epoch 498/500\n",
            "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86.8338\n",
            "Epoch 498: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87.5073 - val_loss: 93.8086\n",
            "Epoch 499/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.4622\n",
            "Epoch 499: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 105.2713 - val_loss: 69.0996\n",
            "Epoch 500/500\n",
            "\u001b[1m40/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.8393\n",
            "Epoch 500: val_loss did not improve from 61.60934\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84.8496 - val_loss: 115.3967\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "Mean Squared Error: 136.9563420715605\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "Room       | Current Light Level | Real Light Level | Predicted Light Level\n",
            "living_room | 34.00              | 46.00            | 40.97\n",
            "balcony    | 32.00              | 80.00            | 79.77\n",
            "bedroom    | 6.00              | 7.00            | 6.77\n",
            "bedroom    | 6.00              | 8.00            | 9.05\n",
            "living_room | 50.00              | 42.00            | 18.17\n",
            "living_room | 9.00              | 9.00            | 7.72\n",
            "balcony    | 7.00              | 7.00            | 9.66\n",
            "living_room | 36.00              | 51.00            | 37.34\n",
            "living_room | 42.00              | 43.00            | 40.22\n",
            "bedroom    | 66.00              | 65.00            | 63.76\n",
            "living_room | 7.00              | 6.00            | 7.00\n",
            "bedroom    | 7.00              | 6.00            | 8.89\n",
            "balcony    | 94.00              | 75.00            | 77.37\n",
            "living_room | 43.00              | 23.00            | 38.04\n",
            "living_room | 7.00              | 9.00            | 7.00\n",
            "balcony    | 7.00              | 7.00            | 6.82\n",
            "living_room | 45.00              | 38.00            | 40.21\n",
            "living_room | 45.00              | 39.00            | 40.21\n",
            "living_room | 9.00              | 6.00            | 5.79\n",
            "bedroom    | 7.00              | 8.00            | 6.88\n",
            "living_room | 37.00              | 35.00            | 37.34\n",
            "balcony    | 82.00              | 6.00            | 12.77\n",
            "balcony    | 78.00              | 79.00            | 26.06\n",
            "bedroom    | 7.00              | 18.00            | 17.82\n",
            "living_room | 6.00              | 9.00            | 7.16\n",
            "balcony    | 36.00              | 95.00            | 80.19\n",
            "bedroom    | 7.00              | 7.00            | 6.79\n",
            "bedroom    | 60.00              | 49.00            | 59.98\n",
            "balcony    | 7.00              | 7.00            | 6.63\n",
            "living_room | 4.00              | 4.00            | 6.74\n",
            "living_room | 12.00              | 39.00            | 34.25\n",
            "living_room | 7.00              | 8.00            | 6.47\n",
            "balcony    | 85.00              | 78.00            | 75.00\n",
            "living_room | 9.00              | 7.00            | 7.72\n",
            "balcony    | 8.00              | 6.00            | 7.76\n",
            "bedroom    | 10.00              | 7.00            | 5.75\n",
            "living_room | 38.00              | 43.00            | 38.78\n",
            "bedroom    | 9.00              | 5.00            | 6.43\n",
            "living_room | 36.00              | 34.00            | 38.06\n",
            "living_room | 10.00              | 8.00            | 6.74\n",
            "balcony    | 25.00              | 77.00            | 77.01\n",
            "bedroom    | 61.00              | 61.00            | 59.91\n",
            "balcony    | 9.00              | 7.00            | 7.51\n",
            "balcony    | 6.00              | 7.00            | 6.78\n",
            "living_room | 7.00              | 5.00            | 9.73\n",
            "living_room | 42.00              | 5.00            | 14.09\n",
            "bedroom    | 7.00              | 8.00            | 6.88\n",
            "bedroom    | 58.00              | 65.00            | 24.54\n",
            "bedroom    | 9.00              | 9.00            | 5.87\n",
            "bedroom    | 58.00              | 53.00            | 60.44\n",
            "bedroom    | 3.00              | 7.00            | 6.86\n",
            "bedroom    | 50.00              | 51.00            | 60.94\n",
            "balcony    | 84.00              | 93.00            | 78.13\n",
            "balcony    | 8.00              | 36.00            | 18.04\n",
            "living_room | 6.00              | 4.00            | 6.85\n",
            "balcony    | 9.00              | 6.00            | 6.81\n",
            "bedroom    | 9.00              | 5.00            | 6.62\n",
            "living_room | 8.00              | 7.00            | 6.74\n",
            "bedroom    | 55.00              | 69.00            | 60.25\n",
            "living_room | 38.00              | 35.00            | 31.53\n",
            "living_room | 34.00              | 42.00            | 32.84\n",
            "bedroom    | 60.00              | 71.00            | 60.31\n",
            "balcony    | 77.00              | 8.00            | 12.44\n",
            "bedroom    | 56.00              | 51.00            | 38.76\n",
            "bedroom    | 51.00              | 64.00            | 60.58\n",
            "living_room | 8.00              | 7.00            | 6.52\n",
            "bedroom    | 9.00              | 7.00            | 8.14\n",
            "bedroom    | 58.00              | 63.00            | 60.44\n",
            "living_room | 10.00              | 8.00            | 6.64\n",
            "living_room | 45.00              | 7.00            | 12.69\n",
            "living_room | 7.00              | 9.00            | 7.47\n",
            "living_room | 5.00              | 3.00            | 6.58\n",
            "living_room | 9.00              | 7.00            | 6.37\n",
            "balcony    | 5.00              | 26.00            | 25.57\n",
            "living_room | 9.00              | 10.00            | 5.79\n",
            "balcony    | 7.00              | 32.00            | 17.79\n",
            "balcony    | 80.00              | 80.00            | 27.99\n",
            "balcony    | 5.00              | 10.00            | 6.92\n",
            "living_room | 8.00              | 5.00            | 6.74\n",
            "living_room | 46.00              | 45.00            | 41.48\n",
            "balcony    | 6.00              | 6.00            | 7.49\n",
            "living_room | 4.00              | 7.00            | 7.57\n",
            "balcony    | 9.00              | 10.00            | 6.60\n",
            "bedroom    | 7.00              | 10.00            | 7.07\n",
            "living_room | 6.00              | 9.00            | 8.02\n",
            "bedroom    | 52.00              | 59.00            | 36.10\n",
            "balcony    | 36.00              | 90.00            | 80.19\n",
            "bedroom    | 65.00              | 5.00            | 13.07\n",
            "living_room | 48.00              | 20.00            | 26.90\n",
            "balcony    | 89.00              | 79.00            | 44.34\n",
            "living_room | 4.00              | 5.00            | 7.06\n",
            "bedroom    | 64.00              | 59.00            | 57.98\n",
            "bedroom    | 54.00              | 5.00            | 17.04\n",
            "balcony    | 75.00              | 87.00            | 24.66\n",
            "balcony    | 75.00              | 86.00            | 79.17\n",
            "balcony    | 7.00              | 9.00            | 7.26\n",
            "balcony    | 5.00              | 6.00            | 7.03\n",
            "living_room | 6.00              | 4.00            | 6.85\n",
            "living_room | 34.00              | 30.00            | 38.07\n",
            "balcony    | 9.00              | 9.00            | 6.28\n",
            "living_room | 5.00              | 5.00            | 11.91\n",
            "living_room | 7.00              | 10.00            | 7.27\n",
            "living_room | 10.00              | 9.00            | 11.58\n",
            "balcony    | 6.00              | 7.00            | 6.55\n",
            "living_room | 42.00              | 49.00            | 29.69\n",
            "living_room | 42.00              | 49.00            | 40.94\n",
            "bedroom    | 66.00              | 59.00            | 58.92\n",
            "living_room | 41.00              | 20.00            | 30.15\n",
            "bedroom    | 10.00              | 9.00            | 6.54\n",
            "balcony    | 9.00              | 8.00            | 6.39\n",
            "living_room | 32.00              | 42.00            | 41.70\n",
            "balcony    | 90.00              | 89.00            | 66.32\n",
            "bedroom    | 5.00              | 7.00            | 6.66\n",
            "balcony    | 8.00              | 6.00            | 6.55\n",
            "balcony    | 6.00              | 4.00            | 7.08\n",
            "living_room | 8.00              | 8.00            | 6.42\n",
            "balcony    | 8.00              | 7.00            | 6.55\n",
            "balcony    | 8.00              | 6.00            | 6.59\n",
            "bedroom    | 8.00              | 7.00            | 5.62\n",
            "balcony    | 7.00              | 4.00            | 6.63\n",
            "bedroom    | 60.00              | 78.00            | 60.31\n",
            "bedroom    | 4.00              | 22.00            | 16.28\n",
            "bedroom    | 8.00              | 7.00            | 6.90\n",
            "bedroom    | 5.00              | 8.00            | 9.43\n",
            "balcony    | 6.00              | 33.00            | 26.35\n",
            "balcony    | 6.00              | 6.00            | 6.55\n",
            "living_room | 50.00              | 38.00            | 40.19\n",
            "balcony    | 72.00              | 6.00            | 13.29\n",
            "living_room | 7.00              | 7.00            | 6.68\n",
            "bedroom    | 61.00              | 65.00            | 59.59\n",
            "bedroom    | 6.00              | 8.00            | 6.78\n",
            "bedroom    | 4.00              | 6.00            | 7.24\n",
            "living_room | 9.00              | 7.00            | 7.46\n",
            "living_room | 7.00              | 55.00            | 29.48\n",
            "bedroom    | 4.00              | 8.00            | 6.76\n",
            "balcony    | 7.00              | 7.00            | 8.00\n",
            "living_room | 30.00              | 32.00            | 38.08\n",
            "balcony    | 7.00              | 6.00            | 6.92\n",
            "bedroom    | 7.00              | 5.00            | 6.79\n",
            "living_room | 32.00              | 43.00            | 38.80\n",
            "living_room | 8.00              | 5.00            | 6.52\n",
            "living_room | 8.00              | 5.00            | 7.45\n",
            "balcony    | 77.00              | 75.00            | 73.83\n",
            "living_room | 7.00              | 6.00            | 6.68\n",
            "balcony    | 82.00              | 79.00            | 78.28\n",
            "living_room | 7.00              | 6.00            | 6.90\n",
            "balcony    | 4.00              | 7.00            | 6.77\n",
            "bedroom    | 8.00              | 5.00            | 6.90\n",
            "living_room | 43.00              | 33.00            | 40.21\n",
            "bedroom    | 9.00              | 6.00            | 11.64\n",
            "living_room | 41.00              | 37.00            | 40.22\n",
            "balcony    | 6.00              | 32.00            | 26.35\n",
            "balcony    | 4.00              | 8.00            | 8.18\n",
            "balcony    | 69.00              | 81.00            | 79.27\n",
            "balcony    | 8.00              | 21.00            | 28.82\n",
            "balcony    | 79.00              | 77.00            | 26.76\n",
            "bedroom    | 62.00              | 53.00            | 59.52\n",
            "living_room | 34.00              | 42.00            | 38.07\n",
            "bedroom    | 6.00              | 7.00            | 7.06\n",
            "balcony    | 7.00              | 6.00            | 8.00\n",
            "balcony    | 8.00              | 6.00            | 6.44\n",
            "balcony    | 4.00              | 6.00            | 7.08\n",
            "balcony    | 71.00              | 78.00            | 24.40\n",
            "bedroom    | 9.00              | 6.00            | 5.44\n",
            "living_room | 51.00              | 45.00            | 40.83\n",
            "bedroom    | 7.00              | 9.00            | 6.88\n",
            "living_room | 12.00              | 53.00            | 38.03\n",
            "balcony    | 5.00              | 5.00            | 6.41\n",
            "bedroom    | 7.00              | 8.00            | 6.50\n",
            "balcony    | 9.00              | 9.00            | 6.71\n",
            "balcony    | 86.00              | 7.00            | 14.28\n",
            "balcony    | 75.00              | 6.00            | 12.31\n",
            "balcony    | 5.00              | 7.00            | 7.03\n",
            "balcony    | 7.00              | 5.00            | 7.26\n",
            "balcony    | 76.00              | 67.00            | 79.10\n",
            "bedroom    | 54.00              | 54.00            | 60.12\n",
            "balcony    | 86.00              | 78.00            | 38.83\n",
            "living_room | 7.00              | 7.00            | 7.47\n",
            "living_room | 36.00              | 38.00            | 40.96\n",
            "balcony    | 89.00              | 69.00            | 74.08\n",
            "balcony    | 5.00              | 7.00            | 7.02\n",
            "bedroom    | 61.00              | 63.00            | 59.59\n",
            "balcony    | 5.00              | 9.00            | 6.41\n",
            "balcony    | 9.00              | 3.00            | 18.73\n",
            "living_room | 8.00              | 32.00            | 19.61\n",
            "balcony    | 84.00              | 8.00            | 13.07\n",
            "bedroom    | 8.00              | 7.00            | 8.58\n",
            "living_room | 7.00              | 8.00            | 7.29\n",
            "balcony    | 5.00              | 7.00            | 7.03\n",
            "living_room | 7.00              | 8.00            | 6.58\n",
            "living_room | 41.00              | 38.00            | 38.05\n",
            "bedroom    | 68.00              | 67.00            | 56.73\n",
            "bedroom    | 58.00              | 56.00            | 59.79\n",
            "balcony    | 71.00              | 78.00            | 79.24\n",
            "living_room | 45.00              | 51.00            | 41.51\n",
            "balcony    | 25.00              | 73.00            | 77.01\n",
            "living_room | 6.00              | 5.00            | 6.85\n",
            "balcony    | 7.00              | 8.00            | 6.92\n",
            "bedroom    | 6.00              | 6.00            | 6.58\n",
            "balcony    | 7.00              | 6.00            | 7.26\n",
            "balcony    | 10.00              | 5.00            | 6.20\n",
            "bedroom    | 60.00              | 7.00            | 14.40\n",
            "balcony    | 78.00              | 94.00            | 51.19\n",
            "living_room | 8.00              | 6.00            | 19.61\n",
            "living_room | 7.00              | 5.00            | 6.57\n",
            "bedroom    | 5.00              | 7.00            | 7.40\n",
            "bedroom    | 5.00              | 10.00            | 9.22\n",
            "bedroom    | 10.00              | 9.00            | 5.75\n",
            "living_room | 41.00              | 9.00            | 14.55\n",
            "living_room | 6.00              | 5.00            | 7.24\n",
            "balcony    | 6.00              | 4.00            | 6.78\n",
            "living_room | 44.00              | 25.00            | 28.76\n",
            "balcony    | 6.00              | 5.00            | 6.98\n",
            "balcony    | 3.00              | 5.00            | 7.09\n",
            "bedroom    | 8.00              | 8.00            | 6.99\n",
            "living_room | 47.00              | 39.00            | 19.56\n",
            "living_room | 9.00              | 7.00            | 7.72\n",
            "balcony    | 85.00              | 78.00            | 78.41\n",
            "living_room | 6.00              | 8.00            | 6.85\n",
            "bedroom    | 60.00              | 60.00            | 59.68\n",
            "balcony    | 4.00              | 6.00            | 7.19\n",
            "living_room | 49.00              | 46.00            | 40.19\n",
            "balcony    | 5.00              | 4.00            | 7.03\n",
            "living_room | 35.00              | 57.00            | 32.52\n",
            "balcony    | 9.00              | 7.00            | 6.92\n",
            "bedroom    | 9.00              | 7.00            | 8.14\n",
            "bedroom    | 25.00              | 67.00            | 61.70\n",
            "bedroom    | 74.00              | 61.00            | 62.65\n",
            "balcony    | 87.00              | 89.00            | 77.90\n",
            "balcony    | 7.00              | 9.00            | 6.63\n",
            "balcony    | 5.00              | 33.00            | 17.33\n",
            "balcony    | 70.00              | 67.00            | 79.43\n",
            "balcony    | 6.00              | 7.00            | 7.49\n",
            "balcony    | 77.00              | 81.00            | 78.78\n",
            "balcony    | 8.00              | 7.00            | 6.65\n",
            "balcony    | 78.00              | 86.00            | 79.07\n",
            "living_room | 31.00              | 38.00            | 40.98\n",
            "bedroom    | 63.00              | 54.00            | 59.24\n",
            "balcony    | 70.00              | 74.00            | 38.12\n",
            "bedroom    | 20.00              | 71.00            | 61.37\n",
            "bedroom    | 61.00              | 64.00            | 25.47\n",
            "bedroom    | 46.00              | 71.00            | 60.86\n",
            "living_room | 8.00              | 38.00            | 32.89\n",
            "bedroom    | 9.00              | 7.00            | 5.87\n",
            "bedroom    | 53.00              | 56.00            | 61.04\n",
            "living_room | 35.00              | 36.00            | 41.69\n",
            "balcony    | 75.00              | 92.00            | 78.93\n",
            "living_room | 7.00              | 5.00            | 7.26\n",
            "bedroom    | 62.00              | 62.00            | 57.84\n",
            "balcony    | 96.00              | 74.00            | 77.58\n",
            "bedroom    | 7.00              | 7.00            | 6.98\n",
            "balcony    | 86.00              | 84.00            | 78.46\n",
            "bedroom    | 7.00              | 5.00            | 7.10\n",
            "balcony    | 75.00              | 80.00            | 72.43\n",
            "bedroom    | 6.00              | 9.00            | 6.77\n",
            "balcony    | 6.00              | 8.00            | 6.98\n",
            "living_room | 9.00              | 7.00            | 6.79\n",
            "bedroom    | 7.00              | 8.00            | 6.50\n",
            "living_room | 37.00              | 38.00            | 38.06\n",
            "living_room | 38.00              | 41.00            | 37.33\n",
            "balcony    | 84.00              | 8.00            | 13.07\n",
            "living_room | 8.00              | 9.00            | 6.84\n",
            "bedroom    | 7.00              | 9.00            | 6.98\n",
            "bedroom    | 62.00              | 61.00            | 42.98\n",
            "balcony    | 62.00              | 94.00            | 36.16\n",
            "balcony    | 10.00              | 6.00            | 6.65\n",
            "balcony    | 65.00              | 72.00            | 79.57\n",
            "living_room | 9.00              | 10.00            | 6.68\n",
            "balcony    | 79.00              | 9.00            | 12.57\n",
            "bedroom    | 3.00              | 9.00            | 8.84\n",
            "bedroom    | 49.00              | 63.00            | 60.83\n",
            "balcony    | 78.00              | 87.00            | 78.94\n",
            "balcony    | 73.00              | 74.00            | 79.32\n",
            "balcony    | 5.00              | 5.00            | 6.71\n",
            "living_room | 20.00              | 54.00            | 36.12\n",
            "living_room | 4.00              | 4.00            | 7.17\n",
            "living_room | 51.00              | 40.00            | 17.71\n",
            "living_room | 7.00              | 10.00            | 16.45\n",
            "bedroom    | 5.00              | 9.00            | 6.95\n",
            "living_room | 42.00              | 44.00            | 39.49\n",
            "bedroom    | 8.00              | 7.00            | 6.80\n",
            "living_room | 48.00              | 35.00            | 26.90\n",
            "living_room | 28.00              | 33.00            | 35.86\n",
            "bedroom    | 7.00              | 7.00            | 6.59\n",
            "bedroom    | 8.00              | 7.00            | 6.63\n",
            "balcony    | 8.00              | 9.00            | 6.87\n",
            "bedroom    | 65.00              | 58.00            | 57.77\n",
            "bedroom    | 59.00              | 60.00            | 60.37\n",
            "living_room | 10.00              | 6.00            | 5.76\n",
            "bedroom    | 7.00              | 5.00            | 7.10\n",
            "bedroom    | 8.00              | 6.00            | 11.65\n",
            "balcony    | 69.00              | 87.00            | 79.15\n",
            "balcony    | 72.00              | 82.00            | 24.47\n",
            "bedroom    | 69.00              | 52.00            | 59.38\n",
            "balcony    | 8.00              | 10.00            | 7.76\n",
            "living_room | 35.00              | 37.00            | 25.14\n",
            "balcony    | 7.00              | 6.00            | 6.33\n",
            "living_room | 9.00              | 6.00            | 5.79\n",
            "living_room | 38.00              | 7.00            | 15.94\n",
            "bedroom    | 10.00              | 7.00            | 6.35\n",
            "bedroom    | 6.00              | 5.00            | 8.77\n",
            "balcony    | 7.00              | 9.00            | 6.33\n",
            "living_room | 6.00              | 9.00            | 7.47\n",
            "balcony    | 7.00              | 7.00            | 6.44\n",
            "balcony    | 4.00              | 6.00            | 7.25\n",
            "living_room | 41.00              | 30.00            | 36.93\n",
            "living_room | 6.00              | 6.00            | 6.74\n",
            "living_room | 37.00              | 46.00            | 38.78\n",
            "living_room | 53.00              | 41.00            | 40.87\n",
            "living_room | 4.00              | 6.00            | 12.12\n",
            "balcony    | 74.00              | 87.00            | 43.84\n",
            "balcony    | 7.00              | 7.00            | 6.71\n",
            "bedroom    | 6.00              | 9.00            | 7.56\n",
            "balcony    | 72.00              | 73.00            | 79.16\n",
            "living_room | 37.00              | 32.00            | 39.51\n",
            "balcony    | 66.00              | 75.00            | 79.61\n",
            "living_room | 8.00              | 10.00            | 6.52\n",
            "bedroom    | 7.00              | 7.00            | 6.88\n",
            "bedroom    | 59.00              | 57.00            | 59.80\n",
            "bedroom    | 5.00              | 10.00            | 6.85\n",
            "bedroom    | 9.00              | 6.00            | 7.62\n",
            "bedroom    | 66.00              | 63.00            | 59.58\n",
            "bedroom    | 9.00              | 4.00            | 6.91\n",
            "balcony    | 76.00              | 85.00            | 79.10\n",
            "living_room | 5.00              | 7.00            | 7.11\n",
            "bedroom    | 63.00              | 7.00            | 13.08\n",
            "balcony    | 76.00              | 77.00            | 24.85\n",
            "balcony    | 5.00              | 8.00            | 11.17\n",
            "bedroom    | 9.00              | 7.00            | 5.87\n",
            "bedroom    | 63.00              | 55.00            | 26.90\n",
            "living_room | 7.00              | 11.00            | 7.11\n",
            "bedroom    | 24.00              | 60.00            | 61.41\n",
            "bedroom    | 51.00              | 47.00            | 36.00\n",
            "bedroom    | 6.00              | 4.00            | 7.56\n",
            "bedroom    | 54.00              | 59.00            | 60.38\n",
            "balcony    | 9.00              | 5.00            | 6.71\n",
            "living_room | 53.00              | 50.00            | 40.87\n",
            "bedroom    | 64.00              | 46.00            | 59.39\n",
            "balcony    | 8.00              | 6.00            | 6.55\n",
            "living_room | 12.00              | 40.00            | 38.03\n",
            "balcony    | 8.00              | 6.00            | 6.87\n",
            "bedroom    | 60.00              | 57.00            | 24.76\n",
            "bedroom    | 55.00              | 49.00            | 60.25\n",
            "living_room | 37.00              | 39.00            | 40.96\n",
            "balcony    | 9.00              | 7.00            | 6.92\n",
            "living_room | 6.00              | 8.00            | 7.06\n",
            "bedroom    | 53.00              | 61.00            | 60.37\n",
            "bedroom    | 8.00              | 4.00            | 6.80\n",
            "living_room | 38.00              | 36.00            | 40.23\n",
            "balcony    | 90.00              | 74.00            | 73.84\n",
            "bedroom    | 64.00              | 48.00            | 59.71\n",
            "balcony    | 7.00              | 8.00            | 6.50\n",
            "balcony    | 10.00              | 6.00            | 6.65\n",
            "balcony    | 89.00              | 85.00            | 77.99\n",
            "bedroom    | 7.00              | 6.00            | 8.89\n",
            "balcony    | 27.00              | 85.00            | 78.80\n",
            "balcony    | 81.00              | 70.00            | 78.72\n",
            "living_room | 7.00              | 5.00            | 6.58\n",
            "living_room | 35.00              | 38.00            | 25.14\n",
            "balcony    | 6.00              | 6.00            | 10.36\n",
            "living_room | 38.00              | 8.00            | 15.94\n",
            "living_room | 5.00              | 7.00            | 6.58\n",
            "balcony    | 4.00              | 8.00            | 7.08\n",
            "living_room | 42.00              | 11.00            | 14.09\n",
            "bedroom    | 67.00              | 60.00            | 29.75\n",
            "balcony    | 79.00              | 80.00            | 78.51\n",
            "bedroom    | 71.00              | 55.00            | 58.43\n",
            "bedroom    | 7.00              | 5.00            | 6.79\n",
            "balcony    | 6.00              | 6.00            | 6.66\n",
            "balcony    | 7.00              | 6.00            | 6.82\n",
            "balcony    | 8.00              | 9.00            | 6.65\n",
            "balcony    | 6.00              | 7.00            | 6.78\n",
            "balcony    | 75.00              | 91.00            | 78.93\n",
            "living_room | 7.00              | 12.00            | 16.45\n",
            "living_room | 7.00              | 7.00            | 6.57\n",
            "living_room | 8.00              | 7.00            | 7.48\n",
            "bedroom    | 14.00              | 72.00            | 57.49\n",
            "living_room | 8.00              | 8.00            | 8.97\n",
            "balcony    | 8.00              | 5.00            | 6.76\n",
            "balcony    | 84.00              | 80.00            | 78.25\n",
            "living_room | 43.00              | 38.00            | 40.94\n",
            "bedroom    | 5.00              | 8.00            | 6.95\n",
            "living_room | 47.00              | 37.00            | 38.75\n",
            "balcony    | 8.00              | 6.00            | 6.97\n",
            "living_room | 6.00              | 9.00            | 7.06\n",
            "living_room | 7.00              | 6.00            | 6.79\n",
            "balcony    | 7.00              | 7.00            | 6.60\n",
            "balcony    | 4.00              | 8.00            | 8.18\n",
            "living_room | 29.00              | 37.00            | 41.71\n",
            "bedroom    | 6.00              | 5.00            | 7.16\n",
            "living_room | 11.00              | 53.00            | 37.53\n",
            "bedroom    | 7.00              | 5.00            | 12.01\n",
            "balcony    | 7.00              | 6.00            | 6.50\n",
            "bedroom    | 58.00              | 6.00            | 15.28\n",
            "living_room | 33.00              | 37.00            | 40.25\n",
            "balcony    | 6.00              | 7.00            | 6.60\n",
            "balcony    | 10.00              | 7.00            | 6.65\n",
            "bedroom    | 64.00              | 45.00            | 44.35\n",
            "balcony    | 8.00              | 7.00            | 7.76\n",
            "balcony    | 77.00              | 82.00            | 49.35\n",
            "living_room | 6.00              | 6.00            | 7.61\n",
            "balcony    | 81.00              | 73.00            | 78.48\n",
            "living_room | 8.00              | 8.00            | 7.06\n",
            "bedroom    | 7.00              | 6.00            | 7.10\n",
            "living_room | 7.00              | 6.00            | 9.73\n",
            "living_room | 6.00              | 6.00            | 7.47\n",
            "living_room | 54.00              | 5.00            | 8.51\n",
            "bedroom    | 51.00              | 62.00            | 26.25\n",
            "bedroom    | 65.00              | 7.00            | 13.07\n",
            "living_room | 7.00              | 6.00            | 7.11\n",
            "balcony    | 7.00              | 5.00            | 6.33\n",
            "living_room | 41.00              | 31.00            | 38.77\n",
            "bedroom    | 5.00              | 6.00            | 6.66\n",
            "living_room | 8.00              | 12.00            | 19.61\n",
            "bedroom    | 8.00              | 7.00            | 8.58\n",
            "balcony    | 70.00              | 72.00            | 79.19\n",
            "bedroom    | 55.00              | 52.00            | 52.16\n",
            "living_room | 7.00              | 7.00            | 7.27\n",
            "bedroom    | 63.00              | 56.00            | 59.24\n",
            "bedroom    | 8.00              | 8.00            | 7.66\n",
            "balcony    | 76.00              | 84.00            | 24.85\n",
            "living_room | 42.00              | 42.00            | 40.94\n",
            "bedroom    | 9.00              | 7.00            | 5.44\n",
            "living_room | 8.00              | 29.00            | 19.61\n",
            "balcony    | 6.00              | 9.00            | 6.76\n",
            "balcony    | 8.00              | 6.00            | 7.02\n",
            "balcony    | 84.00              | 75.00            | 78.01\n",
            "bedroom    | 8.00              | 7.00            | 7.66\n",
            "balcony    | 5.00              | 5.00            | 7.73\n",
            "balcony    | 89.00              | 87.00            | 77.75\n",
            "living_room | 37.00              | 42.00            | 31.89\n",
            "balcony    | 87.00              | 71.00            | 78.02\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Prepare the data as before\n",
        "df = pd.read_csv(\"light_data_with_hour_only.csv\")\n",
        "\n",
        "# Shift the light level to get the next light level\n",
        "df['next_light_level'] = df.groupby('room')['light_level'].shift(-1)\n",
        "df = df.dropna(subset=['next_light_level'])\n",
        "\n",
        "# One-hot encode 'room' column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_rooms = encoder.fit_transform(df[['room']])\n",
        "encoded_rooms_df = pd.DataFrame(encoded_rooms, columns=encoder.categories_[0])\n",
        "\n",
        "# Merge everything into a final DataFrame\n",
        "df = pd.concat([df, encoded_rooms_df], axis=1)\n",
        "X = df[['timestamp'] + list(encoded_rooms_df.columns) + ['light_level']]\n",
        "y = df['next_light_level']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the neural network model using TensorFlow (Keras)\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Input layer: First, we define the input shape, which is the number of features in X\n",
        "model.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
        "\n",
        "#CHANGED STUFF ---------------------\n",
        "# Hidden layer 1: A dense layer with 128 neurons and ReLU activation function\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))  # Dropout layer for regularization\n",
        "model.add(tf.keras.layers.BatchNormalization())  # Batch normalization for stability\n",
        "\n",
        "# Hidden layer 3: A dense layer with 32 neurons and ReLU activation function\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "\n",
        "# Output layer: A single neuron to predict the next light level\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "#CHANGED STUFF ---------------------\n",
        "\n",
        "# Compile the model: We'll use Mean Squared Error loss and Adam optimizer\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define the ModelCheckpoint callback to save only the best weights based on validation loss\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'NN_model_deeper.weights.keras',  # File path to save the best weights\n",
        "    monitor='val_loss',          # Monitor validation loss\n",
        "    save_best_only=True,         # Only save the weights when the model improves\n",
        "    mode='min',                  # Minimize the validation loss\n",
        "    verbose=1                    # Display a message when saving the model\n",
        ")\n",
        "\n",
        "# Train the model with the ModelCheckpoint callback\n",
        "model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Function to print real and predicted light levels\n",
        "def predict_and_print_nn(model, X_data, y_data):\n",
        "    \"\"\"\n",
        "    This function takes a trained model, input features (X_data), and real target values (y_data),\n",
        "    and prints the room, current light level, real light level, and predicted light level.\n",
        "    \"\"\"\n",
        "    # Make predictions on the input data\n",
        "    y_pred = model.predict(X_data)\n",
        "\n",
        "    # Iterate over the data and print the room, current light level, real, and predicted light levels\n",
        "    print(f'Room       | Current Light Level | Real Light Level | Predicted Light Level')\n",
        "\n",
        "    for i, (real, pred) in enumerate(zip(y_data, y_pred)):\n",
        "        # Extract the current light level from the 'light_level' column of X_data\n",
        "        current_light = X_data.iloc[i]['light_level']\n",
        "\n",
        "        # Determine the room by checking which column has a value of 1.0\n",
        "        room = X_data.iloc[i][['balcony', 'bedroom', 'living_room']].idxmax()\n",
        "\n",
        "        print(f'{room:<10} | {current_light:.2f}              | {real:.2f}            | {pred[0]:.2f}')\n",
        "\n",
        "# Example usage:\n",
        "# Predict and print the results for the test data\n",
        "predict_and_print_nn(model, X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}